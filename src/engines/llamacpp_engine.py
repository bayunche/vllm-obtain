"""
LlamaCpp æ¨ç†å¼•æ“å®ç°
æä¾›è·¨å¹³å° CPU/GPU æ¨ç†æ”¯æŒï¼Œä½œä¸ºä¿åº•æ–¹æ¡ˆ
"""

import os
import time
import asyncio
import threading
from typing import Optional, AsyncGenerator
from pathlib import Path

from ..core.inference_engine import InferenceEngine, ModelInfo, ModelStatus
from ..core.inference_engine import InferenceRequest, InferenceResponse, EngineConfig
from ..core.exceptions import ModelLoadError, InferenceError, ResourceLimitError
from ..utils import get_logger, performance_monitor


class LlamaCppEngine(InferenceEngine):
    """LlamaCpp æ¨ç†å¼•æ“"""
    
    def __init__(self, config: EngineConfig):
        super().__init__(config)
        self.llama_models = {}  # å­˜å‚¨åŠ è½½çš„ llama-cpp æ¨¡å‹å®ä¾‹
        self._lock = threading.Lock()
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– LlamaCpp å¼•æ“"""
        try:
            # æ£€æŸ¥ llama-cpp-python æ˜¯å¦å¯ç”¨
            import llama_cpp
            self.llama_cpp = llama_cpp
            
            self.logger.info("LlamaCpp å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            self.logger.info(f"llama-cpp-python ç‰ˆæœ¬: {llama_cpp.__version__}")
            
            # è®¾ç½®è®¾å¤‡ç±»å‹
            self._setup_device()
            
            self._initialized = True
            return True
            
        except ImportError as e:
            self.logger.error(f"LlamaCpp å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error("è¯·å®‰è£… llama-cpp-python: pip install llama-cpp-python")
            return False
        except Exception as e:
            self.logger.error(f"LlamaCpp å¼•æ“åˆå§‹åŒ–å¼‚å¸¸: {e}")
            return False
    
    def _setup_device(self):
        """è®¾ç½®è®¾å¤‡é…ç½®"""
        device_type = self.config.device_type
        
        if device_type == "auto":
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡
            if self._is_cuda_available():
                self.device_type = "cuda"
                self.n_gpu_layers = -1  # å…¨éƒ¨å±‚åœ¨GPU
            elif self._is_metal_available():
                self.device_type = "metal"
                self.n_gpu_layers = -1
            else:
                self.device_type = "cpu"
                self.n_gpu_layers = 0
        else:
            self.device_type = device_type
            if device_type in ["cuda", "metal"]:
                self.n_gpu_layers = -1
            else:
                self.n_gpu_layers = 0
        
        self.logger.info(f"è®¾å¤‡ç±»å‹: {self.device_type}, GPUå±‚æ•°: {self.n_gpu_layers}")
    
    def _is_cuda_available(self) -> bool:
        """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    def _is_metal_available(self) -> bool:
        """æ£€æŸ¥Metalæ˜¯å¦å¯ç”¨ (macOS)"""
        import platform
        return platform.system().lower() == "darwin"
    
    @performance_monitor("load_model")
    async def load_model(self, model_name: str, model_path: str, **kwargs) -> bool:
        """
        åŠ è½½LlamaCppæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.ggufæ ¼å¼)
            **kwargs: é¢å¤–å‚æ•°
        """
        if not self._initialized:
            raise RuntimeError("å¼•æ“æœªåˆå§‹åŒ–")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if self.is_model_loaded(model_name):
            self.logger.warning(f"æ¨¡å‹å·²åŠ è½½: {model_name}")
            return True
        
        try:
            # åˆ›å»ºæ¨¡å‹ä¿¡æ¯
            model_info = self._create_model_info(model_name, model_path)
            model_info.status = ModelStatus.LOADING
            self.models[model_name] = model_info
            
            self.logger.info(f"å¼€å§‹åŠ è½½LlamaCppæ¨¡å‹: {model_name} from {model_path}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_file = Path(model_path)
            if not model_file.exists():
                raise ModelLoadError(model_name, f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size_mb = model_file.stat().st_size / 1024 / 1024
            model_info.size_mb = file_size_mb
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
            if not self._check_memory_limit(file_size_mb):
                raise ResourceLimitError("å†…å­˜", file_size_mb, self._get_available_memory())
            
            # è®¾ç½®æ¨¡å‹å‚æ•°
            model_params = {
                "model_path": str(model_path),
                "n_ctx": kwargs.get("context_length", self.config.max_sequence_length),
                "n_threads": kwargs.get("n_threads", self.config.max_cpu_threads),
                "n_gpu_layers": self.n_gpu_layers,
                "verbose": False,
                "seed": kwargs.get("seed", -1),
                "f16_kv": kwargs.get("f16_kv", True),
                "use_mlock": kwargs.get("use_mlock", False),
                "use_mmap": kwargs.get("use_mmap", True),
            }
            
            # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ (é¿å…é˜»å¡äº‹ä»¶å¾ªç¯)
            loop = asyncio.get_event_loop()
            
            def _load_model():
                with self._lock:
                    return self.llama_cpp.Llama(**model_params)
            
            start_time = time.time()
            llama_model = await loop.run_in_executor(None, _load_model)
            load_time = time.time() - start_time
            
            # å­˜å‚¨æ¨¡å‹å®ä¾‹
            self.llama_models[model_name] = llama_model
            
            # æ›´æ–°æ¨¡å‹ä¿¡æ¯
            model_info.status = ModelStatus.LOADED
            model_info.loaded_at = time.time()
            model_info.context_length = model_params["n_ctx"]
            model_info.memory_usage = self._estimate_memory_usage(file_size_mb)
            
            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name} (è€—æ—¶: {load_time:.2f}s, å¤§å°: {file_size_mb:.1f}MB)")
            return True
            
        except Exception as e:
            self._update_model_status(model_name, ModelStatus.ERROR, str(e))
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_name} - {e}")
            
            # æ¸…ç†å¤±è´¥çš„åŠ è½½
            if model_name in self.llama_models:
                del self.llama_models[model_name]
            
            raise ModelLoadError(model_name, str(e))
    
    @performance_monitor("unload_model")
    async def unload_model(self, model_name: str) -> bool:
        """å¸è½½æ¨¡å‹"""
        if model_name not in self.models:
            self.logger.warning(f"æ¨¡å‹æœªæ‰¾åˆ°: {model_name}")
            return False
        
        try:
            self._update_model_status(model_name, ModelStatus.UNLOADING)
            
            # åˆ é™¤æ¨¡å‹å®ä¾‹
            if model_name in self.llama_models:
                with self._lock:
                    del self.llama_models[model_name]
            
            # åˆ é™¤æ¨¡å‹ä¿¡æ¯
            del self.models[model_name]
            
            self.logger.info(f"æ¨¡å‹å¸è½½æˆåŠŸ: {model_name}")
            return True
            
        except Exception as e:
            self._update_model_status(model_name, ModelStatus.ERROR, str(e))
            self.logger.error(f"æ¨¡å‹å¸è½½å¤±è´¥: {model_name} - {e}")
            return False
    
    async def generate(self, request: InferenceRequest) -> InferenceResponse:
        """æ‰§è¡Œæ¨ç†ç”Ÿæˆ"""
        if not self.is_model_loaded(request.model_name):
            raise InferenceError(f"æ¨¡å‹æœªåŠ è½½: {request.model_name}")
        
        try:
            llama_model = self.llama_models[request.model_name]
            prompt = request.to_prompt()
            
            self.logger.debug(f"å¼€å§‹æ¨ç†: {request.model_name} - {prompt[:100]}...")
            
            # æ„å»ºç”Ÿæˆå‚æ•°
            generate_params = {
                "max_tokens": request.max_tokens,
                "temperature": request.temperature,
                "top_p": request.top_p,
                "top_k": request.top_k,
                "repeat_penalty": request.repetition_penalty,
                "stop": request.stop_sequences or [],
                "echo": False
            }
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ¨ç†
            loop = asyncio.get_event_loop()
            
            def _generate():
                with self._lock:
                    start_time = time.time()
                    result = llama_model(prompt, **generate_params)
                    inference_time = time.time() - start_time
                    return result, inference_time
            
            result, inference_time = await loop.run_in_executor(None, _generate)
            
            # è§£æç»“æœ
            completion_text = result["choices"][0]["text"]
            prompt_tokens = result["usage"]["prompt_tokens"]
            completion_tokens = result["usage"]["completion_tokens"]
            total_tokens = result["usage"]["total_tokens"]
            
            tokens_per_second = completion_tokens / inference_time if inference_time > 0 else 0
            
            response = InferenceResponse(
                model_name=request.model_name,
                text=completion_text,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                inference_time=inference_time,
                tokens_per_second=tokens_per_second,
                finish_reason=result["choices"][0].get("finish_reason", "stop"),
                request_id=request.request_id
            )
            
            # è®°å½•æ¨ç†æ—¥å¿—
            self.logger.log_inference(
                model_name=request.model_name,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                inference_time=inference_time,
                request_id=request.request_id
            )
            
            return response
            
        except Exception as e:
            self.logger.error(f"æ¨ç†å¤±è´¥: {request.model_name} - {e}")
            raise InferenceError(str(e), request.model_name)
    
    async def generate_stream(self, request: InferenceRequest) -> AsyncGenerator[str, None]:
        """æµå¼æ¨ç†ç”Ÿæˆ"""
        if not self.is_model_loaded(request.model_name):
            raise InferenceError(f"æ¨¡å‹æœªåŠ è½½: {request.model_name}")
        
        try:
            llama_model = self.llama_models[request.model_name]
            prompt = request.to_prompt()
            
            # æ„å»ºç”Ÿæˆå‚æ•°
            generate_params = {
                "max_tokens": request.max_tokens,
                "temperature": request.temperature,
                "top_p": request.top_p,
                "top_k": request.top_k,
                "repeat_penalty": request.repetition_penalty,
                "stop": request.stop_sequences or [],
                "stream": True,
                "echo": False
            }
            
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æµå¼ç”Ÿæˆ
            def _stream_generate():
                with self._lock:
                    return llama_model(prompt, **generate_params)
            
            loop = asyncio.get_event_loop()
            stream = await loop.run_in_executor(None, _stream_generate)
            
            # é€æ­¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
            for chunk in stream:
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0].get("delta", {})
                    if "content" in delta:
                        yield delta["content"]
                    elif chunk["choices"][0].get("finish_reason"):
                        break
            
        except Exception as e:
            self.logger.error(f"æµå¼æ¨ç†å¤±è´¥: {request.model_name} - {e}")
            raise InferenceError(str(e), request.model_name)
    
    def _check_memory_limit(self, model_size_mb: float) -> bool:
        """æ£€æŸ¥å†…å­˜é™åˆ¶"""
        try:
            import psutil
            
            # è·å–å¯ç”¨å†…å­˜
            available_memory_mb = psutil.virtual_memory().available / 1024 / 1024
            
            # é¢„ç•™ä¸€äº›å†…å­˜ç»™ç³»ç»Ÿ (è‡³å°‘2GB)
            required_memory_mb = model_size_mb * 1.5 + 2048
            
            return available_memory_mb > required_memory_mb
            
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œè·³è¿‡æ£€æŸ¥
            return True
    
    def _get_available_memory(self) -> float:
        """è·å–å¯ç”¨å†…å­˜ (MB)"""
        try:
            import psutil
            return psutil.virtual_memory().available / 1024 / 1024
        except ImportError:
            return 0.0
    
    def _estimate_memory_usage(self, file_size_mb: float) -> str:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        # é€šå¸¸æ¨¡å‹åœ¨å†…å­˜ä¸­çš„å¤§å°çº¦ä¸ºæ–‡ä»¶å¤§å°çš„1.2-1.5å€
        estimated_mb = file_size_mb * 1.3
        return f"{estimated_mb:.1f}MB"
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†LlamaCppå¼•æ“èµ„æº")
        
        # å¸è½½æ‰€æœ‰æ¨¡å‹
        model_names = list(self.llama_models.keys())
        for model_name in model_names:
            try:
                await self.unload_model(model_name)
            except Exception as e:
                self.logger.error(f"æ¸…ç†æ¨¡å‹å¤±è´¥ {model_name}: {e}")
        
        # æ¸…ç†æ¨¡å‹å­—å…¸
        self.llama_models.clear()
        
        await super().cleanup()


if __name__ == "__main__":
    # æµ‹è¯•LlamaCppå¼•æ“
    import asyncio
    
    async def test_llamacpp_engine():
        print("=== LlamaCpp å¼•æ“æµ‹è¯• ===")
        
        config = EngineConfig(
            engine_type="llama_cpp",
            device_type="auto",
            max_sequence_length=2048,
            max_cpu_threads=4
        )
        
        engine = LlamaCppEngine(config)
        
        # åˆå§‹åŒ–
        success = await engine.initialize()
        if not success:
            print("âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥")
            return
        
        print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦çœŸå®çš„GGUFæ¨¡å‹æ–‡ä»¶è¿›è¡Œæµ‹è¯•
        # model_path = "/path/to/your/model.gguf"
        
        print("ğŸ” ä¸ºäº†å®Œæ•´æµ‹è¯•ï¼Œéœ€è¦æä¾›çœŸå®çš„GGUFæ¨¡å‹æ–‡ä»¶è·¯å¾„")
        print("è¯·å‚è€ƒ README.md ä¸­çš„æ¨¡å‹ä¸‹è½½å’Œé…ç½®è¯´æ˜")
        
        # è·å–å¼•æ“çŠ¶æ€
        status = engine.get_engine_status()
        print(f"âœ… å¼•æ“çŠ¶æ€: {status}")
        
        # å¥åº·æ£€æŸ¥
        health = await engine.health_check()
        print(f"âœ… å¥åº·æ£€æŸ¥: {health}")
        
        await engine.cleanup()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    asyncio.run(test_llamacpp_engine())