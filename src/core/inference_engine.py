"""
æ¨ç†å¼•æ“æŠ½è±¡å±‚
å®šä¹‰ç»Ÿä¸€çš„æ¨ç†å¼•æ“æ¥å£ï¼Œæ”¯æŒä¸åŒçš„æ¨ç†åç«¯
"""

import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, AsyncGenerator, Union
from enum import Enum

from ..utils import get_logger


class ModelStatus(Enum):
    """æ¨¡å‹çŠ¶æ€æšä¸¾"""
    UNLOADED = "unloaded"
    LOADING = "loading"
    LOADED = "loaded"
    UNLOADING = "unloading"
    ERROR = "error"


@dataclass
class EngineConfig:
    """æ¨ç†å¼•æ“é…ç½®"""
    engine_type: str  # vllm, mlx, llama_cpp
    device_type: str = "auto"  # auto, cuda, mps, cpu
    max_gpu_memory: float = 0.8
    max_cpu_threads: int = 8
    enable_streaming: bool = True
    batch_size: int = 1
    max_sequence_length: int = 4096
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.0
    
    # å¼•æ“ç‰¹å®šå‚æ•°
    extra_params: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    path: str
    status: ModelStatus = ModelStatus.UNLOADED
    size_mb: Optional[float] = None
    parameters: Optional[int] = None
    context_length: Optional[int] = None
    loaded_at: Optional[float] = None
    memory_usage: Optional[str] = None
    error_message: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "name": self.name,
            "path": self.path,
            "status": self.status.value,
            "size_mb": self.size_mb,
            "parameters": self.parameters,
            "context_length": self.context_length,
            "loaded_at": self.loaded_at,
            "memory_usage": self.memory_usage,
            "error_message": self.error_message
        }


@dataclass
class InferenceRequest:
    """æ¨ç†è¯·æ±‚"""
    model_name: str
    prompt: str
    max_tokens: int = 100
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.0
    stop_sequences: Optional[List[str]] = None
    stream: bool = False
    
    # OpenAI å…¼å®¹å‚æ•°
    messages: Optional[List[Dict[str, str]]] = None
    request_id: Optional[str] = None
    user: Optional[str] = None
    
    # å¤šæ¨¡æ€æ”¯æŒ
    multimodal_data: Optional[List[Dict[str, Any]]] = None  # å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨
    image_path: Optional[str] = None  # å›¾ç‰‡æ–‡ä»¶è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
    
    def to_prompt(self) -> str:
        """è½¬æ¢ä¸ºæ¨ç†æç¤ºè¯"""
        if self.messages:
            # å°†å¯¹è¯æ¶ˆæ¯è½¬æ¢ä¸ºprompt
            prompt_parts = []
            for message in self.messages:
                role = message.get("role", "user")
                content = message.get("content", "")
                if role == "system":
                    prompt_parts.append(f"System: {content}")
                elif role == "user":
                    prompt_parts.append(f"User: {content}")
                elif role == "assistant":
                    prompt_parts.append(f"Assistant: {content}")
            
            prompt_parts.append("Assistant:")
            return "\n".join(prompt_parts)
        
        return self.prompt


@dataclass
class InferenceResponse:
    """æ¨ç†å“åº”"""
    model_name: str
    text: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    inference_time: float
    tokens_per_second: float
    finish_reason: str = "stop"
    request_id: Optional[str] = None
    
    def to_openai_format(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºOpenAI APIæ ¼å¼"""
        return {
            "id": f"chatcmpl-{self.request_id or int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": self.model_name,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": self.text
                },
                "finish_reason": self.finish_reason
            }],
            "usage": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
                "total_tokens": self.total_tokens
            }
        }


class InferenceEngine(ABC):
    """æ¨ç†å¼•æ“æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: EngineConfig):
        self.config = config
        self.logger = get_logger()
        self.models: Dict[str, ModelInfo] = {}
        self._initialized = False
    
    @abstractmethod
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def load_model(self, model_name: str, model_path: str, **kwargs) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_path: æ¨¡å‹è·¯å¾„
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def unload_model(self, model_name: str) -> bool:
        """
        å¸è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            æ˜¯å¦å¸è½½æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def generate(self, request: InferenceRequest) -> InferenceResponse:
        """
        æ‰§è¡Œæ¨ç†ç”Ÿæˆ
        
        Args:
            request: æ¨ç†è¯·æ±‚
            
        Returns:
            æ¨ç†å“åº”
        """
        pass
    
    @abstractmethod
    async def generate_stream(self, request: InferenceRequest) -> AsyncGenerator[str, None]:
        """
        æµå¼æ¨ç†ç”Ÿæˆ
        
        Args:
            request: æ¨ç†è¯·æ±‚
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        pass
    
    def get_model_info(self, model_name: str) -> Optional[ModelInfo]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.models.get(model_name)
    
    def list_models(self) -> List[ModelInfo]:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        return list(self.models.values())
    
    def get_loaded_models(self) -> List[ModelInfo]:
        """è·å–å·²åŠ è½½çš„æ¨¡å‹"""
        return [model for model in self.models.values() 
                if model.status == ModelStatus.LOADED]
    
    def is_model_loaded(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        model = self.models.get(model_name)
        return model is not None and model.status == ModelStatus.LOADED
    
    def get_engine_status(self) -> Dict[str, Any]:
        """è·å–å¼•æ“çŠ¶æ€"""
        loaded_models = self.get_loaded_models()
        
        return {
            "engine_type": self.config.engine_type,
            "initialized": self._initialized,
            "total_models": len(self.models),
            "loaded_models": len(loaded_models),
            "device_type": self.config.device_type,
            "max_gpu_memory": self.config.max_gpu_memory,
            "models": [model.to_dict() for model in loaded_models]
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        status = {
            "status": "healthy",
            "engine_type": self.config.engine_type,
            "initialized": self._initialized,
            "timestamp": time.time()
        }
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹
            loaded_models = self.get_loaded_models()
            if not loaded_models:
                status["status"] = "no_models"
                status["message"] = "æ²¡æœ‰å·²åŠ è½½çš„æ¨¡å‹"
                return status
            
            # å¯ä»¥æ·»åŠ æ›´å¤šå¥åº·æ£€æŸ¥é€»è¾‘
            # ä¾‹å¦‚ï¼šç®€å•æ¨ç†æµ‹è¯•ã€èµ„æºä½¿ç”¨æ£€æŸ¥ç­‰
            
        except Exception as e:
            status["status"] = "error"
            status["message"] = str(e)
            self.logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        
        return status
    
    def _update_model_status(self, model_name: str, status: ModelStatus, 
                           error_message: str = None):
        """æ›´æ–°æ¨¡å‹çŠ¶æ€"""
        if model_name in self.models:
            self.models[model_name].status = status
            if error_message:
                self.models[model_name].error_message = error_message
            
            self.logger.info(f"æ¨¡å‹çŠ¶æ€æ›´æ–°: {model_name} -> {status.value}")
    
    def _create_model_info(self, model_name: str, model_path: str, **kwargs) -> ModelInfo:
        """åˆ›å»ºæ¨¡å‹ä¿¡æ¯å¯¹è±¡"""
        return ModelInfo(
            name=model_name,
            path=model_path,
            **kwargs
        )
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("å¼€å§‹æ¸…ç†æ¨ç†å¼•æ“èµ„æº")
        
        # å¸è½½æ‰€æœ‰å·²åŠ è½½çš„æ¨¡å‹
        loaded_models = self.get_loaded_models()
        for model in loaded_models:
            try:
                await self.unload_model(model.name)
            except Exception as e:
                self.logger.error(f"å¸è½½æ¨¡å‹å¤±è´¥ {model.name}: {e}")
        
        self._initialized = False
        self.logger.info("æ¨ç†å¼•æ“èµ„æºæ¸…ç†å®Œæˆ")


class DummyEngine(InferenceEngine):
    """è™šæ‹Ÿæ¨ç†å¼•æ“ (ç”¨äºæµ‹è¯•)"""
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è™šæ‹Ÿå¼•æ“"""
        self.logger.info("åˆå§‹åŒ–è™šæ‹Ÿæ¨ç†å¼•æ“")
        self._initialized = True
        return True
    
    async def load_model(self, model_name: str, model_path: str, **kwargs) -> bool:
        """åŠ è½½è™šæ‹Ÿæ¨¡å‹"""
        self.logger.info(f"åŠ è½½è™šæ‹Ÿæ¨¡å‹: {model_name}")
        
        model_info = self._create_model_info(
            model_name, model_path,
            size_mb=100.0,
            parameters=1000000,
            context_length=2048,
            loaded_at=time.time(),
            memory_usage="100MB"
        )
        model_info.status = ModelStatus.LOADED
        
        self.models[model_name] = model_info
        return True
    
    async def unload_model(self, model_name: str) -> bool:
        """å¸è½½è™šæ‹Ÿæ¨¡å‹"""
        if model_name in self.models:
            self.logger.info(f"å¸è½½è™šæ‹Ÿæ¨¡å‹: {model_name}")
            del self.models[model_name]
            return True
        return False
    
    async def generate(self, request: InferenceRequest) -> InferenceResponse:
        """è™šæ‹Ÿæ¨ç†ç”Ÿæˆ"""
        import random
        
        # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
        inference_time = random.uniform(0.5, 2.0)
        await asyncio.sleep(inference_time)
        
        # ç”Ÿæˆè™šæ‹Ÿå“åº”
        completion_text = f"è¿™æ˜¯å¯¹ '{request.prompt}' çš„è™šæ‹Ÿå›å¤ã€‚"
        prompt_tokens = len(request.prompt.split())
        completion_tokens = len(completion_text.split())
        
        return InferenceResponse(
            model_name=request.model_name,
            text=completion_text,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens,
            inference_time=inference_time,
            tokens_per_second=completion_tokens / inference_time,
            request_id=request.request_id
        )
    
    async def generate_stream(self, request: InferenceRequest) -> AsyncGenerator[str, None]:
        """è™šæ‹Ÿæµå¼ç”Ÿæˆ"""
        import asyncio
        
        words = f"è¿™æ˜¯å¯¹ '{request.prompt}' çš„è™šæ‹Ÿæµå¼å›å¤ã€‚".split()
        
        for word in words:
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
            yield word + " "


if __name__ == "__main__":
    # æµ‹è¯•æ¨ç†å¼•æ“æŠ½è±¡å±‚
    import asyncio
    
    async def test_dummy_engine():
        print("=== æ¨ç†å¼•æ“æŠ½è±¡å±‚æµ‹è¯• ===")
        
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        
        # åˆå§‹åŒ–
        await engine.initialize()
        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
        # åŠ è½½æ¨¡å‹
        await engine.load_model("test-model", "/path/to/model")
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æ¨ç†æµ‹è¯•
        request = InferenceRequest(
            model_name="test-model",
            prompt="ä½ å¥½ï¼Œä¸–ç•Œï¼",
            max_tokens=50
        )
        
        response = await engine.generate(request)
        print(f"âœ… æ¨ç†å®Œæˆ: {response.text}")
        print(f"Tokenç»Ÿè®¡: {response.prompt_tokens}+{response.completion_tokens}={response.total_tokens}")
        print(f"æ¨ç†é€Ÿåº¦: {response.tokens_per_second:.1f} tokens/s")
        
        # æµå¼æ¨ç†æµ‹è¯•
        print("\nğŸ”„ æµå¼æ¨ç†æµ‹è¯•:")
        async for chunk in engine.generate_stream(request):
            print(chunk, end="", flush=True)
        print("\n")
        
        # è·å–å¼•æ“çŠ¶æ€
        status = engine.get_engine_status()
        print(f"âœ… å¼•æ“çŠ¶æ€: {status}")
        
        # å¥åº·æ£€æŸ¥
        health = await engine.health_check()
        print(f"âœ… å¥åº·æ£€æŸ¥: {health}")
        
        # æ¸…ç†
        await engine.cleanup()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    asyncio.run(test_dummy_engine())