# 服务配置
HOST=0.0.0.0
PORT=8000
WORKERS=4
DEBUG=False

# 推理引擎配置
INFERENCE_ENGINE=auto  # auto|vllm|mlx|llama_cpp
INFERENCE_MODE=single  # single|multi_instance|load_balance

# 模型配置
MAX_CONCURRENT_MODELS=1
DEFAULT_MODEL=Qwen2.5-7B-Instruct
MODEL_BASE_PATH=./models
MODEL_CACHE_DIR=./cache

# 硬件配置
DEVICE_TYPE=auto  # auto|cuda|mps|cpu
MAX_GPU_MEMORY=0.8
MAX_CPU_THREADS=8

# OpenAI API 兼容配置
OPENAI_API_KEY=your_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1

# 日志配置
LOG_LEVEL=INFO
LOG_FILE=./logs/inference.log
LOG_ROTATION=100MB
LOG_RETENTION=7days

# 负载均衡配置 (仅在 load_balance 模式下使用)
CLUSTER_NODES=127.0.0.1:8001,127.0.0.1:8002
HEALTH_CHECK_INTERVAL=30

# 性能优化配置
ENABLE_CACHING=True
CACHE_SIZE=1000
REQUEST_TIMEOUT=300
MAX_CONCURRENT_REQUESTS=100