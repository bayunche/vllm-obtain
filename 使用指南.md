# 🚀 VLLM 跨平台推理服务使用指南

## 📋 快速开始

### 1. 环境准备

```bash
# 克隆项目
git clone <repository-url>
cd vllm推理框架

# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

### 2. 配置环境

```bash
# 复制配置模板
cp .env.example .env

# 编辑配置文件 (可选)
# 默认配置已经能够自动检测最佳设置
```

### 3. 启动服务

```bash
# 开发模式启动 (推荐开始使用)
python run.py --mode dev

# 生产模式启动
python run.py --mode prod
```

### 4. 验证服务

```bash
# 健康检查
curl http://localhost:8000/health

# 查看可用模型
curl http://localhost:8000/v1/models

# 测试聊天接口
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "test-model",
    "messages": [{"role": "user", "content": "你好"}],
    "max_tokens": 100
  }'
```

---

## 🔧 配置详解

### 核心配置项

| 配置项 | 默认值 | 说明 |
|--------|--------|------|
| `INFERENCE_ENGINE` | `auto` | 推理引擎选择 (auto/vllm/mlx/llama_cpp) |
| `INFERENCE_MODE` | `single` | 运行模式 (single/multi_instance/load_balance) |
| `MAX_CONCURRENT_MODELS` | `1` | 最大并发模型数 |
| `DEFAULT_MODEL` | `Qwen2.5-7B-Instruct` | 默认模型名称 |
| `MODEL_BASE_PATH` | `./models` | 模型存储路径 |
| `DEVICE_TYPE` | `auto` | 设备类型 (auto/cuda/mps/cpu) |

### 平台特定推荐配置

#### macOS (M3 Ultra)
```bash
INFERENCE_ENGINE=auto          # 自动选择 MLX
DEVICE_TYPE=mps
MAX_GPU_MEMORY=0.8
MAX_CONCURRENT_MODELS=3        # 利用大内存
```

#### Linux/Windows (CUDA)
```bash
INFERENCE_ENGINE=auto          # 自动选择 VLLM
DEVICE_TYPE=cuda
MAX_GPU_MEMORY=0.8
MAX_CONCURRENT_MODELS=2
```

#### 通用 CPU
```bash
INFERENCE_ENGINE=llama_cpp
DEVICE_TYPE=cpu
MAX_CPU_THREADS=8
MAX_CONCURRENT_MODELS=1
```

---

## 📚 模型管理

### 支持的模型格式

| 引擎 | 支持格式 | 推荐场景 |
|------|----------|----------|
| **VLLM** | HuggingFace, Safetensors | Linux/Windows GPU 高性能 |
| **MLX** | MLX 格式, HuggingFace | macOS Apple Silicon |
| **LlamaCpp** | GGUF 格式 | 通用 CPU/GPU，资源受限 |

### 模型下载和转换

#### 1. GGUF 模型 (推荐开始)
```bash
# 下载 Qwen2.5-7B GGUF 模型
mkdir -p models
cd models

# 从 HuggingFace 下载
wget https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_0.gguf

# 或使用 git lfs
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF
```

#### 2. HuggingFace 模型
```bash
# 克隆 HuggingFace 模型
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct models/Qwen2.5-7B-Instruct
```

### 动态加载模型

```bash
# 通过 API 加载模型
curl -X POST http://localhost:8000/v1/models/load \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "my-model",
    "model_path": "/path/to/model",
    "engine_type": "llama_cpp"
  }'

# 卸载模型
curl -X DELETE http://localhost:8000/v1/models/my-model/unload

# 查看模型状态
curl http://localhost:8000/v1/models/my-model/status
```

---

## 🔌 OpenAI API 兼容性

### 完全支持的端点

#### 1. 聊天补全 (ChatGPT API)
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "system", "content": "你是一个有用的助手"},
      {"role": "user", "content": "解释什么是机器学习"}
    ],
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": false
  }'
```

#### 2. 流式聊天
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [{"role": "user", "content": "写一首诗"}],
    "stream": true
  }'
```

#### 3. 文本补全
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "prompt": "人工智能的未来发展方向包括",
    "max_tokens": 200
  }'
```

#### 4. 模型列表
```bash
curl http://localhost:8000/v1/models
```

### n8n 集成示例

1. **添加 OpenAI 节点**
2. **配置连接**:
   - Base URL: `http://your-server:8000/v1`
   - API Key: `可以留空或随意填写`
3. **选择模型**: 使用已加载的模型名称
4. **正常使用**: 与 OpenAI 官方 API 完全一致

---

## 🛠️ 管理接口

### 系统监控

```bash
# 系统状态
curl http://localhost:8000/v1/system/status

# 健康检查
curl http://localhost:8000/v1/system/health

# 性能指标
curl http://localhost:8000/v1/system/metrics

# 查看配置
curl http://localhost:8000/v1/config

# 最近日志
curl http://localhost:8000/v1/logs/recent?limit=50
```

### 模型管理

```bash
# 列出所有模型
curl http://localhost:8000/v1/models/list

# 模型详细信息
curl http://localhost:8000/v1/models/{model_name}/status
```

---

## 🔍 故障排除

### 常见问题

#### 1. 模型加载失败
```bash
# 检查模型文件是否存在
ls -la models/

# 检查日志
curl http://localhost:8000/v1/logs/recent

# 检查系统资源
curl http://localhost:8000/v1/system/metrics
```

#### 2. 推理速度慢
```bash
# 检查设备类型
curl http://localhost:8000/v1/config

# 调整并发设置
export MAX_CONCURRENT_REQUESTS=50
export WORKERS=8

# 启用缓存
export ENABLE_CACHING=True
```

#### 3. 内存不足
```bash
# 减少并发模型数
export MAX_CONCURRENT_MODELS=1

# 调整 GPU 内存使用
export MAX_GPU_MEMORY=0.6

# 使用 CPU 模式
export DEVICE_TYPE=cpu
export INFERENCE_ENGINE=llama_cpp
```

### 日志分析

```bash
# 查看服务日志
tail -f logs/inference.log

# 查看性能指标
grep "tokens_per_second" logs/metrics.log

# 查看错误日志
grep "ERROR" logs/inference.log
```

---

## 🚀 生产部署

### Docker 部署

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

EXPOSE 8000
CMD ["python", "run.py", "--mode", "prod"]
```

```bash
# 构建和运行
docker build -t vllm-service .
docker run -p 8000:8000 -v ./models:/app/models vllm-service
```

### nginx 反向代理

```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_buffering off;
        proxy_request_buffering off;
    }
}
```

### 系统服务

```ini
# /etc/systemd/system/vllm-service.service
[Unit]
Description=VLLM Inference Service
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/opt/vllm-service
ExecStart=/opt/vllm-service/venv/bin/python run.py --mode prod
Restart=always
Environment=PATH=/opt/vllm-service/venv/bin

[Install]
WantedBy=multi-user.target
```

```bash
# 启用系统服务
sudo systemctl enable vllm-service
sudo systemctl start vllm-service
sudo systemctl status vllm-service
```

---

## 📊 性能优化

### 硬件建议

| 用途 | CPU | 内存 | GPU | 存储 |
|------|-----|------|-----|------|
| 开发测试 | 4核+ | 16GB+ | 可选 | 100GB+ |
| 小规模生产 | 8核+ | 32GB+ | RTX 4090 24GB | 500GB+ SSD |
| 大规模生产 | 16核+ | 64GB+ | A100 80GB | 1TB+ NVMe |

### 性能调优

1. **模型选择**
   - 7B 模型: 日常对话，16GB+ 显存
   - 13B 模型: 复杂任务，24GB+ 显存
   - 量化模型: 资源受限环境

2. **并发设置**
   ```bash
   # 高端硬件
   MAX_CONCURRENT_MODELS=3
   MAX_CONCURRENT_REQUESTS=100
   WORKERS=8
   
   # 中端硬件
   MAX_CONCURRENT_MODELS=1
   MAX_CONCURRENT_REQUESTS=50
   WORKERS=4
   ```

3. **缓存策略**
   ```bash
   ENABLE_CACHING=True
   CACHE_SIZE=1000
   ```

---

## 🔒 安全建议

### 生产环境安全

1. **API 认证**
   ```bash
   # 设置 API 密钥
   export OPENAI_API_KEY=your-secret-key
   ```

2. **网络安全**
   - 使用 HTTPS
   - 限制访问IP
   - 配置防火墙

3. **日志安全**
   - 定期轮转日志
   - 不记录敏感信息
   - 设置适当权限

---

## 📞 获取帮助

- 📖 [详细文档](开发文档.md)
- 🐛 [问题报告](../../issues)
- 💬 [讨论区](../../discussions)
- 📧 技术支持: support@example.com

---

**快速问题解决**:
1. 先查看健康检查: `curl http://localhost:8000/health`
2. 检查日志: `curl http://localhost:8000/v1/logs/recent`
3. 验证配置: `curl http://localhost:8000/v1/config`
4. 重启服务: `python run.py --mode dev`