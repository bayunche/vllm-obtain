# ğŸš€ VLLM è·¨å¹³å°æ¨ç†æœåŠ¡ä½¿ç”¨æŒ‡å—

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd vllmæ¨ç†æ¡†æ¶

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½®ç¯å¢ƒ

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶ (å¯é€‰)
# é»˜è®¤é…ç½®å·²ç»èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾ç½®
```

### 3. å¯åŠ¨æœåŠ¡

```bash
# å¼€å‘æ¨¡å¼å¯åŠ¨ (æ¨èå¼€å§‹ä½¿ç”¨)
python run.py --mode dev

# ç”Ÿäº§æ¨¡å¼å¯åŠ¨
python run.py --mode prod
```

### 4. éªŒè¯æœåŠ¡

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8000/v1/models

# æµ‹è¯•èŠå¤©æ¥å£
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "test-model",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

---

## ğŸ”§ é…ç½®è¯¦è§£

### æ ¸å¿ƒé…ç½®é¡¹

| é…ç½®é¡¹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `INFERENCE_ENGINE` | `auto` | æ¨ç†å¼•æ“é€‰æ‹© (auto/vllm/mlx/llama_cpp) |
| `INFERENCE_MODE` | `single` | è¿è¡Œæ¨¡å¼ (single/multi_instance/load_balance) |
| `MAX_CONCURRENT_MODELS` | `1` | æœ€å¤§å¹¶å‘æ¨¡å‹æ•° |
| `DEFAULT_MODEL` | `Qwen2.5-7B-Instruct` | é»˜è®¤æ¨¡å‹åç§° |
| `MODEL_BASE_PATH` | `./models` | æ¨¡å‹å­˜å‚¨è·¯å¾„ |
| `DEVICE_TYPE` | `auto` | è®¾å¤‡ç±»å‹ (auto/cuda/mps/cpu) |

### å¹³å°ç‰¹å®šæ¨èé…ç½®

#### macOS (M3 Ultra)
```bash
INFERENCE_ENGINE=auto          # è‡ªåŠ¨é€‰æ‹© MLX
DEVICE_TYPE=mps
MAX_GPU_MEMORY=0.8
MAX_CONCURRENT_MODELS=3        # åˆ©ç”¨å¤§å†…å­˜
```

#### Linux/Windows (CUDA)
```bash
INFERENCE_ENGINE=auto          # è‡ªåŠ¨é€‰æ‹© VLLM
DEVICE_TYPE=cuda
MAX_GPU_MEMORY=0.8
MAX_CONCURRENT_MODELS=2
```

#### é€šç”¨ CPU
```bash
INFERENCE_ENGINE=llama_cpp
DEVICE_TYPE=cpu
MAX_CPU_THREADS=8
MAX_CONCURRENT_MODELS=1
```

---

## ğŸ“š æ¨¡å‹ç®¡ç†

### æ”¯æŒçš„æ¨¡å‹æ ¼å¼

| å¼•æ“ | æ”¯æŒæ ¼å¼ | æ¨èåœºæ™¯ |
|------|----------|----------|
| **VLLM** | HuggingFace, Safetensors | Linux/Windows GPU é«˜æ€§èƒ½ |
| **MLX** | MLX æ ¼å¼, HuggingFace | macOS Apple Silicon |
| **LlamaCpp** | GGUF æ ¼å¼ | é€šç”¨ CPU/GPUï¼Œèµ„æºå—é™ |

### æ¨¡å‹ä¸‹è½½å’Œè½¬æ¢

#### 1. GGUF æ¨¡å‹ (æ¨èå¼€å§‹)
```bash
# ä¸‹è½½ Qwen2.5-7B GGUF æ¨¡å‹
mkdir -p models
cd models

# ä» HuggingFace ä¸‹è½½
wget https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_0.gguf

# æˆ–ä½¿ç”¨ git lfs
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF
```

#### 2. HuggingFace æ¨¡å‹
```bash
# å…‹éš† HuggingFace æ¨¡å‹
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct models/Qwen2.5-7B-Instruct
```

### åŠ¨æ€åŠ è½½æ¨¡å‹

```bash
# é€šè¿‡ API åŠ è½½æ¨¡å‹
curl -X POST http://localhost:8000/v1/models/load \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "my-model",
    "model_path": "/path/to/model",
    "engine_type": "llama_cpp"
  }'

# å¸è½½æ¨¡å‹
curl -X DELETE http://localhost:8000/v1/models/my-model/unload

# æŸ¥çœ‹æ¨¡å‹çŠ¶æ€
curl http://localhost:8000/v1/models/my-model/status
```

---

## ğŸ”Œ OpenAI API å…¼å®¹æ€§

### å®Œå…¨æ”¯æŒçš„ç«¯ç‚¹

#### 1. èŠå¤©è¡¥å…¨ (ChatGPT API)
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"},
      {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
    ],
    "max_tokens": 500,
    "temperature": 0.7,
    "stream": false
  }'
```

#### 2. æµå¼èŠå¤©
```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [{"role": "user", "content": "å†™ä¸€é¦–è¯—"}],
    "stream": true
  }'
```

#### 3. æ–‡æœ¬è¡¥å…¨
```bash
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "prompt": "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬",
    "max_tokens": 200
  }'
```

#### 4. æ¨¡å‹åˆ—è¡¨
```bash
curl http://localhost:8000/v1/models
```

### n8n é›†æˆç¤ºä¾‹

1. **æ·»åŠ  OpenAI èŠ‚ç‚¹**
2. **é…ç½®è¿æ¥**:
   - Base URL: `http://your-server:8000/v1`
   - API Key: `å¯ä»¥ç•™ç©ºæˆ–éšæ„å¡«å†™`
3. **é€‰æ‹©æ¨¡å‹**: ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹åç§°
4. **æ­£å¸¸ä½¿ç”¨**: ä¸ OpenAI å®˜æ–¹ API å®Œå…¨ä¸€è‡´

---

## ğŸ› ï¸ ç®¡ç†æ¥å£

### ç³»ç»Ÿç›‘æ§

```bash
# ç³»ç»ŸçŠ¶æ€
curl http://localhost:8000/v1/system/status

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/v1/system/health

# æ€§èƒ½æŒ‡æ ‡
curl http://localhost:8000/v1/system/metrics

# æŸ¥çœ‹é…ç½®
curl http://localhost:8000/v1/config

# æœ€è¿‘æ—¥å¿—
curl http://localhost:8000/v1/logs/recent?limit=50
```

### æ¨¡å‹ç®¡ç†

```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
curl http://localhost:8000/v1/models/list

# æ¨¡å‹è¯¦ç»†ä¿¡æ¯
curl http://localhost:8000/v1/models/{model_name}/status
```

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la models/

# æ£€æŸ¥æ—¥å¿—
curl http://localhost:8000/v1/logs/recent

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
curl http://localhost:8000/v1/system/metrics
```

#### 2. æ¨ç†é€Ÿåº¦æ…¢
```bash
# æ£€æŸ¥è®¾å¤‡ç±»å‹
curl http://localhost:8000/v1/config

# è°ƒæ•´å¹¶å‘è®¾ç½®
export MAX_CONCURRENT_REQUESTS=50
export WORKERS=8

# å¯ç”¨ç¼“å­˜
export ENABLE_CACHING=True
```

#### 3. å†…å­˜ä¸è¶³
```bash
# å‡å°‘å¹¶å‘æ¨¡å‹æ•°
export MAX_CONCURRENT_MODELS=1

# è°ƒæ•´ GPU å†…å­˜ä½¿ç”¨
export MAX_GPU_MEMORY=0.6

# ä½¿ç”¨ CPU æ¨¡å¼
export DEVICE_TYPE=cpu
export INFERENCE_ENGINE=llama_cpp
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -f logs/inference.log

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
grep "tokens_per_second" logs/metrics.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" logs/inference.log
```

---

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### Docker éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

EXPOSE 8000
CMD ["python", "run.py", "--mode", "prod"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t vllm-service .
docker run -p 8000:8000 -v ./models:/app/models vllm-service
```

### nginx åå‘ä»£ç†

```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_buffering off;
        proxy_request_buffering off;
    }
}
```

### ç³»ç»ŸæœåŠ¡

```ini
# /etc/systemd/system/vllm-service.service
[Unit]
Description=VLLM Inference Service
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/opt/vllm-service
ExecStart=/opt/vllm-service/venv/bin/python run.py --mode prod
Restart=always
Environment=PATH=/opt/vllm-service/venv/bin

[Install]
WantedBy=multi-user.target
```

```bash
# å¯ç”¨ç³»ç»ŸæœåŠ¡
sudo systemctl enable vllm-service
sudo systemctl start vllm-service
sudo systemctl status vllm-service
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç¡¬ä»¶å»ºè®®

| ç”¨é€” | CPU | å†…å­˜ | GPU | å­˜å‚¨ |
|------|-----|------|-----|------|
| å¼€å‘æµ‹è¯• | 4æ ¸+ | 16GB+ | å¯é€‰ | 100GB+ |
| å°è§„æ¨¡ç”Ÿäº§ | 8æ ¸+ | 32GB+ | RTX 4090 24GB | 500GB+ SSD |
| å¤§è§„æ¨¡ç”Ÿäº§ | 16æ ¸+ | 64GB+ | A100 80GB | 1TB+ NVMe |

### æ€§èƒ½è°ƒä¼˜

1. **æ¨¡å‹é€‰æ‹©**
   - 7B æ¨¡å‹: æ—¥å¸¸å¯¹è¯ï¼Œ16GB+ æ˜¾å­˜
   - 13B æ¨¡å‹: å¤æ‚ä»»åŠ¡ï¼Œ24GB+ æ˜¾å­˜
   - é‡åŒ–æ¨¡å‹: èµ„æºå—é™ç¯å¢ƒ

2. **å¹¶å‘è®¾ç½®**
   ```bash
   # é«˜ç«¯ç¡¬ä»¶
   MAX_CONCURRENT_MODELS=3
   MAX_CONCURRENT_REQUESTS=100
   WORKERS=8
   
   # ä¸­ç«¯ç¡¬ä»¶
   MAX_CONCURRENT_MODELS=1
   MAX_CONCURRENT_REQUESTS=50
   WORKERS=4
   ```

3. **ç¼“å­˜ç­–ç•¥**
   ```bash
   ENABLE_CACHING=True
   CACHE_SIZE=1000
   ```

---

## ğŸ”’ å®‰å…¨å»ºè®®

### ç”Ÿäº§ç¯å¢ƒå®‰å…¨

1. **API è®¤è¯**
   ```bash
   # è®¾ç½® API å¯†é’¥
   export OPENAI_API_KEY=your-secret-key
   ```

2. **ç½‘ç»œå®‰å…¨**
   - ä½¿ç”¨ HTTPS
   - é™åˆ¶è®¿é—®IP
   - é…ç½®é˜²ç«å¢™

3. **æ—¥å¿—å®‰å…¨**
   - å®šæœŸè½®è½¬æ—¥å¿—
   - ä¸è®°å½•æ•æ„Ÿä¿¡æ¯
   - è®¾ç½®é€‚å½“æƒé™

---

## ğŸ“ è·å–å¸®åŠ©

- ğŸ“– [è¯¦ç»†æ–‡æ¡£](å¼€å‘æ–‡æ¡£.md)
- ğŸ› [é—®é¢˜æŠ¥å‘Š](../../issues)
- ğŸ’¬ [è®¨è®ºåŒº](../../discussions)
- ğŸ“§ æŠ€æœ¯æ”¯æŒ: support@example.com

---

**å¿«é€Ÿé—®é¢˜è§£å†³**:
1. å…ˆæŸ¥çœ‹å¥åº·æ£€æŸ¥: `curl http://localhost:8000/health`
2. æ£€æŸ¥æ—¥å¿—: `curl http://localhost:8000/v1/logs/recent`
3. éªŒè¯é…ç½®: `curl http://localhost:8000/v1/config`
4. é‡å¯æœåŠ¡: `python run.py --mode dev`