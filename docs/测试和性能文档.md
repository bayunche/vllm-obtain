# ğŸ§ª VLLM è·¨å¹³å°æ¨ç†æœåŠ¡ - æµ‹è¯•å’Œæ€§èƒ½æ–‡æ¡£

## ğŸ“‹ ç›®å½•

- [æµ‹è¯•æ¡†æ¶](#æµ‹è¯•æ¡†æ¶)
- [å•å…ƒæµ‹è¯•](#å•å…ƒæµ‹è¯•)
- [é›†æˆæµ‹è¯•](#é›†æˆæµ‹è¯•)
- [OpenAI API å…¼å®¹æ€§æµ‹è¯•](#openai-api-å…¼å®¹æ€§æµ‹è¯•)
- [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)
- [å‹åŠ›æµ‹è¯•](#å‹åŠ›æµ‹è¯•)
- [è·¨å¹³å°æµ‹è¯•](#è·¨å¹³å°æµ‹è¯•)
- [è‡ªåŠ¨åŒ–æµ‹è¯•](#è‡ªåŠ¨åŒ–æµ‹è¯•)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [ç›‘æ§å’Œåˆ†æ](#ç›‘æ§å’Œåˆ†æ)

---

## ğŸ§ª æµ‹è¯•æ¡†æ¶

### æµ‹è¯•ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install pytest pytest-cov pytest-asyncio pytest-mock
pip install locust  # å‹åŠ›æµ‹è¯•
pip install httpx   # å¼‚æ­¥ HTTP å®¢æˆ·ç«¯

# åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„
mkdir -p tests/{unit,integration,performance,compatibility}
```

### æµ‹è¯•é…ç½®

```python
# tests/conftest.py
import pytest
import asyncio
from src.utils import get_config, setup_logger

@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def test_config():
    """æµ‹è¯•é…ç½®"""
    return get_config()

@pytest.fixture
def test_logger():
    """æµ‹è¯•æ—¥å¿—å™¨"""
    return setup_logger()
```

---

## ğŸ”¬ å•å…ƒæµ‹è¯•

### å¹³å°æ£€æµ‹å™¨æµ‹è¯•

```python
# tests/unit/test_platform_detector.py
import pytest
from src.utils.platform_detector import PlatformDetector

class TestPlatformDetector:
    
    def test_get_platform_info(self):
        """æµ‹è¯•å¹³å°ä¿¡æ¯è·å–"""
        detector = PlatformDetector()
        info = detector.get_platform_info()
        
        assert 'system' in info
        assert 'machine' in info
        assert info['system'] in ['linux', 'darwin', 'windows']
    
    def test_detect_best_engine(self):
        """æµ‹è¯•æœ€ä½³å¼•æ“æ£€æµ‹"""
        detector = PlatformDetector()
        engine = detector.detect_best_engine()
        
        assert engine in ['vllm', 'mlx', 'llama_cpp']
    
    def test_force_engine_selection(self):
        """æµ‹è¯•å¼ºåˆ¶å¼•æ“é€‰æ‹©"""
        detector = PlatformDetector()
        engine = detector.detect_best_engine(force_engine='llama_cpp')
        
        assert engine == 'llama_cpp'
```

### é…ç½®ç®¡ç†æµ‹è¯•

```python
# tests/unit/test_config.py
import pytest
import os
from src.utils.config import ConfigManager, InferenceConfig

class TestConfigManager:
    
    def test_load_default_config(self):
        """æµ‹è¯•é»˜è®¤é…ç½®åŠ è½½"""
        config = ConfigManager.load_config()
        
        assert isinstance(config, InferenceConfig)
        assert config.host == "0.0.0.0"
        assert config.port == 8000
    
    def test_environment_variable_override(self):
        """æµ‹è¯•ç¯å¢ƒå˜é‡è¦†ç›–"""
        os.environ['HOST'] = '127.0.0.1'
        os.environ['PORT'] = '9000'
        
        config = ConfigManager.load_config()
        
        assert config.host == '127.0.0.1'
        assert config.port == 9000
        
        # æ¸…ç†ç¯å¢ƒå˜é‡
        del os.environ['HOST']
        del os.environ['PORT']
    
    def test_config_validation(self):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        with pytest.raises(ValueError):
            InferenceConfig(inference_engine='invalid_engine')
```

### æ¨ç†å¼•æ“æµ‹è¯•

```python
# tests/unit/test_inference_engine.py
import pytest
import asyncio
from src.core.inference_engine import DummyEngine, EngineConfig, InferenceRequest

class TestInferenceEngine:
    
    @pytest.mark.asyncio
    async def test_dummy_engine_initialization(self):
        """æµ‹è¯•è™šæ‹Ÿå¼•æ“åˆå§‹åŒ–"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        
        success = await engine.initialize()
        assert success is True
        assert engine._initialized is True
    
    @pytest.mark.asyncio
    async def test_model_loading(self):
        """æµ‹è¯•æ¨¡å‹åŠ è½½"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        await engine.initialize()
        
        success = await engine.load_model("test-model", "/fake/path")
        assert success is True
        assert engine.is_model_loaded("test-model") is True
    
    @pytest.mark.asyncio
    async def test_inference(self):
        """æµ‹è¯•æ¨ç†åŠŸèƒ½"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        await engine.initialize()
        await engine.load_model("test-model", "/fake/path")
        
        request = InferenceRequest(
            model_name="test-model",
            prompt="Hello, world!",
            max_tokens=50
        )
        
        response = await engine.generate(request)
        
        assert response.model_name == "test-model"
        assert response.text is not None
        assert response.prompt_tokens > 0
        assert response.completion_tokens > 0
```

---

## ğŸ”— é›†æˆæµ‹è¯•

### API é›†æˆæµ‹è¯•

```python
# tests/integration/test_api_integration.py
import pytest
import asyncio
from httpx import AsyncClient
from src.api.app import create_app

class TestAPIIntegration:
    
    @pytest.fixture
    async def client(self):
        """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
        app = create_app()
        async with AsyncClient(app=app, base_url="http://test") as client:
            yield client
    
    @pytest.mark.asyncio
    async def test_health_check(self, client):
        """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
        response = await client.get("/health")
        assert response.status_code == 200
        
        data = response.json()
        assert "status" in data
    
    @pytest.mark.asyncio
    async def test_models_endpoint(self, client):
        """æµ‹è¯•æ¨¡å‹åˆ—è¡¨æ¥å£"""
        response = await client.get("/v1/models")
        assert response.status_code == 200
        
        data = response.json()
        assert data["object"] == "list"
        assert "data" in data
    
    @pytest.mark.asyncio
    async def test_chat_completions(self, client):
        """æµ‹è¯•èŠå¤©è¡¥å…¨æ¥å£"""
        payload = {
            "model": "test-model",
            "messages": [{"role": "user", "content": "Hello!"}],
            "max_tokens": 50
        }
        
        response = await client.post("/v1/chat/completions", json=payload)
        
        if response.status_code == 200:
            data = response.json()
            assert data["object"] == "chat.completion"
            assert "choices" in data
            assert "usage" in data
```

---

## âœ… OpenAI API å…¼å®¹æ€§æµ‹è¯•

### æ‰©å±•å…¼å®¹æ€§æµ‹è¯•è„šæœ¬

```python
# tests/compatibility/test_openai_full_compatibility.py
import pytest
import json
import time
from openai import OpenAI

class TestOpenAICompatibility:
    
    @pytest.fixture
    def client(self):
        """OpenAI å®¢æˆ·ç«¯"""
        return OpenAI(
            api_key="test-key",
            base_url="http://localhost:8000/v1"
        )
    
    def test_models_list(self, client):
        """æµ‹è¯•æ¨¡å‹åˆ—è¡¨å…¼å®¹æ€§"""
        models = client.models.list()
        
        assert hasattr(models, 'data')
        for model in models.data:
            assert hasattr(model, 'id')
            assert hasattr(model, 'object')
            assert model.object == 'model'
    
    def test_chat_completion(self, client):
        """æµ‹è¯•èŠå¤©è¡¥å…¨å…¼å®¹æ€§"""
        response = client.chat.completions.create(
            model="test-model",
            messages=[
                {"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}
            ],
            max_tokens=50
        )
        
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert hasattr(response.choices[0], 'message')
        assert hasattr(response.choices[0].message, 'content')
        assert hasattr(response, 'usage')
    
    def test_streaming_chat(self, client):
        """æµ‹è¯•æµå¼èŠå¤©å…¼å®¹æ€§"""
        stream = client.chat.completions.create(
            model="test-model",
            messages=[{"role": "user", "content": "æµ‹è¯•æµå¼"}],
            stream=True
        )
        
        chunks = []
        for chunk in stream:
            chunks.append(chunk)
            if len(chunks) > 10:  # é™åˆ¶æµ‹è¯•é•¿åº¦
                break
        
        assert len(chunks) > 0
        # éªŒè¯æµå¼æ•°æ®æ ¼å¼
        for chunk in chunks:
            assert hasattr(chunk, 'choices')
```

---

## ğŸš€ æ€§èƒ½æµ‹è¯•

### åŸºç¡€æ€§èƒ½æµ‹è¯•

```python
# tests/performance/test_basic_performance.py
import pytest
import time
import asyncio
from httpx import AsyncClient
from statistics import mean, median

class TestBasicPerformance:
    
    @pytest.mark.asyncio
    async def test_response_time(self):
        """æµ‹è¯•å“åº”æ—¶é—´"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            response_times = []
            
            for _ in range(10):
                start_time = time.time()
                response = await client.get("/health")
                end_time = time.time()
                
                assert response.status_code == 200
                response_times.append(end_time - start_time)
            
            avg_time = mean(response_times)
            median_time = median(response_times)
            
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
            print(f"ä¸­ä½æ•°å“åº”æ—¶é—´: {median_time:.3f}s")
            
            # å¥åº·æ£€æŸ¥åº”è¯¥åœ¨100mså†…å®Œæˆ
            assert avg_time < 0.1
    
    @pytest.mark.asyncio
    async def test_inference_performance(self):
        """æµ‹è¯•æ¨ç†æ€§èƒ½"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            payload = {
                "model": "test-model",
                "messages": [{"role": "user", "content": "ç®€çŸ­å›å¤æµ‹è¯•"}],
                "max_tokens": 20
            }
            
            inference_times = []
            
            for _ in range(5):
                start_time = time.time()
                response = await client.post("/v1/chat/completions", json=payload)
                end_time = time.time()
                
                if response.status_code == 200:
                    data = response.json()
                    inference_time = end_time - start_time
                    tokens_generated = data['usage']['completion_tokens']
                    tokens_per_second = tokens_generated / inference_time
                    
                    inference_times.append(tokens_per_second)
                    print(f"æ¨ç†é€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
            
            if inference_times:
                avg_speed = mean(inference_times)
                print(f"å¹³å‡æ¨ç†é€Ÿåº¦: {avg_speed:.1f} tokens/s")
```

### å†…å­˜ä½¿ç”¨æµ‹è¯•

```python
# tests/performance/test_memory_usage.py
import pytest
import psutil
import os
import time

class TestMemoryUsage:
    
    def test_memory_baseline(self):
        """æµ‹è¯•å†…å­˜åŸºçº¿"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f} MB")
        
        # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
        time.sleep(1)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.1f} MB")
        print(f"å†…å­˜å¢é•¿: {memory_increase:.1f} MB")
        
        # å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡100MB
        assert memory_increase < 100
    
    def test_gpu_memory_usage(self):
        """æµ‹è¯•GPUå†…å­˜ä½¿ç”¨"""
        try:
            import torch
            if torch.cuda.is_available():
                initial_gpu = torch.cuda.memory_allocated() / 1024**3  # GB
                print(f"åˆå§‹GPUå†…å­˜: {initial_gpu:.2f} GB")
                
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹åŠ è½½æµ‹è¯•
                # æš‚æ—¶è·³è¿‡å®é™…GPUæµ‹è¯•
                assert True
            else:
                pytest.skip("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
        except ImportError:
            pytest.skip("PyTorchæœªå®‰è£…ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
```

---

## ğŸ’¥ å‹åŠ›æµ‹è¯•

### Locust å‹åŠ›æµ‹è¯•è„šæœ¬

```python
# tests/stress/locustfile.py
from locust import HttpUser, task, between
import json
import random

class InferenceUser(HttpUser):
    wait_time = between(1, 3)  # è¯·æ±‚é—´éš”1-3ç§’
    
    def on_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶æ‰§è¡Œ"""
        # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
        response = self.client.get("/health")
        if response.status_code != 200:
            print("æœåŠ¡ä¸å¯ç”¨ï¼Œåœæ­¢æµ‹è¯•")
            self.environment.runner.quit()
    
    @task(3)
    def test_health_check(self):
        """å¥åº·æ£€æŸ¥ (æƒé‡3)"""
        self.client.get("/health")
    
    @task(5)
    def test_models_list(self):
        """æ¨¡å‹åˆ—è¡¨ (æƒé‡5)"""
        self.client.get("/v1/models")
    
    @task(10)
    def test_chat_completion(self):
        """èŠå¤©è¡¥å…¨ (æƒé‡10)"""
        messages = [
            "ä½ å¥½",
            "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            "å†™ä¸€ä¸ªPythonå‡½æ•°",
            "è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        ]
        
        payload = {
            "model": "test-model",
            "messages": [{"role": "user", "content": random.choice(messages)}],
            "max_tokens": random.randint(20, 100),
            "temperature": random.uniform(0.1, 0.9)
        }
        
        with self.client.post(
            "/v1/chat/completions",
            json=payload,
            catch_response=True
        ) as response:
            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    response.success()
                else:
                    response.failure("å“åº”æ ¼å¼é”™è¯¯")
            else:
                response.failure(f"çŠ¶æ€ç : {response.status_code}")
    
    @task(2)
    def test_system_status(self):
        """ç³»ç»ŸçŠ¶æ€ (æƒé‡2)"""
        self.client.get("/v1/system/status")

class ConcurrentUser(HttpUser):
    """é«˜å¹¶å‘ç”¨æˆ·"""
    wait_time = between(0.1, 0.5)  # æ›´çŸ­çš„ç­‰å¾…æ—¶é—´
    
    @task
    def rapid_requests(self):
        """å¿«é€Ÿè¯·æ±‚"""
        endpoints = ["/health", "/v1/models", "/v1/system/status"]
        endpoint = random.choice(endpoints)
        self.client.get(endpoint)
```

### å‹åŠ›æµ‹è¯•è¿è¡Œè„šæœ¬

```bash
# tests/stress/run_stress_test.sh
#!/bin/bash

echo "ğŸš€ å¼€å§‹ VLLM æ¨ç†æœåŠ¡å‹åŠ›æµ‹è¯•"

# ç¡®ä¿æœåŠ¡è¿è¡Œ
echo "æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
if ! curl -s http://localhost:8000/health > /dev/null; then
    echo "âŒ æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡"
    exit 1
fi

echo "âœ… æœåŠ¡è¿è¡Œæ­£å¸¸"

# åˆ›å»ºç»“æœç›®å½•
mkdir -p stress_test_results

# åŸºç¡€å‹åŠ›æµ‹è¯• (10ä¸ªç”¨æˆ·ï¼Œ60ç§’)
echo "ğŸ“Š è¿è¡ŒåŸºç¡€å‹åŠ›æµ‹è¯•..."
locust -f tests/stress/locustfile.py \
    --users 10 \
    --spawn-rate 2 \
    --run-time 60s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/basic_test.html \
    --csv stress_test_results/basic_test

# ä¸­ç­‰å‹åŠ›æµ‹è¯• (50ä¸ªç”¨æˆ·ï¼Œ120ç§’)
echo "ğŸ“Š è¿è¡Œä¸­ç­‰å‹åŠ›æµ‹è¯•..."
locust -f tests/stress/locustfile.py \
    --users 50 \
    --spawn-rate 5 \
    --run-time 120s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/medium_test.html \
    --csv stress_test_results/medium_test

# é«˜å‹åŠ›æµ‹è¯• (100ä¸ªç”¨æˆ·ï¼Œ180ç§’)
echo "ğŸ“Š è¿è¡Œé«˜å‹åŠ›æµ‹è¯•..."
locust -f tests/stress/locustfile.py \
    --users 100 \
    --spawn-rate 10 \
    --run-time 180s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/high_test.html \
    --csv stress_test_results/high_test

echo "ğŸ‰ å‹åŠ›æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜åœ¨ stress_test_results/ ç›®å½•"
```

### å‹åŠ›æµ‹è¯•åˆ†æè„šæœ¬

```python
# tests/stress/analyze_results.py
import pandas as pd
import matplotlib.pyplot as plt
import argparse

def analyze_stress_test(csv_file):
    """åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ"""
    
    # è¯»å–æµ‹è¯•æ•°æ®
    stats_df = pd.read_csv(f"{csv_file}_stats.csv")
    history_df = pd.read_csv(f"{csv_file}_stats_history.csv")
    
    print("ğŸ“Š å‹åŠ›æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 50)
    
    # æ•´ä½“ç»Ÿè®¡
    print("ğŸ¯ æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
    print(f"æ€»è¯·æ±‚æ•°: {stats_df['Request Count'].sum()}")
    print(f"å¤±è´¥è¯·æ±‚æ•°: {stats_df['Failure Count'].sum()}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {stats_df['Average Response Time'].mean():.2f}ms")
    print(f"95%å“åº”æ—¶é—´: {stats_df['95%'].mean():.2f}ms")
    print(f"99%å“åº”æ—¶é—´: {stats_df['99%'].mean():.2f}ms")
    print(f"è¯·æ±‚å¤±è´¥ç‡: {(stats_df['Failure Count'].sum() / stats_df['Request Count'].sum() * 100):.2f}%")
    
    # å„ç«¯ç‚¹æ€§èƒ½
    print("\nğŸ” å„ç«¯ç‚¹æ€§èƒ½:")
    for _, row in stats_df.iterrows():
        print(f"{row['Name']}: {row['Average Response Time']:.2f}ms (RPS: {row['Requests/s']:.2f})")
    
    # ç”Ÿæˆå›¾è¡¨
    plt.figure(figsize=(15, 10))
    
    # å“åº”æ—¶é—´è¶‹åŠ¿
    plt.subplot(2, 2, 1)
    plt.plot(history_df['Timestamp'], history_df['95%'])
    plt.title('95%å“åº”æ—¶é—´è¶‹åŠ¿')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å“åº”æ—¶é—´ (ms)')
    
    # RPSè¶‹åŠ¿
    plt.subplot(2, 2, 2)
    plt.plot(history_df['Timestamp'], history_df['Requests/s'])
    plt.title('æ¯ç§’è¯·æ±‚æ•° (RPS) è¶‹åŠ¿')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('RPS')
    
    # ç”¨æˆ·æ•°è¶‹åŠ¿
    plt.subplot(2, 2, 3)
    plt.plot(history_df['Timestamp'], history_df['User Count'])
    plt.title('å¹¶å‘ç”¨æˆ·æ•°è¶‹åŠ¿')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('ç”¨æˆ·æ•°')
    
    # å¤±è´¥ç‡è¶‹åŠ¿
    plt.subplot(2, 2, 4)
    failure_rate = (history_df['Failures/s'] / history_df['Requests/s'] * 100).fillna(0)
    plt.plot(history_df['Timestamp'], failure_rate)
    plt.title('å¤±è´¥ç‡è¶‹åŠ¿')
    plt.xlabel('æ—¶é—´')
    plt.ylabel('å¤±è´¥ç‡ (%)')
    
    plt.tight_layout()
    plt.savefig(f"{csv_file}_analysis.png", dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ å›¾è¡¨å·²ä¿å­˜ä¸º {csv_file}_analysis.png")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ')
    parser.add_argument('csv_file', help='CSVæ–‡ä»¶è·¯å¾„å‰ç¼€')
    args = parser.parse_args()
    
    analyze_stress_test(args.csv_file)
```

---

## ğŸŒ è·¨å¹³å°æµ‹è¯•

### å¹³å°å…¼å®¹æ€§æµ‹è¯•çŸ©é˜µ

```python
# tests/platform/test_cross_platform.py
import pytest
import platform
import subprocess

class TestCrossPlatform:
    
    def test_platform_detection(self):
        """æµ‹è¯•å¹³å°æ£€æµ‹å‡†ç¡®æ€§"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        platform_info = detector.get_platform_info()
        
        # éªŒè¯å¹³å°ä¿¡æ¯
        expected_system = platform.system().lower()
        assert platform_info['system'] == expected_system
    
    @pytest.mark.skipif(platform.system() != "Darwin", reason="ä»…åœ¨macOSä¸Šè¿è¡Œ")
    def test_mlx_availability_macos(self):
        """æµ‹è¯•macOSä¸ŠMLXå¯ç”¨æ€§"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        
        if platform.machine().lower() in ['arm64', 'aarch64']:
            # Apple Silicon åº”è¯¥æ¨èMLX
            engine = detector.detect_best_engine()
            assert engine in ['mlx', 'llama_cpp']  # MLXæˆ–å›é€€åˆ°llama_cpp
    
    @pytest.mark.skipif(platform.system() == "Darwin", reason="émacOSå¹³å°è¿è¡Œ")
    def test_vllm_availability_linux_windows(self):
        """æµ‹è¯•Linux/Windowsä¸ŠVLLMå¯ç”¨æ€§"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        engine = detector.detect_best_engine()
        
        # Linux/Windowsåº”è¯¥ä¼˜å…ˆé€‰æ‹©VLLMæˆ–å›é€€åˆ°llama_cpp
        assert engine in ['vllm', 'llama_cpp']
    
    def test_python_version_compatibility(self):
        """æµ‹è¯•Pythonç‰ˆæœ¬å…¼å®¹æ€§"""
        import sys
        
        # è¦æ±‚Python 3.11+
        assert sys.version_info >= (3, 11), f"Pythonç‰ˆæœ¬ {sys.version} ä¸å—æ”¯æŒ"
    
    def test_dependencies_installation(self):
        """æµ‹è¯•ä¾èµ–å®‰è£…"""
        required_packages = [
            'flask', 'loguru', 'pydantic', 'psutil', 'requests'
        ]
        
        for package in required_packages:
            try:
                __import__(package.replace('-', '_'))
            except ImportError:
                pytest.fail(f"å¿…éœ€åŒ… {package} æœªå®‰è£…")
```

---

## ğŸ¤– è‡ªåŠ¨åŒ–æµ‹è¯•

### GitHub Actions å·¥ä½œæµ

```yaml
# .github/workflows/test.yml
name: æµ‹è¯•å’Œè´¨é‡æ£€æŸ¥

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    
    - name: è¿è¡Œå•å…ƒæµ‹è¯•
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml
    
    - name: è¿è¡Œé›†æˆæµ‹è¯•
      run: |
        pytest tests/integration/ -v
    
    - name: ä¸Šä¼ è¦†ç›–ç‡æŠ¥å‘Š
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
  
  performance:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Python
      uses: actions/setup-python@v3
      with:
        python-version: 3.11
    
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install locust
    
    - name: å¯åŠ¨æœåŠ¡
      run: |
        python run.py --mode dev &
        sleep 30
    
    - name: è¿è¡Œæ€§èƒ½æµ‹è¯•
      run: |
        pytest tests/performance/ -v
    
    - name: è¿è¡Œå‹åŠ›æµ‹è¯•
      run: |
        locust -f tests/stress/locustfile.py \
          --users 10 --spawn-rate 2 --run-time 60s \
          --host http://localhost:8000 --headless
```

### æœ¬åœ°æµ‹è¯•è„šæœ¬

```bash
# scripts/run_all_tests.sh
#!/bin/bash

set -e

echo "ğŸ§ª è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "âš ï¸ è¯·å…ˆæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ"
    exit 1
fi

echo "ğŸ“‹ å®‰è£…æµ‹è¯•ä¾èµ–..."
pip install pytest pytest-cov pytest-asyncio pytest-mock locust

echo "ğŸ”¬ è¿è¡Œå•å…ƒæµ‹è¯•..."
pytest tests/unit/ -v --cov=src --cov-report=html --cov-report=term

echo "ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•..."
pytest tests/integration/ -v

echo "ğŸŒ è¿è¡Œè·¨å¹³å°æµ‹è¯•..."
pytest tests/platform/ -v

echo "âœ… è¿è¡ŒOpenAIå…¼å®¹æ€§æµ‹è¯•..."
python test_openai_compatibility.py

echo "ğŸš€ å¯åŠ¨æœåŠ¡è¿›è¡Œæ€§èƒ½æµ‹è¯•..."
python run.py --mode dev --skip-check &
SERVER_PID=$!
sleep 10

echo "ğŸ“Š è¿è¡Œæ€§èƒ½æµ‹è¯•..."
pytest tests/performance/ -v

echo "ğŸ’¥ è¿è¡Œå‹åŠ›æµ‹è¯•..."
locust -f tests/stress/locustfile.py \
    --users 20 --spawn-rate 4 --run-time 60s \
    --host http://localhost:8000 --headless \
    --html test_results/stress_test.html

echo "ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ..."
kill $SERVER_PID 2>/dev/null || true

echo "ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼"
echo "ğŸ“Š æŸ¥çœ‹è¦†ç›–ç‡æŠ¥å‘Š: htmlcov/index.html"
echo "ğŸ“ˆ æŸ¥çœ‹å‹åŠ›æµ‹è¯•æŠ¥å‘Š: test_results/stress_test.html"
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### åŸºå‡†æµ‹è¯•é…ç½®

```python
# benchmarks/benchmark_config.py
BENCHMARK_CONFIGS = {
    "small_model": {
        "model_size": "7B",
        "test_prompts": [
            "ä½ å¥½",
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹AI",
            "1+1ç­‰äºå‡ ï¼Ÿ"
        ],
        "max_tokens": 50,
        "expected_speed": 20  # tokens/s
    },
    "medium_model": {
        "model_size": "13B", 
        "test_prompts": [
            "è¯·è¯¦ç»†è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",
            "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•",
            "åˆ†æå½“å‰AIæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿"
        ],
        "max_tokens": 200,
        "expected_speed": 10
    },
    "large_model": {
        "model_size": "70B",
        "test_prompts": [
            "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„è¯¦ç»†æ–‡ç« ",
            "è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„Webåº”ç”¨æ¶æ„",
            "åˆ†æå…¨çƒæ°”å€™å˜åŒ–çš„å½±å“å’Œåº”å¯¹æªæ–½"
        ],
        "max_tokens": 500,
        "expected_speed": 5
    }
}

HARDWARE_CONFIGS = {
    "cpu_only": {
        "device": "cpu",
        "engine": "llama_cpp",
        "memory_limit": "16GB"
    },
    "gpu_consumer": {
        "device": "cuda",
        "engine": "vllm", 
        "gpu_memory": "RTX 4090 24GB"
    },
    "gpu_enterprise": {
        "device": "cuda",
        "engine": "vllm",
        "gpu_memory": "A100 80GB"
    },
    "apple_silicon": {
        "device": "mps",
        "engine": "mlx",
        "unified_memory": "64GB+"
    }
}
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# benchmarks/run_benchmarks.py
import asyncio
import time
import json
from statistics import mean, median
from httpx import AsyncClient

async def benchmark_inference_speed():
    """åŸºå‡†æ¨ç†é€Ÿåº¦æµ‹è¯•"""
    
    test_cases = [
        {"prompt": "ä½ å¥½", "max_tokens": 10, "category": "short"},
        {"prompt": "è¯·ä»‹ç»æœºå™¨å­¦ä¹ ", "max_tokens": 100, "category": "medium"},
        {"prompt": "è¯¦ç»†åˆ†æAIå‘å±•è¶‹åŠ¿", "max_tokens": 300, "category": "long"}
    ]
    
    results = {}
    
    async with AsyncClient(base_url="http://localhost:8000") as client:
        for case in test_cases:
            category = case["category"]
            speeds = []
            
            print(f"æµ‹è¯• {category} ç±»åˆ«...")
            
            for i in range(5):  # æ¯ä¸ªç±»åˆ«æµ‹è¯•5æ¬¡
                payload = {
                    "model": "test-model",
                    "messages": [{"role": "user", "content": case["prompt"]}],
                    "max_tokens": case["max_tokens"]
                }
                
                start_time = time.time()
                response = await client.post("/v1/chat/completions", json=payload)
                end_time = time.time()
                
                if response.status_code == 200:
                    data = response.json()
                    inference_time = end_time - start_time
                    completion_tokens = data['usage']['completion_tokens']
                    speed = completion_tokens / inference_time
                    speeds.append(speed)
                    
                    print(f"  ç¬¬{i+1}æ¬¡: {speed:.1f} tokens/s")
            
            if speeds:
                results[category] = {
                    "avg_speed": mean(speeds),
                    "median_speed": median(speeds),
                    "min_speed": min(speeds),
                    "max_speed": max(speeds),
                    "samples": len(speeds)
                }
    
    return results

async def benchmark_concurrent_requests():
    """å¹¶å‘è¯·æ±‚åŸºå‡†æµ‹è¯•"""
    
    concurrent_levels = [1, 5, 10, 20, 50]
    results = {}
    
    for concurrency in concurrent_levels:
        print(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
        
        async def single_request(client, request_id):
            payload = {
                "model": "test-model",
                "messages": [{"role": "user", "content": f"è¯·æ±‚ {request_id}"}],
                "max_tokens": 50
            }
            
            start_time = time.time()
            response = await client.post("/v1/chat/completions", json=payload)
            end_time = time.time()
            
            return {
                "request_id": request_id,
                "status_code": response.status_code,
                "response_time": end_time - start_time,
                "success": response.status_code == 200
            }
        
        async with AsyncClient(base_url="http://localhost:8000") as client:
            start_time = time.time()
            
            tasks = [
                single_request(client, i) 
                for i in range(concurrency)
            ]
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # åˆ†æç»“æœ
            successful_responses = [r for r in responses if isinstance(r, dict) and r['success']]
            failed_responses = [r for r in responses if not (isinstance(r, dict) and r['success'])]
            
            if successful_responses:
                avg_response_time = mean([r['response_time'] for r in successful_responses])
                throughput = len(successful_responses) / total_time
            else:
                avg_response_time = 0
                throughput = 0
            
            results[concurrency] = {
                "total_requests": concurrency,
                "successful_requests": len(successful_responses),
                "failed_requests": len(failed_responses),
                "success_rate": len(successful_responses) / concurrency * 100,
                "avg_response_time": avg_response_time,
                "throughput": throughput,
                "total_time": total_time
            }
            
            print(f"  æˆåŠŸç‡: {results[concurrency]['success_rate']:.1f}%")
            print(f"  ååé‡: {throughput:.1f} req/s")
    
    return results

def save_benchmark_results(results, filename):
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    print("\nğŸ“Š æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•...")
    speed_results = await benchmark_inference_speed()
    
    # å¹¶å‘è¯·æ±‚æµ‹è¯•
    print("\nğŸ“Š å¹¶å‘è¯·æ±‚åŸºå‡†æµ‹è¯•...")
    concurrent_results = await benchmark_concurrent_requests()
    
    # ä¿å­˜ç»“æœ
    all_results = {
        "timestamp": time.time(),
        "inference_speed": speed_results,
        "concurrent_requests": concurrent_results
    }
    
    save_benchmark_results(all_results, f"benchmark_results_{int(time.time())}.json")
    
    # æ‰“å°æ€»ç»“
    print("\nğŸ“ˆ åŸºå‡†æµ‹è¯•æ€»ç»“:")
    print("=" * 50)
    
    for category, data in speed_results.items():
        print(f"{category}: {data['avg_speed']:.1f} tokens/s (å¹³å‡)")
    
    print("\nå¹¶å‘æ€§èƒ½:")
    for concurrency, data in concurrent_results.items():
        print(f"{concurrency} å¹¶å‘: {data['throughput']:.1f} req/s ({data['success_rate']:.1f}% æˆåŠŸç‡)")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“ˆ ç›‘æ§å’Œåˆ†æ

### å®æ—¶æ€§èƒ½ç›‘æ§

```python
# monitoring/performance_monitor.py
import time
import json
import psutil
import threading
from datetime import datetime
from collections import deque

class PerformanceMonitor:
    
    def __init__(self, window_size=300):  # 5åˆ†é’Ÿçª—å£
        self.window_size = window_size
        self.metrics_history = deque(maxlen=window_size)
        self.running = False
        self.thread = None
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.running = True
        self.thread = threading.Thread(target=self._monitoring_loop)
        self.thread.daemon = True
        self.thread.start()
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.thread:
            self.thread.join()
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            metrics = self._collect_metrics()
            self.metrics_history.append(metrics)
            time.sleep(1)  # æ¯ç§’æ”¶é›†ä¸€æ¬¡
    
    def _collect_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_used_gb": memory.used / 1024**3,
            "memory_total_gb": memory.total / 1024**3,
            "disk_percent": (disk.used / disk.total) * 100,
            "disk_free_gb": disk.free / 1024**3
        }
        
        # æ·»åŠ GPUæŒ‡æ ‡ (å¦‚æœå¯ç”¨)
        try:
            import torch
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024**3
                gpu_cached = torch.cuda.memory_reserved() / 1024**3
                metrics.update({
                    "gpu_memory_used_gb": gpu_memory,
                    "gpu_memory_cached_gb": gpu_cached
                })
        except ImportError:
            pass
        
        return metrics
    
    def get_current_metrics(self):
        """è·å–å½“å‰æŒ‡æ ‡"""
        if self.metrics_history:
            return self.metrics_history[-1]
        return self._collect_metrics()
    
    def get_average_metrics(self, minutes=5):
        """è·å–å¹³å‡æŒ‡æ ‡"""
        if not self.metrics_history:
            return None
        
        # è·å–æœ€è¿‘Nåˆ†é’Ÿçš„æ•°æ®
        recent_data = list(self.metrics_history)[-minutes*60:]
        
        if not recent_data:
            return None
        
        avg_metrics = {}
        numeric_keys = ['cpu_percent', 'memory_percent', 'disk_percent']
        
        for key in numeric_keys:
            values = [m[key] for m in recent_data if key in m]
            if values:
                avg_metrics[f"avg_{key}"] = sum(values) / len(values)
        
        return avg_metrics
    
    def export_metrics(self, filename):
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®"""
        data = {
            "export_time": datetime.now().isoformat(),
            "metrics": list(self.metrics_history)
        }
        
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"æŒ‡æ ‡æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    try:
        # è¿è¡Œä¸€æ®µæ—¶é—´
        time.sleep(60)
        
        # è·å–å½“å‰æŒ‡æ ‡
        current = monitor.get_current_metrics()
        print("å½“å‰æŒ‡æ ‡:", current)
        
        # è·å–å¹³å‡æŒ‡æ ‡
        average = monitor.get_average_metrics()
        print("å¹³å‡æŒ‡æ ‡:", average)
        
    finally:
        monitor.stop_monitoring()
        monitor.export_metrics("performance_data.json")
```

---

## ğŸ¯ æµ‹è¯•æ‰§è¡ŒæŒ‡å—

### æ—¥å¸¸å¼€å‘æµ‹è¯•

```bash
# å¿«é€Ÿæµ‹è¯• (å¼€å‘æ—¶ä½¿ç”¨)
pytest tests/unit/ -v

# å®Œæ•´å•å…ƒæµ‹è¯•
pytest tests/unit/ -v --cov=src --cov-report=html

# é›†æˆæµ‹è¯•
pytest tests/integration/ -v
```

### å‘å¸ƒå‰æµ‹è¯•

```bash
# å®Œæ•´æµ‹è¯•å¥—ä»¶
./scripts/run_all_tests.sh

# OpenAIå…¼å®¹æ€§éªŒè¯
python test_openai_compatibility.py

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python benchmarks/run_benchmarks.py
```

### ç”Ÿäº§ç¯å¢ƒç›‘æ§

```bash
# å‹åŠ›æµ‹è¯•
locust -f tests/stress/locustfile.py \
    --users 100 --spawn-rate 10 --run-time 300s \
    --host http://your-production-server:8000 \
    --headless --html production_stress_test.html

# é•¿æœŸç›‘æ§
python monitoring/performance_monitor.py
```

---

## ğŸ“‹ æµ‹è¯•æ£€æŸ¥æ¸…å•

### âœ… å‘å¸ƒå‰æ£€æŸ¥

- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] OpenAI API å…¼å®¹æ€§æµ‹è¯•é€šè¿‡
- [ ] è·¨å¹³å°æµ‹è¯•é€šè¿‡ (è‡³å°‘2ä¸ªå¹³å°)
- [ ] æ€§èƒ½æµ‹è¯•è¾¾æ ‡ (å“åº”æ—¶é—´ < 3s)
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡ (100å¹¶å‘ï¼ŒæˆåŠŸç‡ > 95%)
- [ ] å†…å­˜ä½¿ç”¨æ­£å¸¸ (æ— æ˜æ˜¾å†…å­˜æ³„æ¼)
- [ ] æ–‡æ¡£æ›´æ–°å®Œæ•´

### ğŸ¯ æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | éªŒè¯æ–¹æ³• |
|------|--------|----------|
| å“åº”æ—¶é—´ | < 3ç§’ | æ€§èƒ½æµ‹è¯• |
| ååé‡ | > 50 req/min | å‹åŠ›æµ‹è¯• |
| æˆåŠŸç‡ | > 95% | å‹åŠ›æµ‹è¯• |
| å†…å­˜ä½¿ç”¨ | ç¨³å®š | é•¿æœŸç›‘æ§ |
| CPUä½¿ç”¨ | < 80% | ç³»ç»Ÿç›‘æ§ |

è¿™ä»½æµ‹è¯•æ–‡æ¡£æä¾›äº†å®Œæ•´çš„æµ‹è¯•ç­–ç•¥ï¼Œä»å•å…ƒæµ‹è¯•åˆ°å‹åŠ›æµ‹è¯•ï¼Œç¡®ä¿æœåŠ¡çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚