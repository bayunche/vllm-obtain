#!/usr/bin/env python3
"""
å…¨é¢æµ‹è¯•è„šæœ¬ - ç”Ÿäº§çº§æµ‹è¯•
æµ‹è¯•å¹¶å‘æ€§èƒ½ã€Tokenç”Ÿæˆé€Ÿåº¦ã€ç³»ç»Ÿèµ„æºä½¿ç”¨
"""

import requests
import time
import json
import threading
import concurrent.futures
import psutil
import os
from datetime import datetime
from collections import defaultdict
import statistics

BASE_URL = "http://127.0.0.1:8001"
API_BASE = f"{BASE_URL}/v1"

class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.cpu_samples = []
        self.memory_samples = []
        self.monitoring = False
        self.start_time = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.start_time = time.time()
        
        def monitor():
            while self.monitoring:
                cpu = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                
                self.cpu_samples.append({
                    'timestamp': time.time() - self.start_time,
                    'cpu_percent': cpu,
                    'memory_percent': memory.percent,
                    'memory_used_gb': memory.used / 1024**3,
                    'memory_available_gb': memory.available / 1024**3
                })
                
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=2)
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.cpu_samples:
            return {}
            
        cpu_values = [s['cpu_percent'] for s in self.cpu_samples]
        memory_values = [s['memory_percent'] for s in self.cpu_samples]
        memory_used_values = [s['memory_used_gb'] for s in self.cpu_samples]
        
        return {
            'duration': time.time() - self.start_time if self.start_time else 0,
            'cpu': {
                'avg': statistics.mean(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            },
            'memory': {
                'avg_percent': statistics.mean(memory_values),
                'max_percent': max(memory_values),
                'avg_used_gb': statistics.mean(memory_used_values),
                'max_used_gb': max(memory_used_values)
            },
            'samples': len(self.cpu_samples)
        }

def single_request(request_id, prompt_length="short"):
    """å•ä¸ªè¯·æ±‚æµ‹è¯•"""
    prompts = {
        "short": f"ç®€å•å›ç­”ï¼šä»€ä¹ˆæ˜¯AIï¼Ÿ(è¯·æ±‚{request_id})",
        "medium": f"è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„ä¸‰ä¸ªä¸»è¦åº”ç”¨é¢†åŸŸï¼Œæ¯ä¸ªé¢†åŸŸç”¨2-3å¥è¯è¯´æ˜ã€‚(è¯·æ±‚{request_id})",
        "long": f"è¯·å†™ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ åœ¨ç°ä»£ç¤¾ä¼šä¸­åº”ç”¨çš„çŸ­æ–‡ï¼ŒåŒ…æ‹¬è‡³å°‘3ä¸ªå…·ä½“ä¾‹å­ï¼Œæ¯ä¸ªä¾‹å­éƒ½è¦è¯´æ˜å…¶å·¥ä½œåŸç†å’Œå®é™…æ•ˆæœã€‚è¯·ç¡®ä¿å†…å®¹å‡†ç¡®ä¸”æ˜“äºç†è§£ã€‚(è¯·æ±‚{request_id})"
    }
    
    max_tokens = {
        "short": 30,
        "medium": 100,
        "long": 200
    }
    
    try:
        start_time = time.time()
        
        payload = {
            "model": "qwen-0.5b",
            "messages": [
                {"role": "user", "content": prompts[prompt_length]}
            ],
            "max_tokens": max_tokens[prompt_length],
            "temperature": 0.7
        }
        
        response = requests.post(
            f"{API_BASE}/chat/completions",
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        response_time = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            usage = data.get('usage', {})
            
            return {
                "request_id": request_id,
                "success": True,
                "response_time": response_time,
                "content_length": len(content),
                "prompt_tokens": usage.get('prompt_tokens', 0),
                "completion_tokens": usage.get('completion_tokens', 0),
                "total_tokens": usage.get('total_tokens', 0),
                "tokens_per_second": usage.get('completion_tokens', 0) / response_time if response_time > 0 else 0,
                "content": content[:100] + "..." if len(content) > 100 else content,
                "prompt_length": prompt_length
            }
        else:
            return {
                "request_id": request_id,
                "success": False,
                "response_time": response_time,
                "error": f"HTTP {response.status_code}: {response.text[:100]}"
            }
        
    except Exception as e:
        return {
            "request_id": request_id,
            "success": False,
            "response_time": 0,
            "error": str(e)
        }

def test_concurrent_performance(num_requests=50, max_workers=10):
    """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
    print(f"\nğŸš€ å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print(f"   è¯·æ±‚æ•°: {num_requests}")
    print(f"   å¹¶å‘æ•°: {max_workers}")
    print("=" * 60)
    
    monitor = SystemMonitor()
    monitor.start_monitoring()
    
    start_time = time.time()
    
    # ä½¿ç”¨ä¸åŒé•¿åº¦çš„æç¤ºè¯æµ‹è¯•
    prompt_types = ["short"] * (num_requests // 2) + ["medium"] * (num_requests // 3) + ["long"] * (num_requests - num_requests//2 - num_requests//3)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(single_request, i, prompt_types[i % len(prompt_types)]) for i in range(num_requests)]
        results = []
        
        for i, future in enumerate(concurrent.futures.as_completed(futures)):
            result = future.result()
            results.append(result)
            
            if result["success"]:
                print(f"âœ… {i+1:2d}/{num_requests} - {result['response_time']:.3f}s - {result['tokens_per_second']:.1f} t/s")
            else:
                print(f"âŒ {i+1:2d}/{num_requests} - å¤±è´¥: {result.get('error', 'Unknown')[:50]}")
    
    total_time = time.time() - start_time
    monitor.stop_monitoring()
    system_stats = monitor.get_stats()
    
    return results, total_time, system_stats

def test_token_generation_speed():
    """æµ‹è¯•ä¸åŒé•¿åº¦æ–‡æœ¬çš„Tokenç”Ÿæˆé€Ÿåº¦"""
    print(f"\nğŸ“Š Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•")
    print("=" * 60)
    
    test_cases = [
        ("çŸ­æ–‡æœ¬", "short", 5),
        ("ä¸­ç­‰æ–‡æœ¬", "medium", 5), 
        ("é•¿æ–‡æœ¬", "long", 5)
    ]
    
    results = {}
    
    for case_name, prompt_type, num_tests in test_cases:
        print(f"\næµ‹è¯• {case_name} ({num_tests}æ¬¡)...")
        case_results = []
        
        for i in range(num_tests):
            result = single_request(f"{case_name}_{i}", prompt_type)
            case_results.append(result)
            
            if result["success"]:
                print(f"  {i+1}: {result['response_time']:.3f}s - {result['completion_tokens']} tokens - {result['tokens_per_second']:.1f} t/s")
            else:
                print(f"  {i+1}: å¤±è´¥ - {result.get('error', 'Unknown')}")
        
        # ç»Ÿè®¡
        successful = [r for r in case_results if r["success"]]
        if successful:
            response_times = [r["response_time"] for r in successful]
            tokens_per_sec = [r["tokens_per_second"] for r in successful]
            completion_tokens = [r["completion_tokens"] for r in successful]
            
            results[case_name] = {
                "success_rate": len(successful) / len(case_results) * 100,
                "avg_response_time": statistics.mean(response_times),
                "avg_tokens_per_second": statistics.mean(tokens_per_sec),
                "avg_completion_tokens": statistics.mean(completion_tokens),
                "max_tokens_per_second": max(tokens_per_sec),
                "min_tokens_per_second": min(tokens_per_sec)
            }
        else:
            results[case_name] = {"success_rate": 0}
    
    return results

def test_stress_limits():
    """å‹åŠ›æé™æµ‹è¯•"""
    print(f"\nâš¡ å‹åŠ›æé™æµ‹è¯•")
    print("=" * 60)
    
    # é€æ­¥å¢åŠ å¹¶å‘æ•°æµ‹è¯•
    concurrent_tests = [1, 2, 5, 10, 15, 20]
    results = {}
    
    for concurrent_num in concurrent_tests:
        print(f"\næµ‹è¯•å¹¶å‘æ•°: {concurrent_num}")
        
        try:
            test_results, total_time, system_stats = test_concurrent_performance(
                num_requests=concurrent_num * 3,  # æ¯ä¸ªå¹¶å‘æ•°æµ‹è¯•3å€è¯·æ±‚
                max_workers=concurrent_num
            )
            
            successful = [r for r in test_results if r["success"]]
            success_rate = len(successful) / len(test_results) * 100
            
            if successful:
                avg_response_time = statistics.mean([r["response_time"] for r in successful])
                avg_tokens_per_sec = statistics.mean([r["tokens_per_second"] for r in successful])
                throughput = len(test_results) / total_time
                
                results[concurrent_num] = {
                    "success_rate": success_rate,
                    "avg_response_time": avg_response_time,
                    "avg_tokens_per_second": avg_tokens_per_sec,
                    "throughput": throughput,
                    "system_stats": system_stats
                }
                
                print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
                print(f"  å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_tokens_per_sec:.1f} tokens/s")
                print(f"  ååé‡: {throughput:.2f} è¯·æ±‚/s")
                print(f"  å¹³å‡CPU: {system_stats['cpu']['avg']:.1f}%")
            else:
                results[concurrent_num] = {"success_rate": 0}
                print(f"  å…¨éƒ¨å¤±è´¥")
                break  # å¦‚æœå…¨éƒ¨å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•æ›´é«˜å¹¶å‘
                
        except Exception as e:
            print(f"  æµ‹è¯•å¤±è´¥: {e}")
            break
    
    return results

def generate_report(concurrent_results, token_speed_results, stress_results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    cpu_info = os.popen('sysctl -n machdep.cpu.brand_string').read().strip()
    cpu_cores = os.popen('sysctl -n hw.ncpu').read().strip()
    memory_total = psutil.virtual_memory().total / 1024**3
    
    report = f"""# Mac Studio ç”Ÿäº§ç¯å¢ƒæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ

- **è®¾å¤‡**: Mac Studio 
- **å¤„ç†å™¨**: {cpu_info}
- **æ ¸å¿ƒæ•°**: {cpu_cores}
- **å†…å­˜**: {memory_total:.1f}GB
- **æ¨ç†å¼•æ“**: MLX (Apple Siliconä¼˜åŒ–)
- **æ¨¡å‹**: Qwen2.5-0.5B-Instruct
- **æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å¹¶å‘æ€§èƒ½æµ‹è¯•ç»“æœ

"""
    
    if concurrent_results:
        successful = [r for r in concurrent_results if r["success"]]
        if successful:
            response_times = [r["response_time"] for r in successful]
            tokens_per_sec_list = [r["tokens_per_second"] for r in successful]
            
            report += f"""### åŸºç¡€æ€§èƒ½æŒ‡æ ‡

- **æ€»è¯·æ±‚æ•°**: {len(concurrent_results)}
- **æˆåŠŸè¯·æ±‚**: {len(successful)}
- **æˆåŠŸç‡**: {len(successful)/len(concurrent_results)*100:.1f}%
- **å¹³å‡å“åº”æ—¶é—´**: {statistics.mean(response_times):.3f}ç§’
- **æœ€å¿«å“åº”**: {min(response_times):.3f}ç§’
- **æœ€æ…¢å“åº”**: {max(response_times):.3f}ç§’
- **å¹³å‡Tokenç”Ÿæˆé€Ÿåº¦**: {statistics.mean(tokens_per_sec_list):.1f} tokens/ç§’
- **æœ€å¿«ç”Ÿæˆé€Ÿåº¦**: {max(tokens_per_sec_list):.1f} tokens/ç§’

"""

    # Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•
    report += "## Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•\n\n"
    
    for test_name, results in token_speed_results.items():
        if results.get("success_rate", 0) > 0:
            report += f"""### {test_name}

- **æˆåŠŸç‡**: {results['success_rate']:.1f}%
- **å¹³å‡å“åº”æ—¶é—´**: {results['avg_response_time']:.3f}ç§’
- **å¹³å‡Tokenæ•°**: {results['avg_completion_tokens']:.0f}
- **å¹³å‡ç”Ÿæˆé€Ÿåº¦**: {results['avg_tokens_per_second']:.1f} tokens/ç§’
- **æœ€å¿«ç”Ÿæˆé€Ÿåº¦**: {results['max_tokens_per_second']:.1f} tokens/ç§’
- **æœ€æ…¢ç”Ÿæˆé€Ÿåº¦**: {results['min_tokens_per_second']:.1f} tokens/ç§’

"""

    # å‹åŠ›æµ‹è¯•ç»“æœ
    report += "## å‹åŠ›æé™æµ‹è¯•\n\n"
    report += "| å¹¶å‘æ•° | æˆåŠŸç‡ | å¹³å‡å“åº”æ—¶é—´ | Tokenç”Ÿæˆé€Ÿåº¦ | ååé‡ | å¹³å‡CPU |\n"
    report += "|--------|--------|--------------|---------------|--------|----------|\n"
    
    for concurrent_num, result in stress_results.items():
        if result.get("success_rate", 0) > 0:
            report += f"| {concurrent_num} | {result['success_rate']:.1f}% | {result['avg_response_time']:.3f}s | {result['avg_tokens_per_second']:.1f} t/s | {result['throughput']:.2f} req/s | {result['system_stats']['cpu']['avg']:.1f}% |\n"
        else:
            report += f"| {concurrent_num} | 0% | - | - | - | - |\n"
    
    # æ€§èƒ½åˆ†æ
    report += "\n## æ€§èƒ½åˆ†æ\n\n"
    
    # æ‰¾åˆ°æœ€ä¼˜å¹¶å‘æ•°
    best_concurrent = None
    best_throughput = 0
    
    for concurrent_num, result in stress_results.items():
        if result.get("success_rate", 0) >= 95 and result.get("throughput", 0) > best_throughput:
            best_throughput = result["throughput"]
            best_concurrent = concurrent_num
    
    if best_concurrent:
        report += f"""### æœ€ä¼˜é…ç½®

- **æ¨èå¹¶å‘æ•°**: {best_concurrent}
- **æœ€å¤§ååé‡**: {best_throughput:.2f} è¯·æ±‚/ç§’
- **åœ¨è¯¥é…ç½®ä¸‹çš„æ€§èƒ½**:
  - æˆåŠŸç‡: {stress_results[best_concurrent]['success_rate']:.1f}%
  - å¹³å‡å“åº”æ—¶é—´: {stress_results[best_concurrent]['avg_response_time']:.3f}ç§’
  - Tokenç”Ÿæˆé€Ÿåº¦: {stress_results[best_concurrent]['avg_tokens_per_second']:.1f} tokens/ç§’

"""

    # ç³»ç»Ÿèµ„æºä½¿ç”¨
    if stress_results:
        max_cpu_result = max(stress_results.values(), 
                           key=lambda x: x.get('system_stats', {}).get('cpu', {}).get('avg', 0))
        
        if 'system_stats' in max_cpu_result:
            stats = max_cpu_result['system_stats']
            report += f"""### ç³»ç»Ÿèµ„æºä½¿ç”¨

- **æœ€é«˜CPUä½¿ç”¨ç‡**: {stats['cpu']['max']:.1f}%
- **å¹³å‡CPUä½¿ç”¨ç‡**: {stats['cpu']['avg']:.1f}%
- **æœ€é«˜å†…å­˜ä½¿ç”¨**: {stats['memory']['max_used_gb']:.1f}GB ({stats['memory']['max_percent']:.1f}%)
- **å¹³å‡å†…å­˜ä½¿ç”¨**: {stats['memory']['avg_used_gb']:.1f}GB ({stats['memory']['avg_percent']:.1f}%)

"""

    # ç»“è®ºå’Œå»ºè®®
    report += """## ç»“è®ºä¸å»ºè®®

### æ€§èƒ½ç‰¹ç‚¹

1. **MLXå¼•æ“ä¼˜åŠ¿**: åœ¨Apple Siliconä¸Šè¡¨ç°ä¼˜ç§€ï¼ŒTokenç”Ÿæˆé€Ÿåº¦ç¨³å®š
2. **å†…å­˜æ•ˆç‡**: 0.5Bå‚æ•°æ¨¡å‹å†…å­˜ä½¿ç”¨åˆç†ï¼Œé€‚åˆå•æœºéƒ¨ç½²
3. **å¹¶å‘èƒ½åŠ›**: å—é™äºFlaskå¼€å‘æœåŠ¡å™¨ï¼Œå»ºè®®ç”Ÿäº§ç¯å¢ƒä½¿ç”¨Gunicorn

### ç”Ÿäº§éƒ¨ç½²å»ºè®®

1. **æ¨èé…ç½®**:
   - ä½¿ç”¨Gunicorn + Gevent workers
   - Workeræ•°é‡: 4-8ä¸ª
   - æ¯ä¸ªWorkerå¤„ç†2-3ä¸ªå¹¶å‘è¯·æ±‚

2. **æ€§èƒ½ä¼˜åŒ–**:
   - å¯ç”¨æ¨¡å‹ç¼“å­˜
   - ä½¿ç”¨SSDå­˜å‚¨
   - é…ç½®é€‚å½“çš„max_tokensé™åˆ¶

3. **ç›‘æ§æŒ‡æ ‡**:
   - å“åº”æ—¶é—´ < 1ç§’
   - CPUä½¿ç”¨ç‡ < 80%
   - å†…å­˜ä½¿ç”¨ç‡ < 70%

### é€‚ç”¨åœºæ™¯

- **é€‚åˆ**: ä¸­å°è§„æ¨¡åº”ç”¨ã€å¼€å‘æµ‹è¯•ã€åŸå‹éªŒè¯
- **ä¸é€‚åˆ**: é«˜å¹¶å‘ç”Ÿäº§ç¯å¢ƒï¼ˆå»ºè®®ä½¿ç”¨æ›´å¤§æ¨¡å‹+GPUé›†ç¾¤ï¼‰

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸ¯ Mac Studio å…¨é¢æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    try:
        response = requests.get(f"{BASE_URL}/health", timeout=5)
        if response.status_code != 200:
            print("âŒ æœåŠ¡ä¸å¯ç”¨")
            return
        
        data = response.json()
        if data.get('models', 0) == 0:
            print("âŒ æ²¡æœ‰åŠ è½½çš„æ¨¡å‹")
            return
        
        print(f"âœ… æœåŠ¡çŠ¶æ€: {data.get('status')}")
        print(f"âœ… å·²åŠ è½½æ¨¡å‹: {data.get('models')} ä¸ª")
        
    except Exception as e:
        print(f"âŒ æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
        return
    
    # 1. å¹¶å‘æ€§èƒ½æµ‹è¯•
    print("\n" + "="*60)
    print("ç¬¬ä¸€é˜¶æ®µ: å¹¶å‘æ€§èƒ½æµ‹è¯•")
    concurrent_results, total_time, system_stats = test_concurrent_performance(30, 8)
    
    # 2. Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•  
    print("\n" + "="*60)
    print("ç¬¬äºŒé˜¶æ®µ: Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•")
    token_results = test_token_generation_speed()
    
    # 3. å‹åŠ›æé™æµ‹è¯•
    print("\n" + "="*60)  
    print("ç¬¬ä¸‰é˜¶æ®µ: å‹åŠ›æé™æµ‹è¯•")
    stress_results = test_stress_limits()
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    report = generate_report(concurrent_results, token_results, stress_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"macæµ‹è¯•æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æµ‹è¯•å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print("\n" + "="*60)

if __name__ == "__main__":
    main()