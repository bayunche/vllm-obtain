#!/usr/bin/env python3
"""
å®‰å…¨çš„å¹¶å‘æµ‹è¯•è„šæœ¬
æµ‹è¯•MLXå¼•æ“çš„å¹¶å‘å¤„ç†èƒ½åŠ›
"""

import asyncio
import time
import traceback
from src.engines.mlx_engine import MlxEngine
from src.core.inference_engine import EngineConfig, InferenceRequest

async def test_concurrent_inference():
    print("="*60)
    print("ğŸ§ª MLXå¹¶å‘æ¨ç†å®‰å…¨æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºå¼•æ“é…ç½®
    config = EngineConfig(
        engine_type='mlx',
        device_type='mps',
        max_gpu_memory=0.8,
        max_cpu_threads=8,
        max_sequence_length=2048
    )
    
    # åˆå§‹åŒ–å¼•æ“
    print("\n1. åˆå§‹åŒ–MLXå¼•æ“...")
    engine = MlxEngine(config)
    init_success = await engine.initialize()
    
    if not init_success:
        print("âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return False
    print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
    
    # åŠ è½½æ¨¡å‹
    print("\n2. åŠ è½½æ¨¡å‹...")
    model_path = './models/qwen-0.5b'
    model_name = 'qwen-test'
    
    load_success = await engine.load_model(model_name, model_path)
    if not load_success:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return False
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # æµ‹è¯•å•ä¸ªè¯·æ±‚
    print("\n3. æµ‹è¯•å•ä¸ªè¯·æ±‚...")
    single_request = InferenceRequest(
        model_name=model_name,
        prompt="ä½ å¥½",
        max_tokens=20,
        temperature=0.7
    )
    
    try:
        response = await engine.generate(single_request)
        print(f"âœ… å•ä¸ªè¯·æ±‚æˆåŠŸ: {response.text[:50]}...")
    except Exception as e:
        print(f"âŒ å•ä¸ªè¯·æ±‚å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•é¡ºåºè¯·æ±‚
    print("\n4. æµ‹è¯•é¡ºåºè¯·æ±‚ï¼ˆ3ä¸ªï¼‰...")
    for i in range(3):
        request = InferenceRequest(
            model_name=model_name,
            prompt=f"æµ‹è¯•{i+1}",
            max_tokens=10,
            temperature=0.7
        )
        try:
            response = await engine.generate(request)
            print(f"  âœ… è¯·æ±‚{i+1}æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ è¯·æ±‚{i+1}å¤±è´¥: {e}")
    
    # æµ‹è¯•å¹¶å‘è¯·æ±‚ï¼ˆä½¿ç”¨é”ä¿æŠ¤ï¼‰
    print("\n5. æµ‹è¯•å¹¶å‘è¯·æ±‚ï¼ˆå¸¦ä¿æŠ¤ï¼‰...")
    
    async def safe_generate(req_id, prompt):
        """å®‰å…¨çš„æ¨ç†è¯·æ±‚"""
        try:
            request = InferenceRequest(
                model_name=model_name,
                prompt=prompt,
                max_tokens=20,
                temperature=0.7
            )
            
            start_time = time.time()
            response = await engine.generate(request)
            elapsed = time.time() - start_time
            
            return {
                'id': req_id,
                'success': True,
                'time': elapsed,
                'text': response.text[:30]
            }
        except Exception as e:
            return {
                'id': req_id,
                'success': False,
                'error': str(e)
            }
    
    # å¹¶å‘æ‰§è¡Œ3ä¸ªè¯·æ±‚
    prompts = ["å¹¶å‘æµ‹è¯•1", "å¹¶å‘æµ‹è¯•2", "å¹¶å‘æµ‹è¯•3"]
    tasks = [safe_generate(i, prompt) for i, prompt in enumerate(prompts)]
    
    print("  å¯åŠ¨3ä¸ªå¹¶å‘è¯·æ±‚...")
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    total_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    successful = 0
    failed = 0
    for result in results:
        if isinstance(result, Exception):
            print(f"  âŒ å¼‚å¸¸: {result}")
            failed += 1
        elif isinstance(result, dict):
            if result['success']:
                print(f"  âœ… è¯·æ±‚{result['id']}æˆåŠŸ (è€—æ—¶: {result['time']:.2f}s)")
                successful += 1
            else:
                print(f"  âŒ è¯·æ±‚{result['id']}å¤±è´¥: {result.get('error', 'Unknown')}")
                failed += 1
    
    print(f"\n  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  æˆåŠŸ: {successful}/{len(results)}")
    print(f"  å¤±è´¥: {failed}/{len(results)}")
    
    # æµ‹è¯•æ›´å¤šå¹¶å‘ï¼ˆé€æ­¥å¢åŠ ï¼‰
    if successful == len(results):
        print("\n6. æµ‹è¯•æ¸è¿›å¼å¹¶å‘...")
        for concurrent_count in [2, 3, 4, 5]:
            print(f"\n  æµ‹è¯•{concurrent_count}ä¸ªå¹¶å‘è¯·æ±‚...")
            
            # æ·»åŠ å»¶è¿Ÿå¯åŠ¨ä»¥é¿å…åŒæ—¶å†²å‡»
            async def delayed_generate(delay, req_id, prompt):
                await asyncio.sleep(delay * 0.05)  # 50msé—´éš”
                return await safe_generate(req_id, prompt)
            
            prompts = [f"æ¸è¿›æµ‹è¯•{i}" for i in range(concurrent_count)]
            tasks = [delayed_generate(i, i, prompt) for i, prompt in enumerate(prompts)]
            
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                success_count = sum(1 for r in results if isinstance(r, dict) and r['success'])
                print(f"    ç»“æœ: {success_count}/{concurrent_count} æˆåŠŸ")
                
                if success_count < concurrent_count:
                    print(f"    âš ï¸ åœ¨{concurrent_count}ä¸ªå¹¶å‘æ—¶å‡ºç°é—®é¢˜ï¼Œåœæ­¢æµ‹è¯•")
                    break
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                break
    
    # å¸è½½æ¨¡å‹
    print("\n7. å¸è½½æ¨¡å‹...")
    await engine.unload_model(model_name)
    print("âœ… æ¨¡å‹å¸è½½æˆåŠŸ")
    
    return True

async def main():
    try:
        success = await test_concurrent_inference()
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        else:
            print("\nâŒ æµ‹è¯•æœªå®Œå…¨é€šè¿‡")
        return success
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)