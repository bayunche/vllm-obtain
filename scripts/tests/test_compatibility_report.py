#!/usr/bin/env python3
"""
è·¨å¹³å°å…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
è‡ªåŠ¨è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š
"""

import sys
import os
import platform
import json
import time
import traceback
import asyncio
import statistics
import threading
import concurrent.futures
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.utils.platform_detector import PlatformDetector, get_optimal_engine
from src.utils.config import ConfigManager
from src.engines import VLLM_AVAILABLE, MLX_AVAILABLE, create_engine
from src.core.inference_engine import EngineConfig, InferenceRequest


class CompatibilityTestReporter:
    """å…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.detector = PlatformDetector()
        self.test_results = []
        self.performance_results = {}
        self.engine_instance = None
        self.summary = {
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'warnings': 0,
            'test_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'platform': platform.platform(),
            'python_version': platform.python_version()
        }
        
    def run_test(self, test_name: str, test_func, category: str = "åŠŸèƒ½æµ‹è¯•") -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è®°å½•ç»“æœ"""
        result = {
            'name': test_name,
            'category': category,
            'status': 'pending',
            'message': '',
            'details': [],
            'duration': 0
        }
        
        start_time = time.time()
        try:
            test_result = test_func()
            result['duration'] = round(time.time() - start_time, 3)
            
            if isinstance(test_result, tuple):
                success, message, details = test_result
                result['status'] = 'passed' if success else 'failed'
                result['message'] = message
                result['details'] = details if isinstance(details, list) else [details]
            else:
                result['status'] = 'passed' if test_result else 'failed'
                result['message'] = 'æµ‹è¯•é€šè¿‡' if test_result else 'æµ‹è¯•å¤±è´¥'
                
        except Exception as e:
            result['status'] = 'failed'
            result['message'] = str(e)
            result['details'] = [traceback.format_exc()]
            result['duration'] = round(time.time() - start_time, 3)
            
        self.test_results.append(result)
        self.summary['total_tests'] += 1
        if result['status'] == 'passed':
            self.summary['passed'] += 1
        elif result['status'] == 'failed':
            self.summary['failed'] += 1
        else:
            self.summary['warnings'] += 1
            
        return result
    
    def test_platform_detection(self):
        """æµ‹è¯•å¹³å°æ£€æµ‹"""
        try:
            platform_info = self.detector.get_platform_info()
            device_info = self.detector.get_device_info()
            
            details = []
            details.append(f"æ“ä½œç³»ç»Ÿ: {platform_info['system']}")
            details.append(f"æ¶æ„: {platform_info['machine']}")
            details.append(f"Pythonç‰ˆæœ¬: {platform_info['python_version']}")
            details.append(f"CPUæ ¸å¿ƒæ•°: {device_info.get('cpu_count', 'æœªçŸ¥')}")
            
            if 'total_memory_gb' in device_info:
                details.append(f"æ€»å†…å­˜: {device_info['total_memory_gb']} GB")
            
            if device_info.get('cuda_available'):
                details.append(f"CUDA: å¯ç”¨")
                if 'cuda_devices' in device_info:
                    for device in device_info['cuda_devices']:
                        details.append(f"  - {device['name']} ({device['memory_gb']} GB)")
            else:
                details.append(f"CUDA: ä¸å¯ç”¨")
                
            if device_info.get('mps_available'):
                details.append(f"Metal (MPS): å¯ç”¨")
            else:
                details.append(f"Metal (MPS): ä¸å¯ç”¨")
                
            return True, "å¹³å°æ£€æµ‹æˆåŠŸ", details
            
        except Exception as e:
            return False, f"å¹³å°æ£€æµ‹å¤±è´¥: {str(e)}", []
    
    def test_engine_availability(self):
        """æµ‹è¯•å¼•æ“å¯ç”¨æ€§"""
        details = []
        engines_status = {
            'MLX': ('å¯ç”¨' if MLX_AVAILABLE else 'ä¸å¯ç”¨', MLX_AVAILABLE),
            'VLLM': ('å¯ç”¨' if VLLM_AVAILABLE else 'ä¸å¯ç”¨', VLLM_AVAILABLE),
            'LlamaCpp': ('å¯ç”¨', True)  # æ€»æ˜¯å¯ç”¨ä½œä¸ºåå¤‡
        }
        
        all_ok = True
        for engine, (status, available) in engines_status.items():
            details.append(f"{engine}: {status}")
            if engine != 'LlamaCpp' and not available:
                all_ok = False
                
        # æ£€æŸ¥å¹³å°æ¨èå¼•æ“
        optimal_engine = get_optimal_engine()
        details.append(f"æ¨èå¼•æ“: {optimal_engine}")
        
        message = "æ‰€æœ‰å¼•æ“çŠ¶æ€æ­£å¸¸" if all_ok else "éƒ¨åˆ†å¼•æ“ä¸å¯ç”¨ï¼ˆå°†è‡ªåŠ¨å›é€€ï¼‰"
        return True, message, details
    
    def test_config_validation(self):
        """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
        try:
            details = []
            
            # æµ‹è¯•é…ç½®åŠ è½½
            config = ConfigManager.load_config()
            details.append(f"é…ç½®åŠ è½½: æˆåŠŸ")
            
            # æ£€æŸ¥å¹³å°ç‰¹å®šé…ç½®
            system = platform.system().lower()
            platform_env_map = {
                'darwin': '.env.mac',
                'linux': '.env.linux',
                'windows': '.env.windows'
            }
            
            expected_env = platform_env_map.get(system)
            if expected_env and os.path.exists(expected_env):
                details.append(f"å¹³å°é…ç½®æ–‡ä»¶: {expected_env} (å­˜åœ¨)")
            else:
                details.append(f"å¹³å°é…ç½®æ–‡ä»¶: ä½¿ç”¨é»˜è®¤é…ç½®")
            
            # éªŒè¯å…³é”®é…ç½®
            details.append(f"æ¨ç†å¼•æ“: {config.inference_engine}")
            details.append(f"è®¾å¤‡ç±»å‹: {config.device_type}")
            details.append(f"Workeræ•°: {config.workers}")
            details.append(f"ç«¯å£: {config.port}")
            
            # éªŒè¯ç¯å¢ƒ
            validation = ConfigManager.validate_environment()
            if validation['valid']:
                details.append("ç¯å¢ƒéªŒè¯: é€šè¿‡")
            else:
                details.append("ç¯å¢ƒéªŒè¯: å¤±è´¥")
                for error in validation.get('errors', []):
                    details.append(f"  é”™è¯¯: {error}")
                    
            for warning in validation.get('warnings', []):
                details.append(f"  è­¦å‘Š: {warning}")
                
            return validation['valid'], "é…ç½®ç³»ç»Ÿæ­£å¸¸" if validation['valid'] else "é…ç½®å­˜åœ¨é—®é¢˜", details
            
        except Exception as e:
            return False, f"é…ç½®æµ‹è¯•å¤±è´¥: {str(e)}", []
    
    def test_path_compatibility(self):
        """æµ‹è¯•è·¯å¾„å…¼å®¹æ€§"""
        details = []
        test_paths = {
            'ç›¸å¯¹è·¯å¾„': './models',
            'åæ–œæ è·¯å¾„': '.\\cache',
            'æ··åˆè·¯å¾„': 'models/qwen\\model.bin'
        }
        
        all_ok = True
        for desc, test_path in test_paths.items():
            try:
                normalized = Path(test_path)
                details.append(f"{desc}: {test_path} â†’ {normalized} âœ“")
            except Exception as e:
                details.append(f"{desc}: {test_path} â†’ å¤±è´¥ ({e})")
                all_ok = False
                
        return all_ok, "è·¯å¾„å¤„ç†æ­£å¸¸" if all_ok else "è·¯å¾„å¤„ç†å­˜åœ¨é—®é¢˜", details
    
    def test_engine_fallback(self):
        """æµ‹è¯•å¼•æ“å›é€€æœºåˆ¶"""
        try:
            from src.engines import create_engine
            from src.core.inference_engine import EngineConfig
            
            details = []
            
            # æµ‹è¯•æ— æ•ˆå¼•æ“å›é€€
            config = EngineConfig(
                engine_type='invalid_engine',
                device_type='auto'
            )
            
            engine = create_engine('invalid_engine', config)
            details.append(f"æ— æ•ˆå¼•æ“ â†’ {engine.__class__.__name__} (è‡ªåŠ¨å›é€€)")
            
            # æµ‹è¯•ä¸å¯ç”¨å¼•æ“å›é€€
            if not MLX_AVAILABLE:
                engine = create_engine('mlx', config)
                details.append(f"MLXä¸å¯ç”¨ â†’ {engine.__class__.__name__} (è‡ªåŠ¨å›é€€)")
                
            if not VLLM_AVAILABLE:
                engine = create_engine('vllm', config)
                details.append(f"VLLMä¸å¯ç”¨ â†’ {engine.__class__.__name__} (è‡ªåŠ¨å›é€€)")
                
            return True, "å¼•æ“å›é€€æœºåˆ¶æ­£å¸¸", details
            
        except Exception as e:
            return False, f"å¼•æ“å›é€€æµ‹è¯•å¤±è´¥: {str(e)}", []
    
    async def test_model_loading(self):
        """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
        try:
            details = []
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
            model_paths = [
                "./models/qwen3-7b",
                "./models/qwen-7b", 
                "./models/Qwen2.5-7B-Instruct",
                "./models/qwen-0.5b"
            ]
            
            available_model = None
            for model_path in model_paths:
                if os.path.exists(model_path):
                    available_model = model_path
                    break
            
            if not available_model:
                details.append("æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
                details.extend([f"æ£€æŸ¥è·¯å¾„: {path}" for path in model_paths])
                return False, "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶", details
            
            # è·å–æœ€ä¼˜å¼•æ“
            engine_type = get_optimal_engine()
            details.append(f"ä½¿ç”¨å¼•æ“: {engine_type}")
            details.append(f"æ¨¡å‹è·¯å¾„: {available_model}")
            
            # åˆ›å»ºå¼•æ“é…ç½®
            config = EngineConfig(
                engine_type=engine_type,
                device_type='auto',
                max_gpu_memory=0.8,
                max_cpu_threads=8,
                max_sequence_length=2048
            )
            
            # åˆ›å»ºå¹¶åˆå§‹åŒ–å¼•æ“
            self.engine_instance = create_engine(engine_type, config)
            init_success = await self.engine_instance.initialize()
            
            if not init_success:
                return False, "å¼•æ“åˆå§‹åŒ–å¤±è´¥", details
            
            details.append("å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            model_name = "qwen-test"
            load_start = time.time()
            load_success = await self.engine_instance.load_model(model_name, available_model)
            load_time = time.time() - load_start
            
            if load_success:
                details.append(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}s")
                self.performance_results['model_load_time'] = load_time
                return True, f"æ¨¡å‹åŠ è½½æˆåŠŸ ({load_time:.2f}s)", details
            else:
                return False, "æ¨¡å‹åŠ è½½å¤±è´¥", details
                
        except Exception as e:
            return False, f"æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}", [str(e)]
    
    async def test_token_generation_speed(self):
        """æµ‹è¯• token ç”Ÿæˆé€Ÿåº¦"""
        if not self.engine_instance:
            return False, "å¼•æ“æœªåˆå§‹åŒ–", ["éœ€è¦å…ˆé€šè¿‡æ¨¡å‹åŠ è½½æµ‹è¯•"]
        
        try:
            details = []
            
            # æµ‹è¯•æ–‡æœ¬
            test_prompts = [
                "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
                "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†",
                "Python æœ‰å“ªäº›ä¸»è¦çš„ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼Ÿ"
            ]
            
            token_speeds = []
            response_times = []
            
            for i, prompt in enumerate(test_prompts):
                details.append(f"æµ‹è¯• {i+1}: {prompt[:30]}...")
                
                # åˆ›å»ºæ¨ç†è¯·æ±‚
                request = InferenceRequest(
                    model_name="qwen-test",
                    prompt=prompt,
                    max_tokens=100,
                    temperature=0.7,
                    top_p=0.9
                )
                
                # æ‰§è¡Œæ¨ç†
                start_time = time.time()
                try:
                    response = await self.engine_instance.generate(request)
                    end_time = time.time()
                    
                    response_time = end_time - start_time
                    response_times.append(response_time)
                    
                    # ä¼°ç®— token æ•°é‡ï¼ˆç®€å•ä¼°è®¡ï¼šå­—ç¬¦æ•° / 2ï¼‰
                    estimated_tokens = len(response.text) // 2
                    token_speed = estimated_tokens / response_time if response_time > 0 else 0
                    token_speeds.append(token_speed)
                    
                    details.append(f"  å“åº”æ—¶é—´: {response_time:.2f}s")
                    details.append(f"  ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(response.text)} å­—ç¬¦")
                    details.append(f"  ä¼°ç®— tokens: {estimated_tokens}")
                    details.append(f"  Token é€Ÿåº¦: {token_speed:.1f} tokens/s")
                    
                except Exception as e:
                    details.append(f"  æ¨ç†å¤±è´¥: {str(e)}")
                    continue
            
            if token_speeds:
                avg_token_speed = statistics.mean(token_speeds)
                avg_response_time = statistics.mean(response_times)
                
                self.performance_results.update({
                    'avg_token_speed': avg_token_speed,
                    'avg_response_time': avg_response_time,
                    'token_speeds': token_speeds,
                    'response_times': response_times
                })
                
                details.append(f"å¹³å‡ Token é€Ÿåº¦: {avg_token_speed:.1f} tokens/s")
                details.append(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
                
                return True, f"Token ç”Ÿæˆæµ‹è¯•å®Œæˆ (å¹³å‡ {avg_token_speed:.1f} tokens/s)", details
            else:
                return False, "æ‰€æœ‰æ¨ç†è¯·æ±‚éƒ½å¤±è´¥äº†", details
                
        except Exception as e:
            return False, f"Token ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}", [str(e)]
    
    async def test_concurrent_throughput(self):
        """æµ‹è¯•3ä¸ªå¹¶å‘è¯·æ±‚çš„ååé€Ÿåº¦"""
        if not self.engine_instance:
            return False, "å¼•æ“æœªåˆå§‹åŒ–", ["éœ€è¦å…ˆé€šè¿‡æ¨¡å‹åŠ è½½æµ‹è¯•"]
        
        try:
            details = []
            concurrent_requests = 3  # é™ä½å¹¶å‘æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
            
            # å¹¶å‘æµ‹è¯•çš„æç¤ºè¯ï¼ˆå‡å°‘åˆ°ä¸‰ä¸ªï¼‰
            test_prompts = [
                "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "Pythonçš„ç‰¹ç‚¹ï¼Ÿ",
                "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ"
            ][:concurrent_requests]  # ç¡®ä¿æç¤ºè¯æ•°é‡ä¸è¯·æ±‚æ•°åŒ¹é…
            
            details.append(f"å¼€å§‹ {concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚æµ‹è¯•...")
            
            # åˆ›å»ºå¹¶å‘è¯·æ±‚
            requests = []
            for i in range(concurrent_requests):
                request = InferenceRequest(
                    model_name="qwen-test",
                    prompt=test_prompts[i],
                    max_tokens=30,  # å‡å°‘tokenæ•°ä»¥åŠ å¿«æµ‹è¯•
                    temperature=0.7
                )
                requests.append(request)
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            start_time = time.time()
            
            async def single_request(req_id, request):
                """å•ä¸ªè¯·æ±‚å¤„ç†"""
                req_start = time.time()
                try:
                    response = await self.engine_instance.generate(request)
                    req_end = time.time()
                    return {
                        'id': req_id,
                        'success': True,
                        'response_time': req_end - req_start,
                        'text_length': len(response.text),
                        'estimated_tokens': len(response.text) // 2
                    }
                except Exception as e:
                    req_end = time.time()
                    return {
                        'id': req_id,
                        'success': False,
                        'response_time': req_end - req_start,
                        'error': str(e)
                    }
            
            # å¹¶å‘æ‰§è¡Œï¼Œä½†æ·»åŠ å»¶è¿Ÿä»¥é¿å…åŒæ—¶å¯åŠ¨
            async def delayed_request(delay, req_id, request):
                await asyncio.sleep(delay * 0.1)  # æ¯ä¸ªè¯·æ±‚é—´éš”100ms
                return await single_request(req_id, request)
            
            tasks = [delayed_request(i, i, req) for i, req in enumerate(requests)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            total_time = time.time() - start_time
            
            # åˆ†æç»“æœ
            successful_requests = [r for r in results if isinstance(r, dict) and r.get('success', False)]
            failed_requests = len(results) - len(successful_requests)
            
            if successful_requests:
                response_times = [r['response_time'] for r in successful_requests]
                total_tokens = sum(r['estimated_tokens'] for r in successful_requests)
                
                avg_response_time = statistics.mean(response_times)
                max_response_time = max(response_times)
                min_response_time = min(response_times)
                
                # è®¡ç®—ååé‡
                requests_per_second = len(successful_requests) / total_time
                tokens_per_second = total_tokens / total_time
                
                self.performance_results.update({
                    'concurrent_total_time': total_time,
                    'concurrent_successful': len(successful_requests),
                    'concurrent_failed': failed_requests,
                    'concurrent_rps': requests_per_second,
                    'concurrent_tps': tokens_per_second,
                    'concurrent_avg_response': avg_response_time,
                    'concurrent_min_response': min_response_time,
                    'concurrent_max_response': max_response_time
                })
                
                details.append(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}s")
                details.append(f"æˆåŠŸè¯·æ±‚: {len(successful_requests)}/{concurrent_requests}")
                details.append(f"å¤±è´¥è¯·æ±‚: {failed_requests}")
                details.append(f"è¯·æ±‚ååé‡: {requests_per_second:.2f} req/s")
                details.append(f"Token ååé‡: {tokens_per_second:.1f} tokens/s")
                details.append(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
                details.append(f"æœ€å°å“åº”æ—¶é—´: {min_response_time:.2f}s")
                details.append(f"æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.2f}s")
                
                # è¯¦ç»†çš„æ¯ä¸ªè¯·æ±‚ç»“æœ
                for i, result in enumerate(successful_requests):
                    details.append(f"è¯·æ±‚ {result['id']+1}: {result['response_time']:.2f}s, {result['estimated_tokens']} tokens")
                
                success_rate = len(successful_requests) / concurrent_requests * 100
                return True, f"å¹¶å‘æµ‹è¯•å®Œæˆ ({success_rate:.0f}% æˆåŠŸç‡, {requests_per_second:.2f} req/s)", details
            else:
                return False, "æ‰€æœ‰å¹¶å‘è¯·æ±‚éƒ½å¤±è´¥äº†", details
                
        except Exception as e:
            return False, f"å¹¶å‘æµ‹è¯•å¤±è´¥: {str(e)}", [str(e)]
    
    def generate_html_report(self, output_file: str = None):
        """ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"compatibility_report_{timestamp}.html"
            
        html_template = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è·¨å¹³å°å…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}
        
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}
        
        .header h1 {{
            font-size: 2.5rem;
            margin-bottom: 10px;
        }}
        
        .header p {{
            opacity: 0.9;
            font-size: 1.1rem;
        }}
        
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
            border-bottom: 1px solid #dee2e6;
        }}
        
        .summary-card {{
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        
        .summary-card h3 {{
            color: #6c757d;
            font-size: 0.9rem;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}
        
        .summary-card .value {{
            font-size: 2rem;
            font-weight: bold;
        }}
        
        .summary-card.passed .value {{ color: #28a745; }}
        .summary-card.failed .value {{ color: #dc3545; }}
        .summary-card.total .value {{ color: #007bff; }}
        .summary-card.warnings .value {{ color: #ffc107; }}
        
        .platform-info {{
            padding: 30px;
            background: white;
            border-bottom: 1px solid #dee2e6;
        }}
        
        .platform-info h2 {{
            color: #343a40;
            margin-bottom: 20px;
            font-size: 1.5rem;
        }}
        
        .info-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 15px;
        }}
        
        .info-item {{
            display: flex;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 5px;
        }}
        
        .info-label {{
            font-weight: bold;
            color: #6c757d;
            margin-right: 10px;
            min-width: 120px;
        }}
        
        .info-value {{
            color: #343a40;
        }}
        
        .test-results {{
            padding: 30px;
        }}
        
        .test-results h2 {{
            color: #343a40;
            margin-bottom: 20px;
            font-size: 1.5rem;
        }}
        
        .test-item {{
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 10px;
            margin-bottom: 20px;
            overflow: hidden;
            transition: all 0.3s ease;
        }}
        
        .test-item:hover {{
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }}
        
        .test-header {{
            padding: 20px;
            background: #f8f9fa;
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: pointer;
        }}
        
        .test-header.passed {{ background: #d4edda; }}
        .test-header.failed {{ background: #f8d7da; }}
        .test-header.warning {{ background: #fff3cd; }}
        
        .test-name {{
            font-weight: bold;
            color: #343a40;
            font-size: 1.1rem;
        }}
        
        .test-status {{
            display: flex;
            align-items: center;
            gap: 10px;
        }}
        
        .status-badge {{
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: bold;
            text-transform: uppercase;
        }}
        
        .status-badge.passed {{
            background: #28a745;
            color: white;
        }}
        
        .status-badge.failed {{
            background: #dc3545;
            color: white;
        }}
        
        .status-badge.warning {{
            background: #ffc107;
            color: #343a40;
        }}
        
        .test-duration {{
            color: #6c757d;
            font-size: 0.9rem;
        }}
        
        .test-details {{
            padding: 20px;
            background: white;
            border-top: 1px solid #dee2e6;
            display: none;
        }}
        
        .test-details.show {{
            display: block;
        }}
        
        .test-message {{
            color: #495057;
            margin-bottom: 15px;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 5px;
        }}
        
        .detail-list {{
            list-style: none;
        }}
        
        .detail-list li {{
            padding: 8px 0;
            color: #495057;
            border-bottom: 1px solid #f8f9fa;
        }}
        
        .detail-list li:last-child {{
            border-bottom: none;
        }}
        
        .detail-list li:before {{
            content: 'â–¸ ';
            color: #667eea;
            font-weight: bold;
            margin-right: 5px;
        }}
        
        .footer {{
            padding: 20px;
            text-align: center;
            background: #f8f9fa;
            color: #6c757d;
        }}
        
        .success-rate {{
            margin-top: 20px;
            padding: 20px;
            background: white;
            border-radius: 10px;
        }}
        
        .progress-bar {{
            width: 100%;
            height: 30px;
            background: #e9ecef;
            border-radius: 15px;
            overflow: hidden;
            position: relative;
        }}
        
        .progress-fill {{
            height: 100%;
            background: linear-gradient(90deg, #28a745 0%, #20c997 100%);
            transition: width 1s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }}
        
        @media (max-width: 768px) {{
            .header h1 {{
                font-size: 1.8rem;
            }}
            
            .summary {{
                grid-template-columns: 1fr 1fr;
            }}
            
            .info-grid {{
                grid-template-columns: 1fr;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸš€ è·¨å¹³å°å…¼å®¹æ€§æµ‹è¯•æŠ¥å‘Š</h1>
            <p>VLLM æ¨ç†æœåŠ¡ - è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æœ</p>
        </div>
        
        <div class="summary">
            <div class="summary-card total">
                <h3>æµ‹è¯•æ€»æ•°</h3>
                <div class="value">{{total_tests}}</div>
            </div>
            <div class="summary-card passed">
                <h3>é€šè¿‡</h3>
                <div class="value">{{passed}}</div>
            </div>
            <div class="summary-card failed">
                <h3>å¤±è´¥</h3>
                <div class="value">{{failed}}</div>
            </div>
            <div class="summary-card warnings">
                <h3>è­¦å‘Š</h3>
                <div class="value">{{warnings}}</div>
            </div>
        </div>
        
        <div class="platform-info">
            <h2>ğŸ“Š æµ‹è¯•ç¯å¢ƒ</h2>
            <div class="info-grid">
                <div class="info-item">
                    <span class="info-label">æµ‹è¯•æ—¶é—´:</span>
                    <span class="info-value">{{test_time}}</span>
                </div>
                <div class="info-item">
                    <span class="info-label">æ“ä½œç³»ç»Ÿ:</span>
                    <span class="info-value">{{platform}}</span>
                </div>
                <div class="info-item">
                    <span class="info-label">Pythonç‰ˆæœ¬:</span>
                    <span class="info-value">{{python_version}}</span>
                </div>
            </div>
            
            <div class="success-rate">
                <h3>æˆåŠŸç‡</h3>
                <div class="progress-bar">
                    <div class="progress-fill" style="width: {{success_rate}}%">
                        {{success_rate}}%
                    </div>
                </div>
            </div>
        </div>
        
        {{performance_section}}
        
        <div class="test-results">
            <h2>ğŸ§ª æµ‹è¯•ç»“æœè¯¦æƒ…</h2>
            {{test_items}}
        </div>
        
        <div class="footer">
            <p>Â© 2024 VLLM è·¨å¹³å°æ¨ç†æœåŠ¡ | è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•æŠ¥å‘Š</p>
        </div>
    </div>
    
    <script>
        // ç‚¹å‡»å±•å¼€/æ”¶èµ·æµ‹è¯•è¯¦æƒ…
        document.querySelectorAll('.test-header').forEach(header => {{
            header.addEventListener('click', () => {{
                const details = header.nextElementSibling;
                details.classList.toggle('show');
            }});
        }});
        
        // è‡ªåŠ¨å±•å¼€å¤±è´¥çš„æµ‹è¯•
        document.querySelectorAll('.test-header.failed').forEach(header => {{
            const details = header.nextElementSibling;
            details.classList.add('show');
        }});
    </script>
</body>
</html>
        """
        
        # ç”Ÿæˆæµ‹è¯•é¡¹HTML
        test_items_html = ""
        for test in self.test_results:
            status_class = test['status']
            details_html = ""
            
            if test['details']:
                details_html = "<ul class='detail-list'>"
                for detail in test['details']:
                    details_html += f"<li>{detail}</li>"
                details_html += "</ul>"
            
            test_items_html += f"""
            <div class="test-item">
                <div class="test-header {status_class}">
                    <div>
                        <div class="test-name">{test['name']}</div>
                        <div style="color: #6c757d; font-size: 0.9rem; margin-top: 5px;">{test['category']}</div>
                    </div>
                    <div class="test-status">
                        <span class="test-duration">{test['duration']}s</span>
                        <span class="status-badge {status_class}">
                            {'âœ“ é€šè¿‡' if test['status'] == 'passed' else 'âœ— å¤±è´¥'}
                        </span>
                    </div>
                </div>
                <div class="test-details">
                    <div class="test-message">{test['message']}</div>
                    {details_html}
                </div>
            </div>
            """
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = 0
        if self.summary['total_tests'] > 0:
            success_rate = round((self.summary['passed'] / self.summary['total_tests']) * 100, 1)
        
        # ç”Ÿæˆæ€§èƒ½ç»“æœéƒ¨åˆ†
        performance_section = ""
        if self.performance_results:
            performance_cards = ""
            
            if 'avg_token_speed' in self.performance_results:
                performance_cards += f"""
                <div class="summary-card">
                    <h3>Token é€Ÿåº¦</h3>
                    <div class="value" style="color: #17a2b8;">{self.performance_results['avg_token_speed']:.1f}</div>
                    <small>tokens/s</small>
                </div>
                """
            
            if 'avg_response_time' in self.performance_results:
                performance_cards += f"""
                <div class="summary-card">
                    <h3>å¹³å‡å“åº”æ—¶é—´</h3>
                    <div class="value" style="color: #6610f2;">{self.performance_results['avg_response_time']:.2f}s</div>
                    <small>ç§’</small>
                </div>
                """
            
            if 'concurrent_rps' in self.performance_results:
                performance_cards += f"""
                <div class="summary-card">
                    <h3>å¹¶å‘ååé‡</h3>
                    <div class="value" style="color: #e83e8c;">{self.performance_results['concurrent_rps']:.2f}</div>
                    <small>req/s</small>
                </div>
                """
            
            if 'model_load_time' in self.performance_results:
                performance_cards += f"""
                <div class="summary-card">
                    <h3>æ¨¡å‹åŠ è½½æ—¶é—´</h3>
                    <div class="value" style="color: #fd7e14;">{self.performance_results['model_load_time']:.2f}s</div>
                    <small>ç§’</small>
                </div>
                """
            
            if performance_cards:
                performance_section = f"""
                <div class="platform-info">
                    <h2>âš¡ æ€§èƒ½æŒ‡æ ‡</h2>
                    <div class="summary">
                        {performance_cards}
                    </div>
                </div>
                """
        
        # å¡«å……æ¨¡æ¿
        html_content = html_template.format(
            total_tests=self.summary['total_tests'],
            passed=self.summary['passed'],
            failed=self.summary['failed'],
            warnings=self.summary['warnings'],
            test_time=self.summary['test_time'],
            platform=self.summary['platform'],
            python_version=self.summary['python_version'],
            success_rate=success_rate,
            performance_section=performance_section,
            test_items=test_items_html
        )
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        return output_file
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹è¿è¡Œè·¨å¹³å°å…¼å®¹æ€§å’Œæ€§èƒ½æµ‹è¯•...")
        print("=" * 60)
        
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼ˆåŒæ­¥ï¼‰
        basic_tests = [
            ("å¹³å°æ£€æµ‹", self.test_platform_detection, "åŸºç¡€åŠŸèƒ½"),
            ("å¼•æ“å¯ç”¨æ€§æ£€æŸ¥", self.test_engine_availability, "åŸºç¡€åŠŸèƒ½"),
            ("é…ç½®ç³»ç»ŸéªŒè¯", self.test_config_validation, "æ ¸å¿ƒåŠŸèƒ½"),
            ("è·¯å¾„å…¼å®¹æ€§æµ‹è¯•", self.test_path_compatibility, "å…¼å®¹æ€§"),
            ("å¼•æ“å›é€€æœºåˆ¶", self.test_engine_fallback, "å®¹é”™æœºåˆ¶"),
        ]
        
        # æ€§èƒ½æµ‹è¯•ï¼ˆå¼‚æ­¥ï¼‰
        performance_tests = [
            ("æ¨¡å‹åŠ è½½æµ‹è¯•", self.test_model_loading, "æ€§èƒ½æµ‹è¯•"),
            ("Tokenç”Ÿæˆé€Ÿåº¦æµ‹è¯•", self.test_token_generation_speed, "æ€§èƒ½æµ‹è¯•"),
            ("å¹¶å‘ååé‡æµ‹è¯•", self.test_concurrent_throughput, "æ€§èƒ½æµ‹è¯•"),
        ]
        
        # è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•
        for test_name, test_func, category in basic_tests:
            print(f"\næ­£åœ¨è¿è¡Œ: {test_name}...", end=" ")
            result = self.run_test(test_name, test_func, category)
            
            if result['status'] == 'passed':
                print("âœ… é€šè¿‡")
            elif result['status'] == 'failed':
                print("âŒ å¤±è´¥")
            else:
                print("âš ï¸ è­¦å‘Š")
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆå¼‚æ­¥ï¼‰
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        print("=" * 60)
        
        for test_name, test_func, category in performance_tests:
            print(f"\næ­£åœ¨è¿è¡Œ: {test_name}...", end=" ")
            
            # å¼‚æ­¥æµ‹è¯•éœ€è¦ç‰¹æ®Šå¤„ç†
            try:
                start_time = time.time()
                test_result = await test_func()
                duration = round(time.time() - start_time, 3)
                
                result = {
                    'name': test_name,
                    'category': category,
                    'duration': duration
                }
                
                if isinstance(test_result, tuple):
                    success, message, details = test_result
                    result['status'] = 'passed' if success else 'failed'
                    result['message'] = message
                    result['details'] = details if isinstance(details, list) else [details]
                else:
                    result['status'] = 'passed' if test_result else 'failed'
                    result['message'] = 'æµ‹è¯•é€šè¿‡' if test_result else 'æµ‹è¯•å¤±è´¥'
                    result['details'] = []
                
                self.test_results.append(result)
                self.summary['total_tests'] += 1
                
                if result['status'] == 'passed':
                    self.summary['passed'] += 1
                    print("âœ… é€šè¿‡")
                else:
                    self.summary['failed'] += 1
                    print("âŒ å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ å¼‚å¸¸: {str(e)}")
                result = {
                    'name': test_name,
                    'category': category,
                    'status': 'failed',
                    'message': str(e),
                    'details': [traceback.format_exc()],
                    'duration': 0
                }
                self.test_results.append(result)
                self.summary['total_tests'] += 1
                self.summary['failed'] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        report_file = self.generate_html_report()
        
        print(f"\nâœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.summary['passed']}/{self.summary['total_tests']} ({round((self.summary['passed']/self.summary['total_tests']*100) if self.summary['total_tests'] > 0 else 0, 1)}%)")
        
        # å°è¯•è‡ªåŠ¨æ‰“å¼€æŠ¥å‘Š
        try:
            import webbrowser
            webbrowser.open(f'file://{os.path.abspath(report_file)}')
            print("ğŸŒ å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š")
        except:
            print(f"ğŸ’¡ è¯·æ‰‹åŠ¨æ‰“å¼€æŠ¥å‘Šæ–‡ä»¶æŸ¥çœ‹: {report_file}")
        
        return report_file


async def main():
    """ä¸»å‡½æ•°"""
    reporter = CompatibilityTestReporter()
    report_file = await reporter.run_all_tests()
    
    print("\n" + "=" * 60)
    print("âœ¨ æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))