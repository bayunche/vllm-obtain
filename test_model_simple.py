#!/usr/bin/env python3
"""
ç®€å•çš„æ¨¡å‹åŠ è½½å’Œæ¨ç†æµ‹è¯•
"""

import asyncio
import time
from src.engines.mlx_engine import MlxEngine
from src.core.inference_engine import EngineConfig, InferenceRequest

async def main():
    print("="*60)
    print("ğŸš€ ç®€å•æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºå¼•æ“é…ç½®
    config = EngineConfig(
        engine_type='mlx',
        device_type='mps',
        max_gpu_memory=0.8,
        max_cpu_threads=8,
        max_sequence_length=2048
    )
    
    # åˆå§‹åŒ–å¼•æ“
    print("\n1. åˆå§‹åŒ–MLXå¼•æ“...")
    engine = MlxEngine(config)
    init_success = await engine.initialize()
    
    if not init_success:
        print("âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        return False
    print("âœ… å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
    
    # åŠ è½½æ¨¡å‹
    print("\n2. åŠ è½½æ¨¡å‹...")
    model_path = './models/qwen-0.5b'
    model_name = 'qwen-test'
    
    start_time = time.time()
    load_success = await engine.load_model(model_name, model_path)
    load_time = time.time() - start_time
    
    if not load_success:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {model_path}")
        return False
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}ç§’)")
    
    # æµ‹è¯•æ¨ç†
    print("\n3. æµ‹è¯•æ¨ç†åŠŸèƒ½...")
    test_prompts = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "Pythonç¼–ç¨‹è¯­è¨€æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    ]
    
    all_success = True
    token_speeds = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\næµ‹è¯• {i}/{len(test_prompts)}: {prompt}")
        
        request = InferenceRequest(
            model_name=model_name,
            prompt=prompt,
            max_tokens=50,
            temperature=0.7
        )
        
        try:
            start_time = time.time()
            response = await engine.generate(request)
            inference_time = time.time() - start_time
            
            # è®¡ç®—tokené€Ÿåº¦
            total_tokens = len(response.text.split())  # ç®€å•ä¼°ç®—
            tokens_per_second = total_tokens / inference_time if inference_time > 0 else 0
            token_speeds.append(tokens_per_second)
            
            print(f"âœ… æ¨ç†æˆåŠŸ")
            print(f"   å“åº”: {response.text[:100]}...")
            print(f"   è€—æ—¶: {inference_time:.2f}ç§’")
            print(f"   é€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            all_success = False
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
    print(f"æ¨ç†æµ‹è¯•: {'å…¨éƒ¨é€šè¿‡' if all_success else 'éƒ¨åˆ†å¤±è´¥'}")
    
    if token_speeds:
        avg_speed = sum(token_speeds) / len(token_speeds)
        print(f"å¹³å‡Tokené€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")
    
    # å¸è½½æ¨¡å‹
    print("\n4. å¸è½½æ¨¡å‹...")
    unload_success = await engine.unload_model(model_name)
    if unload_success:
        print("âœ… æ¨¡å‹å¸è½½æˆåŠŸ")
    else:
        print("âŒ æ¨¡å‹å¸è½½å¤±è´¥")
    
    return all_success

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)