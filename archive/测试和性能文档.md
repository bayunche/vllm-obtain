# 🧪 VLLM 跨平台推理服务 - 测试和性能文档

## 📋 目录

- [测试框架](#测试框架)
- [单元测试](#单元测试)
- [集成测试](#集成测试)
- [OpenAI API 兼容性测试](#openai-api-兼容性测试)
- [性能测试](#性能测试)
- [压力测试](#压力测试)
- [跨平台测试](#跨平台测试)
- [自动化测试](#自动化测试)
- [性能基准](#性能基准)
- [监控和分析](#监控和分析)

---

## 🧪 测试框架

### 测试环境准备

```bash
# 安装测试依赖
pip install pytest pytest-cov pytest-asyncio pytest-mock
pip install locust  # 压力测试
pip install httpx   # 异步 HTTP 客户端

# 创建测试目录结构
mkdir -p tests/{unit,integration,performance,compatibility}
```

### 测试配置

```python
# tests/conftest.py
import pytest
import asyncio
from src.utils import get_config, setup_logger

@pytest.fixture(scope="session")
def event_loop():
    """创建事件循环"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def test_config():
    """测试配置"""
    return get_config()

@pytest.fixture
def test_logger():
    """测试日志器"""
    return setup_logger()
```

---

## 🔬 单元测试

### 平台检测器测试

```python
# tests/unit/test_platform_detector.py
import pytest
from src.utils.platform_detector import PlatformDetector

class TestPlatformDetector:
    
    def test_get_platform_info(self):
        """测试平台信息获取"""
        detector = PlatformDetector()
        info = detector.get_platform_info()
        
        assert 'system' in info
        assert 'machine' in info
        assert info['system'] in ['linux', 'darwin', 'windows']
    
    def test_detect_best_engine(self):
        """测试最佳引擎检测"""
        detector = PlatformDetector()
        engine = detector.detect_best_engine()
        
        assert engine in ['vllm', 'mlx', 'llama_cpp']
    
    def test_force_engine_selection(self):
        """测试强制引擎选择"""
        detector = PlatformDetector()
        engine = detector.detect_best_engine(force_engine='llama_cpp')
        
        assert engine == 'llama_cpp'
```

### 配置管理测试

```python
# tests/unit/test_config.py
import pytest
import os
from src.utils.config import ConfigManager, InferenceConfig

class TestConfigManager:
    
    def test_load_default_config(self):
        """测试默认配置加载"""
        config = ConfigManager.load_config()
        
        assert isinstance(config, InferenceConfig)
        assert config.host == "0.0.0.0"
        assert config.port == 8000
    
    def test_environment_variable_override(self):
        """测试环境变量覆盖"""
        os.environ['HOST'] = '127.0.0.1'
        os.environ['PORT'] = '9000'
        
        config = ConfigManager.load_config()
        
        assert config.host == '127.0.0.1'
        assert config.port == 9000
        
        # 清理环境变量
        del os.environ['HOST']
        del os.environ['PORT']
    
    def test_config_validation(self):
        """测试配置验证"""
        with pytest.raises(ValueError):
            InferenceConfig(inference_engine='invalid_engine')
```

### 推理引擎测试

```python
# tests/unit/test_inference_engine.py
import pytest
import asyncio
from src.core.inference_engine import DummyEngine, EngineConfig, InferenceRequest

class TestInferenceEngine:
    
    @pytest.mark.asyncio
    async def test_dummy_engine_initialization(self):
        """测试虚拟引擎初始化"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        
        success = await engine.initialize()
        assert success is True
        assert engine._initialized is True
    
    @pytest.mark.asyncio
    async def test_model_loading(self):
        """测试模型加载"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        await engine.initialize()
        
        success = await engine.load_model("test-model", "/fake/path")
        assert success is True
        assert engine.is_model_loaded("test-model") is True
    
    @pytest.mark.asyncio
    async def test_inference(self):
        """测试推理功能"""
        config = EngineConfig(engine_type="dummy")
        engine = DummyEngine(config)
        await engine.initialize()
        await engine.load_model("test-model", "/fake/path")
        
        request = InferenceRequest(
            model_name="test-model",
            prompt="Hello, world!",
            max_tokens=50
        )
        
        response = await engine.generate(request)
        
        assert response.model_name == "test-model"
        assert response.text is not None
        assert response.prompt_tokens > 0
        assert response.completion_tokens > 0
```

---

## 🔗 集成测试

### API 集成测试

```python
# tests/integration/test_api_integration.py
import pytest
import asyncio
from httpx import AsyncClient
from src.api.app import create_app

class TestAPIIntegration:
    
    @pytest.fixture
    async def client(self):
        """创建测试客户端"""
        app = create_app()
        async with AsyncClient(app=app, base_url="http://test") as client:
            yield client
    
    @pytest.mark.asyncio
    async def test_health_check(self, client):
        """测试健康检查接口"""
        response = await client.get("/health")
        assert response.status_code == 200
        
        data = response.json()
        assert "status" in data
    
    @pytest.mark.asyncio
    async def test_models_endpoint(self, client):
        """测试模型列表接口"""
        response = await client.get("/v1/models")
        assert response.status_code == 200
        
        data = response.json()
        assert data["object"] == "list"
        assert "data" in data
    
    @pytest.mark.asyncio
    async def test_chat_completions(self, client):
        """测试聊天补全接口"""
        payload = {
            "model": "test-model",
            "messages": [{"role": "user", "content": "Hello!"}],
            "max_tokens": 50
        }
        
        response = await client.post("/v1/chat/completions", json=payload)
        
        if response.status_code == 200:
            data = response.json()
            assert data["object"] == "chat.completion"
            assert "choices" in data
            assert "usage" in data
```

---

## ✅ OpenAI API 兼容性测试

### 扩展兼容性测试脚本

```python
# tests/compatibility/test_openai_full_compatibility.py
import pytest
import json
import time
from openai import OpenAI

class TestOpenAICompatibility:
    
    @pytest.fixture
    def client(self):
        """OpenAI 客户端"""
        return OpenAI(
            api_key="test-key",
            base_url="http://localhost:8000/v1"
        )
    
    def test_models_list(self, client):
        """测试模型列表兼容性"""
        models = client.models.list()
        
        assert hasattr(models, 'data')
        for model in models.data:
            assert hasattr(model, 'id')
            assert hasattr(model, 'object')
            assert model.object == 'model'
    
    def test_chat_completion(self, client):
        """测试聊天补全兼容性"""
        response = client.chat.completions.create(
            model="test-model",
            messages=[
                {"role": "user", "content": "测试消息"}
            ],
            max_tokens=50
        )
        
        assert hasattr(response, 'choices')
        assert len(response.choices) > 0
        assert hasattr(response.choices[0], 'message')
        assert hasattr(response.choices[0].message, 'content')
        assert hasattr(response, 'usage')
    
    def test_streaming_chat(self, client):
        """测试流式聊天兼容性"""
        stream = client.chat.completions.create(
            model="test-model",
            messages=[{"role": "user", "content": "测试流式"}],
            stream=True
        )
        
        chunks = []
        for chunk in stream:
            chunks.append(chunk)
            if len(chunks) > 10:  # 限制测试长度
                break
        
        assert len(chunks) > 0
        # 验证流式数据格式
        for chunk in chunks:
            assert hasattr(chunk, 'choices')
```

---

## 🚀 性能测试

### 基础性能测试

```python
# tests/performance/test_basic_performance.py
import pytest
import time
import asyncio
from httpx import AsyncClient
from statistics import mean, median

class TestBasicPerformance:
    
    @pytest.mark.asyncio
    async def test_response_time(self):
        """测试响应时间"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            response_times = []
            
            for _ in range(10):
                start_time = time.time()
                response = await client.get("/health")
                end_time = time.time()
                
                assert response.status_code == 200
                response_times.append(end_time - start_time)
            
            avg_time = mean(response_times)
            median_time = median(response_times)
            
            print(f"平均响应时间: {avg_time:.3f}s")
            print(f"中位数响应时间: {median_time:.3f}s")
            
            # 健康检查应该在100ms内完成
            assert avg_time < 0.1
    
    @pytest.mark.asyncio
    async def test_inference_performance(self):
        """测试推理性能"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            payload = {
                "model": "test-model",
                "messages": [{"role": "user", "content": "简短回复测试"}],
                "max_tokens": 20
            }
            
            inference_times = []
            
            for _ in range(5):
                start_time = time.time()
                response = await client.post("/v1/chat/completions", json=payload)
                end_time = time.time()
                
                if response.status_code == 200:
                    data = response.json()
                    inference_time = end_time - start_time
                    tokens_generated = data['usage']['completion_tokens']
                    tokens_per_second = tokens_generated / inference_time
                    
                    inference_times.append(tokens_per_second)
                    print(f"推理速度: {tokens_per_second:.1f} tokens/s")
            
            if inference_times:
                avg_speed = mean(inference_times)
                print(f"平均推理速度: {avg_speed:.1f} tokens/s")
```

### 内存使用测试

```python
# tests/performance/test_memory_usage.py
import pytest
import psutil
import os
import time

class TestMemoryUsage:
    
    def test_memory_baseline(self):
        """测试内存基线"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"初始内存使用: {initial_memory:.1f} MB")
        
        # 模拟一些操作
        time.sleep(1)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"最终内存使用: {final_memory:.1f} MB")
        print(f"内存增长: {memory_increase:.1f} MB")
        
        # 内存增长不应超过100MB
        assert memory_increase < 100
    
    def test_gpu_memory_usage(self):
        """测试GPU内存使用"""
        try:
            import torch
            if torch.cuda.is_available():
                initial_gpu = torch.cuda.memory_allocated() / 1024**3  # GB
                print(f"初始GPU内存: {initial_gpu:.2f} GB")
                
                # 这里可以添加模型加载测试
                # 暂时跳过实际GPU测试
                assert True
            else:
                pytest.skip("CUDA不可用，跳过GPU内存测试")
        except ImportError:
            pytest.skip("PyTorch未安装，跳过GPU内存测试")
```

---

## 💥 压力测试

### Locust 压力测试脚本

```python
# tests/stress/locustfile.py
from locust import HttpUser, task, between
import json
import random

class InferenceUser(HttpUser):
    wait_time = between(1, 3)  # 请求间隔1-3秒
    
    def on_start(self):
        """测试开始时执行"""
        # 检查服务是否可用
        response = self.client.get("/health")
        if response.status_code != 200:
            print("服务不可用，停止测试")
            self.environment.runner.quit()
    
    @task(3)
    def test_health_check(self):
        """健康检查 (权重3)"""
        self.client.get("/health")
    
    @task(5)
    def test_models_list(self):
        """模型列表 (权重5)"""
        self.client.get("/v1/models")
    
    @task(10)
    def test_chat_completion(self):
        """聊天补全 (权重10)"""
        messages = [
            "你好",
            "请介绍一下人工智能",
            "写一个Python函数",
            "解释机器学习的概念",
            "今天天气怎么样？"
        ]
        
        payload = {
            "model": "test-model",
            "messages": [{"role": "user", "content": random.choice(messages)}],
            "max_tokens": random.randint(20, 100),
            "temperature": random.uniform(0.1, 0.9)
        }
        
        with self.client.post(
            "/v1/chat/completions",
            json=payload,
            catch_response=True
        ) as response:
            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    response.success()
                else:
                    response.failure("响应格式错误")
            else:
                response.failure(f"状态码: {response.status_code}")
    
    @task(2)
    def test_system_status(self):
        """系统状态 (权重2)"""
        self.client.get("/v1/system/status")

class ConcurrentUser(HttpUser):
    """高并发用户"""
    wait_time = between(0.1, 0.5)  # 更短的等待时间
    
    @task
    def rapid_requests(self):
        """快速请求"""
        endpoints = ["/health", "/v1/models", "/v1/system/status"]
        endpoint = random.choice(endpoints)
        self.client.get(endpoint)
```

### 压力测试运行脚本

```bash
# tests/stress/run_stress_test.sh
#!/bin/bash

echo "🚀 开始 VLLM 推理服务压力测试"

# 确保服务运行
echo "检查服务状态..."
if ! curl -s http://localhost:8000/health > /dev/null; then
    echo "❌ 服务未运行，请先启动服务"
    exit 1
fi

echo "✅ 服务运行正常"

# 创建结果目录
mkdir -p stress_test_results

# 基础压力测试 (10个用户，60秒)
echo "📊 运行基础压力测试..."
locust -f tests/stress/locustfile.py \
    --users 10 \
    --spawn-rate 2 \
    --run-time 60s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/basic_test.html \
    --csv stress_test_results/basic_test

# 中等压力测试 (50个用户，120秒)
echo "📊 运行中等压力测试..."
locust -f tests/stress/locustfile.py \
    --users 50 \
    --spawn-rate 5 \
    --run-time 120s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/medium_test.html \
    --csv stress_test_results/medium_test

# 高压力测试 (100个用户，180秒)
echo "📊 运行高压力测试..."
locust -f tests/stress/locustfile.py \
    --users 100 \
    --spawn-rate 10 \
    --run-time 180s \
    --host http://localhost:8000 \
    --headless \
    --html stress_test_results/high_test.html \
    --csv stress_test_results/high_test

echo "🎉 压力测试完成！结果保存在 stress_test_results/ 目录"
```

### 压力测试分析脚本

```python
# tests/stress/analyze_results.py
import pandas as pd
import matplotlib.pyplot as plt
import argparse

def analyze_stress_test(csv_file):
    """分析压力测试结果"""
    
    # 读取测试数据
    stats_df = pd.read_csv(f"{csv_file}_stats.csv")
    history_df = pd.read_csv(f"{csv_file}_stats_history.csv")
    
    print("📊 压力测试结果分析")
    print("=" * 50)
    
    # 整体统计
    print("🎯 整体性能指标:")
    print(f"总请求数: {stats_df['Request Count'].sum()}")
    print(f"失败请求数: {stats_df['Failure Count'].sum()}")
    print(f"平均响应时间: {stats_df['Average Response Time'].mean():.2f}ms")
    print(f"95%响应时间: {stats_df['95%'].mean():.2f}ms")
    print(f"99%响应时间: {stats_df['99%'].mean():.2f}ms")
    print(f"请求失败率: {(stats_df['Failure Count'].sum() / stats_df['Request Count'].sum() * 100):.2f}%")
    
    # 各端点性能
    print("\n🔍 各端点性能:")
    for _, row in stats_df.iterrows():
        print(f"{row['Name']}: {row['Average Response Time']:.2f}ms (RPS: {row['Requests/s']:.2f})")
    
    # 生成图表
    plt.figure(figsize=(15, 10))
    
    # 响应时间趋势
    plt.subplot(2, 2, 1)
    plt.plot(history_df['Timestamp'], history_df['95%'])
    plt.title('95%响应时间趋势')
    plt.xlabel('时间')
    plt.ylabel('响应时间 (ms)')
    
    # RPS趋势
    plt.subplot(2, 2, 2)
    plt.plot(history_df['Timestamp'], history_df['Requests/s'])
    plt.title('每秒请求数 (RPS) 趋势')
    plt.xlabel('时间')
    plt.ylabel('RPS')
    
    # 用户数趋势
    plt.subplot(2, 2, 3)
    plt.plot(history_df['Timestamp'], history_df['User Count'])
    plt.title('并发用户数趋势')
    plt.xlabel('时间')
    plt.ylabel('用户数')
    
    # 失败率趋势
    plt.subplot(2, 2, 4)
    failure_rate = (history_df['Failures/s'] / history_df['Requests/s'] * 100).fillna(0)
    plt.plot(history_df['Timestamp'], failure_rate)
    plt.title('失败率趋势')
    plt.xlabel('时间')
    plt.ylabel('失败率 (%)')
    
    plt.tight_layout()
    plt.savefig(f"{csv_file}_analysis.png", dpi=300, bbox_inches='tight')
    print(f"\n📈 图表已保存为 {csv_file}_analysis.png")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='分析压力测试结果')
    parser.add_argument('csv_file', help='CSV文件路径前缀')
    args = parser.parse_args()
    
    analyze_stress_test(args.csv_file)
```

---

## 🌍 跨平台测试

### 平台兼容性测试矩阵

```python
# tests/platform/test_cross_platform.py
import pytest
import platform
import subprocess

class TestCrossPlatform:
    
    def test_platform_detection(self):
        """测试平台检测准确性"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        platform_info = detector.get_platform_info()
        
        # 验证平台信息
        expected_system = platform.system().lower()
        assert platform_info['system'] == expected_system
    
    @pytest.mark.skipif(platform.system() != "Darwin", reason="仅在macOS上运行")
    def test_mlx_availability_macos(self):
        """测试macOS上MLX可用性"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        
        if platform.machine().lower() in ['arm64', 'aarch64']:
            # Apple Silicon 应该推荐MLX
            engine = detector.detect_best_engine()
            assert engine in ['mlx', 'llama_cpp']  # MLX或回退到llama_cpp
    
    @pytest.mark.skipif(platform.system() == "Darwin", reason="非macOS平台运行")
    def test_vllm_availability_linux_windows(self):
        """测试Linux/Windows上VLLM可用性"""
        from src.utils.platform_detector import PlatformDetector
        
        detector = PlatformDetector()
        engine = detector.detect_best_engine()
        
        # Linux/Windows应该优先选择VLLM或回退到llama_cpp
        assert engine in ['vllm', 'llama_cpp']
    
    def test_python_version_compatibility(self):
        """测试Python版本兼容性"""
        import sys
        
        # 要求Python 3.11+
        assert sys.version_info >= (3, 11), f"Python版本 {sys.version} 不受支持"
    
    def test_dependencies_installation(self):
        """测试依赖安装"""
        required_packages = [
            'flask', 'loguru', 'pydantic', 'psutil', 'requests'
        ]
        
        for package in required_packages:
            try:
                __import__(package.replace('-', '_'))
            except ImportError:
                pytest.fail(f"必需包 {package} 未安装")
```

---

## 🤖 自动化测试

### GitHub Actions 工作流

```yaml
# .github/workflows/test.yml
name: 测试和质量检查

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: 设置Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: 安装依赖
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio
    
    - name: 运行单元测试
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml
    
    - name: 运行集成测试
      run: |
        pytest tests/integration/ -v
    
    - name: 上传覆盖率报告
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
  
  performance:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: 设置Python
      uses: actions/setup-python@v3
      with:
        python-version: 3.11
    
    - name: 安装依赖
      run: |
        pip install -r requirements.txt
        pip install locust
    
    - name: 启动服务
      run: |
        python run.py --mode dev &
        sleep 30
    
    - name: 运行性能测试
      run: |
        pytest tests/performance/ -v
    
    - name: 运行压力测试
      run: |
        locust -f tests/stress/locustfile.py \
          --users 10 --spawn-rate 2 --run-time 60s \
          --host http://localhost:8000 --headless
```

### 本地测试脚本

```bash
# scripts/run_all_tests.sh
#!/bin/bash

set -e

echo "🧪 运行完整测试套件"

# 检查虚拟环境
if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "⚠️ 请先激活虚拟环境"
    exit 1
fi

echo "📋 安装测试依赖..."
pip install pytest pytest-cov pytest-asyncio pytest-mock locust

echo "🔬 运行单元测试..."
pytest tests/unit/ -v --cov=src --cov-report=html --cov-report=term

echo "🔗 运行集成测试..."
pytest tests/integration/ -v

echo "🌍 运行跨平台测试..."
pytest tests/platform/ -v

echo "✅ 运行OpenAI兼容性测试..."
python test_openai_compatibility.py

echo "🚀 启动服务进行性能测试..."
python run.py --mode dev --skip-check &
SERVER_PID=$!
sleep 10

echo "📊 运行性能测试..."
pytest tests/performance/ -v

echo "💥 运行压力测试..."
locust -f tests/stress/locustfile.py \
    --users 20 --spawn-rate 4 --run-time 60s \
    --host http://localhost:8000 --headless \
    --html test_results/stress_test.html

echo "🧹 清理测试环境..."
kill $SERVER_PID 2>/dev/null || true

echo "🎉 所有测试完成！"
echo "📊 查看覆盖率报告: htmlcov/index.html"
echo "📈 查看压力测试报告: test_results/stress_test.html"
```

---

## 📊 性能基准

### 基准测试配置

```python
# benchmarks/benchmark_config.py
BENCHMARK_CONFIGS = {
    "small_model": {
        "model_size": "7B",
        "test_prompts": [
            "你好",
            "请简单介绍一下AI",
            "1+1等于几？"
        ],
        "max_tokens": 50,
        "expected_speed": 20  # tokens/s
    },
    "medium_model": {
        "model_size": "13B", 
        "test_prompts": [
            "请详细解释机器学习的概念",
            "写一个Python快速排序算法",
            "分析当前AI技术的发展趋势"
        ],
        "max_tokens": 200,
        "expected_speed": 10
    },
    "large_model": {
        "model_size": "70B",
        "test_prompts": [
            "请写一篇关于人工智能未来发展的详细文章",
            "设计一个完整的Web应用架构",
            "分析全球气候变化的影响和应对措施"
        ],
        "max_tokens": 500,
        "expected_speed": 5
    }
}

HARDWARE_CONFIGS = {
    "cpu_only": {
        "device": "cpu",
        "engine": "llama_cpp",
        "memory_limit": "16GB"
    },
    "gpu_consumer": {
        "device": "cuda",
        "engine": "vllm", 
        "gpu_memory": "RTX 4090 24GB"
    },
    "gpu_enterprise": {
        "device": "cuda",
        "engine": "vllm",
        "gpu_memory": "A100 80GB"
    },
    "apple_silicon": {
        "device": "mps",
        "engine": "mlx",
        "unified_memory": "64GB+"
    }
}
```

### 性能基准测试

```python
# benchmarks/run_benchmarks.py
import asyncio
import time
import json
from statistics import mean, median
from httpx import AsyncClient

async def benchmark_inference_speed():
    """基准推理速度测试"""
    
    test_cases = [
        {"prompt": "你好", "max_tokens": 10, "category": "short"},
        {"prompt": "请介绍机器学习", "max_tokens": 100, "category": "medium"},
        {"prompt": "详细分析AI发展趋势", "max_tokens": 300, "category": "long"}
    ]
    
    results = {}
    
    async with AsyncClient(base_url="http://localhost:8000") as client:
        for case in test_cases:
            category = case["category"]
            speeds = []
            
            print(f"测试 {category} 类别...")
            
            for i in range(5):  # 每个类别测试5次
                payload = {
                    "model": "test-model",
                    "messages": [{"role": "user", "content": case["prompt"]}],
                    "max_tokens": case["max_tokens"]
                }
                
                start_time = time.time()
                response = await client.post("/v1/chat/completions", json=payload)
                end_time = time.time()
                
                if response.status_code == 200:
                    data = response.json()
                    inference_time = end_time - start_time
                    completion_tokens = data['usage']['completion_tokens']
                    speed = completion_tokens / inference_time
                    speeds.append(speed)
                    
                    print(f"  第{i+1}次: {speed:.1f} tokens/s")
            
            if speeds:
                results[category] = {
                    "avg_speed": mean(speeds),
                    "median_speed": median(speeds),
                    "min_speed": min(speeds),
                    "max_speed": max(speeds),
                    "samples": len(speeds)
                }
    
    return results

async def benchmark_concurrent_requests():
    """并发请求基准测试"""
    
    concurrent_levels = [1, 5, 10, 20, 50]
    results = {}
    
    for concurrency in concurrent_levels:
        print(f"测试并发级别: {concurrency}")
        
        async def single_request(client, request_id):
            payload = {
                "model": "test-model",
                "messages": [{"role": "user", "content": f"请求 {request_id}"}],
                "max_tokens": 50
            }
            
            start_time = time.time()
            response = await client.post("/v1/chat/completions", json=payload)
            end_time = time.time()
            
            return {
                "request_id": request_id,
                "status_code": response.status_code,
                "response_time": end_time - start_time,
                "success": response.status_code == 200
            }
        
        async with AsyncClient(base_url="http://localhost:8000") as client:
            start_time = time.time()
            
            tasks = [
                single_request(client, i) 
                for i in range(concurrency)
            ]
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # 分析结果
            successful_responses = [r for r in responses if isinstance(r, dict) and r['success']]
            failed_responses = [r for r in responses if not (isinstance(r, dict) and r['success'])]
            
            if successful_responses:
                avg_response_time = mean([r['response_time'] for r in successful_responses])
                throughput = len(successful_responses) / total_time
            else:
                avg_response_time = 0
                throughput = 0
            
            results[concurrency] = {
                "total_requests": concurrency,
                "successful_requests": len(successful_responses),
                "failed_requests": len(failed_responses),
                "success_rate": len(successful_responses) / concurrency * 100,
                "avg_response_time": avg_response_time,
                "throughput": throughput,
                "total_time": total_time
            }
            
            print(f"  成功率: {results[concurrency]['success_rate']:.1f}%")
            print(f"  吞吐量: {throughput:.1f} req/s")
    
    return results

def save_benchmark_results(results, filename):
    """保存基准测试结果"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"结果已保存到: {filename}")

async def main():
    """主测试函数"""
    print("🚀 开始性能基准测试")
    
    # 推理速度测试
    print("\n📊 推理速度基准测试...")
    speed_results = await benchmark_inference_speed()
    
    # 并发请求测试
    print("\n📊 并发请求基准测试...")
    concurrent_results = await benchmark_concurrent_requests()
    
    # 保存结果
    all_results = {
        "timestamp": time.time(),
        "inference_speed": speed_results,
        "concurrent_requests": concurrent_results
    }
    
    save_benchmark_results(all_results, f"benchmark_results_{int(time.time())}.json")
    
    # 打印总结
    print("\n📈 基准测试总结:")
    print("=" * 50)
    
    for category, data in speed_results.items():
        print(f"{category}: {data['avg_speed']:.1f} tokens/s (平均)")
    
    print("\n并发性能:")
    for concurrency, data in concurrent_results.items():
        print(f"{concurrency} 并发: {data['throughput']:.1f} req/s ({data['success_rate']:.1f}% 成功率)")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 📈 监控和分析

### 实时性能监控

```python
# monitoring/performance_monitor.py
import time
import json
import psutil
import threading
from datetime import datetime
from collections import deque

class PerformanceMonitor:
    
    def __init__(self, window_size=300):  # 5分钟窗口
        self.window_size = window_size
        self.metrics_history = deque(maxlen=window_size)
        self.running = False
        self.thread = None
    
    def start_monitoring(self):
        """开始监控"""
        self.running = True
        self.thread = threading.Thread(target=self._monitoring_loop)
        self.thread.daemon = True
        self.thread.start()
        print("📊 性能监控已启动")
    
    def stop_monitoring(self):
        """停止监控"""
        self.running = False
        if self.thread:
            self.thread.join()
        print("📊 性能监控已停止")
    
    def _monitoring_loop(self):
        """监控循环"""
        while self.running:
            metrics = self._collect_metrics()
            self.metrics_history.append(metrics)
            time.sleep(1)  # 每秒收集一次
    
    def _collect_metrics(self):
        """收集系统指标"""
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_used_gb": memory.used / 1024**3,
            "memory_total_gb": memory.total / 1024**3,
            "disk_percent": (disk.used / disk.total) * 100,
            "disk_free_gb": disk.free / 1024**3
        }
        
        # 添加GPU指标 (如果可用)
        try:
            import torch
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024**3
                gpu_cached = torch.cuda.memory_reserved() / 1024**3
                metrics.update({
                    "gpu_memory_used_gb": gpu_memory,
                    "gpu_memory_cached_gb": gpu_cached
                })
        except ImportError:
            pass
        
        return metrics
    
    def get_current_metrics(self):
        """获取当前指标"""
        if self.metrics_history:
            return self.metrics_history[-1]
        return self._collect_metrics()
    
    def get_average_metrics(self, minutes=5):
        """获取平均指标"""
        if not self.metrics_history:
            return None
        
        # 获取最近N分钟的数据
        recent_data = list(self.metrics_history)[-minutes*60:]
        
        if not recent_data:
            return None
        
        avg_metrics = {}
        numeric_keys = ['cpu_percent', 'memory_percent', 'disk_percent']
        
        for key in numeric_keys:
            values = [m[key] for m in recent_data if key in m]
            if values:
                avg_metrics[f"avg_{key}"] = sum(values) / len(values)
        
        return avg_metrics
    
    def export_metrics(self, filename):
        """导出指标数据"""
        data = {
            "export_time": datetime.now().isoformat(),
            "metrics": list(self.metrics_history)
        }
        
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"指标数据已导出到: {filename}")

# 使用示例
if __name__ == "__main__":
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    try:
        # 运行一段时间
        time.sleep(60)
        
        # 获取当前指标
        current = monitor.get_current_metrics()
        print("当前指标:", current)
        
        # 获取平均指标
        average = monitor.get_average_metrics()
        print("平均指标:", average)
        
    finally:
        monitor.stop_monitoring()
        monitor.export_metrics("performance_data.json")
```

---

## 🎯 测试执行指南

### 日常开发测试

```bash
# 快速测试 (开发时使用)
pytest tests/unit/ -v

# 完整单元测试
pytest tests/unit/ -v --cov=src --cov-report=html

# 集成测试
pytest tests/integration/ -v
```

### 发布前测试

```bash
# 完整测试套件
./scripts/run_all_tests.sh

# OpenAI兼容性验证
python test_openai_compatibility.py

# 性能基准测试
python benchmarks/run_benchmarks.py
```

### 生产环境监控

```bash
# 压力测试
locust -f tests/stress/locustfile.py \
    --users 100 --spawn-rate 10 --run-time 300s \
    --host http://your-production-server:8000 \
    --headless --html production_stress_test.html

# 长期监控
python monitoring/performance_monitor.py
```

---

## 📋 测试检查清单

### ✅ 发布前检查

- [ ] 所有单元测试通过
- [ ] 集成测试通过
- [ ] OpenAI API 兼容性测试通过
- [ ] 跨平台测试通过 (至少2个平台)
- [ ] 性能测试达标 (响应时间 < 3s)
- [ ] 压力测试通过 (100并发，成功率 > 95%)
- [ ] 内存使用正常 (无明显内存泄漏)
- [ ] 文档更新完整

### 🎯 性能目标

| 指标 | 目标值 | 验证方法 |
|------|--------|----------|
| 响应时间 | < 3秒 | 性能测试 |
| 吞吐量 | > 50 req/min | 压力测试 |
| 成功率 | > 95% | 压力测试 |
| 内存使用 | 稳定 | 长期监控 |
| CPU使用 | < 80% | 系统监控 |

这份测试文档提供了完整的测试策略，从单元测试到压力测试，确保服务的稳定性和性能。