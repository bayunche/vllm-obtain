# VLLM è·¨å¹³å°æ¨ç†æœåŠ¡ - Mac Studio ç”Ÿäº§éƒ¨ç½²æŒ‡å—

## ğŸŒŸ é¡¹ç›®æ¦‚è¿°

VLLM è·¨å¹³å°æ¨ç†æœåŠ¡æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œä¸“ä¸ºMac Studioç­‰Apple Siliconè®¾å¤‡ä¼˜åŒ–ã€‚æ”¯æŒMLXã€VLLMã€llama.cppç­‰å¤šç§æ¨ç†å¼•æ“ï¼Œæä¾›OpenAIå…¼å®¹çš„APIæ¥å£ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **Apple Silicon ä¼˜åŒ–**: ä¸“ä¸ºMç³»åˆ—èŠ¯ç‰‡ä¼˜åŒ–çš„MLXå¼•æ“
- âœ… **OpenAI API å…¼å®¹**: å®Œå…¨å…¼å®¹OpenAI ChatGPT APIæ ¼å¼
- âœ… **å¤šå¼•æ“æ”¯æŒ**: MLXã€VLLMã€llama.cppè‡ªåŠ¨æ£€æµ‹å’Œåˆ‡æ¢
- âœ… **ç”Ÿäº§çº§éƒ¨ç½²**: Gunicorn + Geventå¼‚æ­¥å¹¶å‘å¤„ç†
- âœ… **å®Œå–„ç›‘æ§**: è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—å’Œç³»ç»Ÿç›‘æ§
- âœ… **é«˜å¹¶å‘æ”¯æŒ**: å•workeræ”¯æŒ20+å¹¶å‘è¯·æ±‚

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡ (Mac Studio M3 Ultra)

| æŒ‡æ ‡ | å¼€å‘æ¨¡å¼(Flask) | ç”Ÿäº§æ¨¡å¼(Gunicorn) |
|------|----------------|-------------------|
| æœ€å¤§å¹¶å‘ | 7 (å´©æºƒ) | 20+ (ç¨³å®š) |
| ååé‡ | 2.86 è¯·æ±‚/ç§’ | 2.64 è¯·æ±‚/ç§’ |
| æˆåŠŸç‡ | 100% (ä½å¹¶å‘) | 100% (é«˜å¹¶å‘) |
| å“åº”æ—¶é—´ | 0.25ç§’ | 0.77-7.07ç§’ |
| Tokenç”Ÿæˆé€Ÿåº¦ | 186.5 tokens/ç§’ | 0.7-6.7 tokens/ç§’ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: macOS 15.0+
- **èŠ¯ç‰‡**: Apple Mç³»åˆ— (M1/M2/M3/M4)
- **Python**: 3.8+
- **å†…å­˜**: 16GB+ (æ¨è32GB+)
- **å­˜å‚¨**: 10GB+ å¯ç”¨ç©ºé—´

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
```bash
git clone <repository-url>
cd vllm-obtain
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python3 -m venv venv
source venv/bin/activate
```

3. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

4. **ä¸‹è½½æ¨¡å‹**
```bash
# ä½¿ç”¨ ModelScope (å›½å†…æ¨è)
python -c "from modelscope import snapshot_download; snapshot_download('qwen/Qwen2.5-0.5B-Instruct', cache_dir='./models/qwen-0.5b')"
```

5. **é…ç½®ç¯å¢ƒ**
```bash
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œè®¾ç½®åˆé€‚çš„é…ç½®
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒé…ç½® (.env)

```bash
# æœåŠ¡é…ç½®
HOST=127.0.0.1
PORT=8001
DEBUG=false
WORKERS=1

# æ¨ç†å¼•æ“é…ç½®
INFERENCE_ENGINE=mlx
DEVICE_TYPE=mps
DEFAULT_MODEL=qwen-0.5b
MAX_CONCURRENT_MODELS=1

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
ENABLE_LOG_FILE=true
LOG_RETENTION_DAYS=30

# æ€§èƒ½é…ç½®
REQUEST_TIMEOUT=300
MAX_CONCURRENT_REQUESTS=100
ENABLE_CACHING=true
CACHE_SIZE=1000
```

### å…³é”®é…ç½®è¯´æ˜

- **INFERENCE_ENGINE=mlx**: Mac Studioå¿…é¡»ä½¿ç”¨MLXå¼•æ“
- **WORKERS=1**: MLXå¼•æ“åªæ”¯æŒå•workeræ¨¡å¼
- **DEVICE_TYPE=mps**: Apple Silicon Metalæ€§èƒ½ç€è‰²å™¨
- **MAX_CONCURRENT_MODELS=1**: å•æ¨¡å‹å®ä¾‹é¿å…å†…å­˜å†²çª

## ğŸ¯ éƒ¨ç½²æ¨¡å¼

### å¼€å‘æ¨¡å¼

```bash
# ä½¿ç”¨å†…ç½®å¯åŠ¨è„šæœ¬
python run.py --mode dev

# æˆ–ç›´æ¥è¿è¡Œ
python -m src.api.app
```

**ç‰¹ç‚¹**:
- è‡ªåŠ¨é‡è½½ä»£ç æ›´æ”¹
- è¯¦ç»†è°ƒè¯•æ—¥å¿—
- å•çº¿ç¨‹å¤„ç†
- é€‚åˆå¼€å‘è°ƒè¯•

### ç”Ÿäº§æ¨¡å¼ (æ¨è)

```bash
# ä½¿ç”¨å¯åŠ¨è„šæœ¬
python run.py --mode prod

# æˆ–ç›´æ¥ä½¿ç”¨Gunicorn
source venv/bin/activate
gunicorn --bind 127.0.0.1:8001 \
         --workers 1 \
         --worker-class gevent \
         --worker-connections 1000 \
         --timeout 300 \
         --keep-alive 5 \
         --max-requests 1000 \
         --max-requests-jitter 100 \
         --access-logfile - \
         --error-logfile - \
         src.api.app:application
```

**ç‰¹ç‚¹**:
- é«˜å¹¶å‘å¼‚æ­¥å¤„ç†
- ç”Ÿäº§çº§ç¨³å®šæ€§
- å®Œå–„é”™è¯¯å¤„ç†
- æ€§èƒ½ç›‘æ§

## ğŸ“¡ API ä½¿ç”¨

### å¥åº·æ£€æŸ¥

```bash
curl http://127.0.0.1:8001/health
```

### OpenAIå…¼å®¹æ¥å£

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-0.5b",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### æ¨¡å‹ç®¡ç†

```bash
# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://127.0.0.1:8001/v1/models

# ç³»ç»ŸçŠ¶æ€
curl http://127.0.0.1:8001/v1/system/status

# åŠ è½½æ¨¡å‹
curl -X POST http://127.0.0.1:8001/v1/models/load \
  -H "Content-Type: application/json" \
  -d '{"model_name": "qwen-0.5b"}'
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–

### Mac Studio ä¸“ç”¨ä¼˜åŒ–

1. **MLXå¼•æ“ä¼˜åŒ–**
```bash
# .env é…ç½®
INFERENCE_ENGINE=mlx
DEVICE_TYPE=mps
MLX_MEMORY_FRACTION=0.8
```

2. **å¹¶å‘é…ç½®**
```bash
# å•worker + Gevent
WORKERS=1
WORKER_CLASS=gevent
WORKER_CONNECTIONS=1000
```

3. **å†…å­˜ç®¡ç†**
```bash
# æ¨¡å‹ç¼“å­˜
ENABLE_CACHING=true
CACHE_SIZE=1000
MAX_CONCURRENT_MODELS=1
```

### æ¨èç¡¬ä»¶é…ç½®

| é…ç½® | æœ€ä½è¦æ±‚ | æ¨èé…ç½® | é«˜æ€§èƒ½é…ç½® |
|------|----------|----------|------------|
| èŠ¯ç‰‡ | M1 8æ ¸ | M2 Pro | M3 Ultra |
| å†…å­˜ | 16GB | 32GB | 64GB+ |
| å­˜å‚¨ | 256GB SSD | 512GB SSD | 1TB+ SSD |
| å¹¶å‘æ•° | 5 | 10 | 20+ |

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—ç³»ç»Ÿ

```bash
# æ—¥å¿—ç›®å½•ç»“æ„
logs/
â”œâ”€â”€ app.log              # åº”ç”¨æ—¥å¿—
â”œâ”€â”€ api_requests.log     # APIè¯·æ±‚æ—¥å¿—  
â”œâ”€â”€ model_operations.log # æ¨¡å‹æ“ä½œæ—¥å¿—
â”œâ”€â”€ system_metrics.log   # ç³»ç»ŸæŒ‡æ ‡æ—¥å¿—
â””â”€â”€ errors.log          # é”™è¯¯æ—¥å¿—
```

### å…³é”®ç›‘æ§æŒ‡æ ‡

1. **æ€§èƒ½æŒ‡æ ‡**
   - å“åº”æ—¶é—´ < 5ç§’ (P95)
   - ååé‡ > 2.0 è¯·æ±‚/ç§’
   - æˆåŠŸç‡ > 99%
   - Tokenç”Ÿæˆé€Ÿåº¦ > 100 tokens/ç§’

2. **ç³»ç»ŸæŒ‡æ ‡**
   - CPUä½¿ç”¨ç‡ < 60%
   - å†…å­˜ä½¿ç”¨ç‡ < 70%
   - ç£ç›˜I/Oå»¶è¿Ÿ < 100ms
   - ç½‘ç»œå»¶è¿Ÿ < 50ms

3. **ä¸šåŠ¡æŒ‡æ ‡**
   - æ¨¡å‹åŠ è½½æ—¶é—´ < 10ç§’
   - æ¨ç†é”™è¯¯ç‡ < 1%
   - å¹¶å‘å¤„ç†æ•° > 10

### Prometheus ç›‘æ§ (å¯é€‰)

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'vllm-api'
    static_configs:
      - targets: ['127.0.0.1:8001']
    scrape_interval: 5s
    metrics_path: '/metrics'
```

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

1. **Metalåº“æ„å»ºé”™è¯¯**
```
[metal::Device] Unable to build metal library from source
```
**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨å•workeræ¨¡å¼ (`--workers 1`)

2. **å†…å­˜ä¸è¶³**
```
MLX Error: Unable to allocate memory
```
**è§£å†³æ–¹æ¡ˆ**: å‡å°‘`max_tokens`æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
```
Model not found: models/qwen-0.5b
```
**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼Œé‡æ–°ä¸‹è½½æ¨¡å‹

4. **ç«¯å£å ç”¨**
```
Address already in use: 127.0.0.1:8001
```
**è§£å†³æ–¹æ¡ˆ**: 
```bash
lsof -ti:8001 | xargs kill -9
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export LOG_LEVEL=DEBUG
python run.py --mode dev

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f logs/app.log
```

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### å®šæœŸç»´æŠ¤

1. **æ—¥å¿—è½®è½¬**
```bash
# æ¸…ç†30å¤©å‰çš„æ—¥å¿—
find logs/ -name "*.log" -mtime +30 -delete
```

2. **æ¨¡å‹æ›´æ–°**
```bash
# æ›´æ–°æ¨¡å‹
python scripts/update_models.py
```

3. **ä¾èµ–æ›´æ–°**
```bash
pip list --outdated
pip install --upgrade package_name
```

### å¤‡ä»½ç­–ç•¥

```bash
# é…ç½®æ–‡ä»¶å¤‡ä»½
cp .env .env.backup.$(date +%Y%m%d)

# æ—¥å¿—å¤‡ä»½
tar -czf logs_backup_$(date +%Y%m%d).tar.gz logs/

# æ¨¡å‹å¤‡ä»½
rsync -av models/ backup/models/
```

## ğŸš¨ å®‰å…¨å»ºè®®

1. **ç½‘ç»œå®‰å…¨**
   - ä»…ç»‘å®šå†…ç½‘IP (127.0.0.1)
   - ä½¿ç”¨é˜²ç«å¢™é™åˆ¶è®¿é—®
   - å¯ç”¨HTTPS (ç”Ÿäº§ç¯å¢ƒ)

2. **è®¤è¯æˆæƒ**
   - å®ç°APIå¯†é’¥éªŒè¯
   - é™åˆ¶è¯·æ±‚é¢‘ç‡
   - è®°å½•è®¿é—®æ—¥å¿—

3. **æ•°æ®å®‰å…¨**
   - ä¸è®°å½•æ•æ„Ÿè¾“å…¥å†…å®¹
   - å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶
   - åŠ å¯†å­˜å‚¨é…ç½®ä¿¡æ¯

## ğŸ“‹ æœ€ä½³å®è·µ

### å¼€å‘è§„èŒƒ

1. **ä»£ç è´¨é‡**
   - éµå¾ªPEP8è§„èŒƒ
   - ç¼–å†™å•å…ƒæµ‹è¯•
   - å®šæœŸä»£ç å®¡æŸ¥

2. **é…ç½®ç®¡ç†**
   - ä½¿ç”¨ç¯å¢ƒå˜é‡
   - åˆ†ç¦»å¼€å‘/ç”Ÿäº§é…ç½®
   - ç‰ˆæœ¬åŒ–é…ç½®æ–‡ä»¶

3. **é”™è¯¯å¤„ç†**
   - ä¼˜é›…å¤„ç†å¼‚å¸¸
   - è¯¦ç»†é”™è¯¯æ—¥å¿—
   - ç”¨æˆ·å‹å¥½é”™è¯¯ä¿¡æ¯

### ç”Ÿäº§éƒ¨ç½²

1. **æœåŠ¡ç®¡ç†**
```bash
# systemd æœåŠ¡é…ç½®
[Unit]
Description=VLLM Inference Service
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/path/to/vllm-obtain
ExecStart=/path/to/vllm-obtain/venv/bin/python run.py --mode prod
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

2. **è´Ÿè½½å‡è¡¡** (å¤šå®ä¾‹)
```nginx
upstream vllm_backend {
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
    server 127.0.0.1:8003;
}

server {
    listen 80;
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [MLX å®˜æ–¹æ–‡æ¡£](https://ml-explore.github.io/mlx/build/html/index.html)
- [Gunicorn é…ç½®æŒ‡å—](https://docs.gunicorn.org/en/stable/configure.html)
- [OpenAI API å‚è€ƒ](https://platform.openai.com/docs/api-reference)
- [Flask ç”Ÿäº§éƒ¨ç½²](https://flask.palletsprojects.com/en/2.3.x/deploying/)

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»º Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

---

**ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-08-21  
**æµ‹è¯•ç¯å¢ƒ**: Mac Studio M3 Ultra, macOS 15.6, Python 3.13