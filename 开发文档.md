# VLLM 跨平台推理服务开发文档

## 🎯 项目概述

本项目是一个跨平台的大语言模型推理服务，支持 OpenAI API 兼容接口，具备动态模型切换和负载均衡功能。

### 核心特性
- 🚀 **跨平台支持**: macOS(MLX) / Linux+Windows(VLLM) / 通用(llama.cpp)
- 🔄 **动态模型切换**: 支持运行时加载/卸载模型
- ⚖️ **负载均衡**: 多实例部署和请求分发
- 🔌 **OpenAI 兼容**: 完整的 OpenAI API 兼容性
- 📊 **性能监控**: 详细的推理性能和资源使用监控

## 📁 项目结构

```
vllm推理框架/
├── src/
│   ├── core/                   # 核心模块
│   │   ├── __init__.py
│   │   ├── model_manager.py    # 模型管理器
│   │   ├── inference_engine.py # 推理引擎抽象
│   │   └── load_balancer.py    # 负载均衡器
│   ├── engines/                # 推理引擎实现
│   │   ├── __init__.py
│   │   ├── vllm_engine.py      # VLLM 引擎
│   │   ├── mlx_engine.py       # MLX 引擎
│   │   └── llamacpp_engine.py  # llama.cpp 引擎
│   ├── api/                    # API 层
│   │   ├── __init__.py
│   │   ├── app.py              # Flask 应用 (单实例)
│   │   ├── load_balanced_app.py # 负载均衡应用
│   │   ├── routes/             # 路由模块
│   │   │   ├── __init__.py
│   │   │   ├── openai_compat.py # OpenAI 兼容接口
│   │   │   ├── management.py   # 管理接口
│   │   │   └── load_balanced.py # 负载均衡路由
│   │   └── middleware/         # 中间件
│   │       ├── __init__.py
│   │       ├── auth.py         # 认证中间件
│   │       └── rate_limit.py   # 限流中间件
│   └── utils/                  # 工具模块
│       ├── __init__.py
│       ├── platform_detector.py # 平台检测
│       ├── config.py           # 配置管理
│       ├── logger.py           # 日志系统
│       └── cluster_manager.py  # 集群管理器
├── logs/                       # 日志文件
├── models/                     # 模型存储
├── cache/                      # 缓存目录
├── tests/                      # 测试代码
├── requirements.txt            # Python 依赖
├── .env.example               # 环境变量模板
├── README.md                  # 项目说明
├── CLAUDE.md                  # Claude 开发助手配置
└── 开发文档.md                # 本文件
```

## 🛠️ 开发环境配置

### 1. Python 环境
```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows
venv\\Scripts\\activate
# macOS/Linux
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

### 2. 环境变量配置
```bash
# 复制环境变量模板
cp .env.example .env

# 编辑配置文件
# 根据部署平台调整 INFERENCE_ENGINE 和其他参数
```

### 3. 平台特定配置

#### macOS (M3 Ultra)
```bash
# 安装 MLX
pip install mlx-lm

# 环境变量建议
INFERENCE_ENGINE=auto  # 将自动选择 MLX
DEVICE_TYPE=mps
MAX_GPU_MEMORY=0.8
```

#### Linux/Windows (CUDA)
```bash
# 安装 VLLM
pip install vllm

# 环境变量建议
INFERENCE_ENGINE=auto  # 将自动选择 VLLM
DEVICE_TYPE=cuda
MAX_GPU_MEMORY=0.8
```

#### 通用 CPU
```bash
# 安装 llama.cpp
pip install llama-cpp-python

# 环境变量建议
INFERENCE_ENGINE=llama_cpp
DEVICE_TYPE=cpu
MAX_CPU_THREADS=8
```

## 🏗️ 架构设计

### 核心组件

#### 1. 平台检测器 (`platform_detector.py`)
- **职责**: 自动检测运行平台并选择最佳推理引擎
- **特性**: 
  - 智能检测 macOS(MLX) / Linux+Windows(VLLM)
  - 支持通过环境变量强制指定引擎
  - 硬件兼容性验证

#### 2. 模型管理器 (`model_manager.py`)
- **职责**: 模型生命周期管理
- **功能**:
  - 动态加载/卸载模型
  - 内存和显存监控
  - 模型状态管理
  - 并发模型数量控制

#### 3. 推理引擎抽象 (`inference_engine.py`)
- **职责**: 统一不同推理引擎的接口
- **支持引擎**:
  - `VLLMEngine`: CUDA 优化，高性能
  - `MLXEngine`: Apple Silicon 优化
  - `LlamaCppEngine`: 跨平台 CPU/GPU

#### 4. 负载均衡器 (`load_balancer.py`)
- **职责**: 多实例请求分发
- **策略**:
  - 轮询 (Round Robin)
  - 最少连接 (Least Connections)
  - 加权分配 (Weighted)

### API 接口设计

#### OpenAI 兼容接口
```python
# 对话补全
POST /v1/chat/completions
{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [...],
    "temperature": 0.7,
    "max_tokens": 2048
}

# 文本补全
POST /v1/completions
{
    "model": "Qwen2.5-7B-Instruct", 
    "prompt": "Hello, world!",
    "max_tokens": 100
}

# 模型列表
GET /v1/models
```

#### 管理接口
```python
# 加载模型
POST /v1/models/load
{
    "model_name": "Qwen2.5-14B-Instruct",
    "model_path": "/models/qwen2.5-14b"
}

# 卸载模型
DELETE /v1/models/{model_name}

# 系统状态
GET /v1/system/health
GET /v1/system/metrics
```

## 🔧 配置管理

### 环境变量配置

| 变量名 | 默认值 | 描述 |
|--------|--------|------|
| `INFERENCE_ENGINE` | `auto` | 推理引擎选择 |
| `INFERENCE_MODE` | `single` | 运行模式 |
| `MAX_CONCURRENT_MODELS` | `1` | 最大并发模型数 |
| `DEFAULT_MODEL` | `Qwen2.5-7B-Instruct` | 默认模型 |
| `HOST` | `0.0.0.0` | 服务地址 |
| `PORT` | `8000` | 服务端口 |
| `LOG_LEVEL` | `INFO` | 日志级别 |

### 运行模式

#### 单实例模式 (`single`)
```bash
INFERENCE_MODE=single
MAX_CONCURRENT_MODELS=1
```
- 适用: 开发测试、资源受限
- 特点: 节省资源，模型切换有延迟

#### 多实例模式 (`multi_instance`)
```bash
INFERENCE_MODE=multi_instance
MAX_CONCURRENT_MODELS=3
```
- 适用: 高配置设备 (如 M3 Ultra)
- 特点: 快速响应，资源消耗大

#### 负载均衡模式 (`load_balance`)
```bash
INFERENCE_MODE=load_balance
CLUSTER_INSTANCES=3                    # 本地集群实例数
LOAD_BALANCE_STRATEGY=round_robin      # 负载均衡策略
AUTO_SCALING=true                      # 启用自动扩缩容
MIN_INSTANCES=1                        # 最小实例数
MAX_INSTANCES=10                       # 最大实例数
HEALTH_CHECK_INTERVAL=30               # 健康检查间隔(秒)
```
- 适用: 生产环境、高并发、高可用性要求
- 特点: 自动负载分发、故障转移、横向扩展
- 支持策略: 轮询、最少连接、加权、随机、响应时间优先

## ⚖️ 负载均衡系统

### 系统架构

```
                   ┌─────────────────┐
                   │   负载均衡器     │
                   │  (主应用入口)    │
                   └─────────────────┘
                           │
           ┌───────────────┼───────────────┐
           │               │               │
    ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
    │  实例 1     │ │  实例 2     │ │  实例 3     │
    │ Port: 8001  │ │ Port: 8002  │ │ Port: 8003  │
    └─────────────┘ └─────────────┘ └─────────────┘
```

### 负载均衡策略

| 策略 | 描述 | 适用场景 |
|------|------|----------|
| `round_robin` | 轮询分发请求 | 实例性能相近 |
| `least_connections` | 选择连接数最少的实例 | 请求处理时间差异大 |
| `weighted` | 基于权重分发 | 实例性能差异较大 |
| `random` | 随机选择实例 | 简单场景，避免热点 |
| `response_time` | 选择响应时间最短的实例 | 优化用户体验 |

### 集群管理接口

#### 获取集群状态
```bash
curl http://localhost:8000/v1/cluster/status
```

响应示例:
```json
{
  "is_running": true,
  "total_instances": 3,
  "running_instances": 3,
  "load_balancer_strategy": "round_robin",
  "instances": {
    "instance_0": {
      "port": 8001,
      "weight": 1,
      "is_running": true,
      "cpu_percent": 45.2,
      "memory_mb": 2048.5
    }
  }
}
```

#### 动态扩缩容
```bash
# 扩容: 增加 2 个实例
curl -X POST http://localhost:8000/v1/cluster/scale \
  -H "Content-Type: application/json" \
  -d '{"action": "scale_up", "count": 2}'

# 缩容: 减少 1 个实例
curl -X POST http://localhost:8000/v1/cluster/scale \
  -H "Content-Type: application/json" \
  -d '{"action": "scale_down", "count": 1}'
```

#### 切换负载均衡策略
```bash
curl -X POST http://localhost:8000/v1/cluster/strategy \
  -H "Content-Type: application/json" \
  -d '{"strategy": "least_connections"}'
```

#### 重启实例
```bash
curl -X POST http://localhost:8000/v1/cluster/restart \
  -H "Content-Type: application/json" \
  -d '{"instance_id": "instance_1"}'
```

### 健康检查与故障转移

- **健康检查**: 每30秒检查一次实例状态
- **故障检测**: 连续3次检查失败标记为不健康
- **自动恢复**: 不健康实例会被自动重启
- **故障转移**: 请求自动路由到健康实例
- **优雅关闭**: 实例停止前完成当前请求

### 监控指标

```bash
# 获取负载均衡器指标
curl http://localhost:8000/v1/system/metrics
```

返回数据包括:
- 各实例请求数、成功率、平均响应时间
- 活跃连接数、健康状态
- CPU、内存使用情况
- 负载分发统计

## 📊 性能监控

### 关键指标
- **Token 生成速度**: tokens/second
- **请求延迟**: 平均响应时间
- **资源使用率**: CPU/GPU/内存占用
- **并发请求数**: 实时并发量
- **模型切换时间**: 加载/卸载耗时

### 日志格式
```json
{
    "timestamp": "2024-01-20 10:30:45",
    "level": "INFO",
    "module": "inference_engine",
    "event": "model_inference",
    "data": {
        "model_name": "Qwen2.5-7B-Instruct",
        "prompt_tokens": 128,
        "completion_tokens": 256,
        "total_tokens": 384,
        "inference_time": 2.45,
        "tokens_per_second": 104.5,
        "gpu_memory_used": "12.3GB",
        "request_id": "req_123456"
    }
}
```

## 🚀 部署指南

### 开发环境启动

#### 单实例模式
```bash
# 激活虚拟环境
source venv/bin/activate  # macOS/Linux
# 或
venv\\Scripts\\activate    # Windows

# 启动开发服务器 (默认单实例)
python run.py --mode dev
```

#### 负载均衡模式
```bash
# 设置负载均衡配置
export INFERENCE_MODE=load_balance
export CLUSTER_INSTANCES=3
export LOAD_BALANCE_STRATEGY=round_robin

# 启动负载均衡集群
python run.py --mode dev
```

### 生产环境部署

#### 单实例生产部署
```bash
# 使用 Gunicorn (推荐)
python run.py --mode prod

# 或手动启动
gunicorn --bind 0.0.0.0:8000 --workers 4 --worker-class gevent src.api.app:get_app
```

#### 负载均衡生产部署
```bash
# 配置负载均衡环境变量
export INFERENCE_MODE=load_balance
export CLUSTER_INSTANCES=5
export LOAD_BALANCE_STRATEGY=least_connections

# 启动负载均衡集群
python run.py --mode prod

# 或手动启动
gunicorn --bind 0.0.0.0:8000 --workers 4 --worker-class gevent src.api.load_balanced_app:get_load_balanced_app
```

### Docker 部署
```dockerfile
# Dockerfile 示例
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY .env ./

EXPOSE 8000
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "src.api.app:app"]
```

## 🧪 测试

### 单元测试
```bash
# 运行所有测试
python -m pytest tests/

# 运行特定测试
python -m pytest tests/test_model_manager.py -v

# 生成覆盖率报告
python -m pytest --cov=src tests/
```

### API 测试
```bash
# 测试 OpenAI 兼容接口
curl -X POST http://localhost:8000/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'
```

## 🐛 故障排除

### 常见问题

#### 1. 模型加载失败
```bash
# 检查模型路径
ls -la $MODEL_BASE_PATH

# 检查显存使用
nvidia-smi  # CUDA
# 或
ps aux | grep python  # 进程内存使用
```

#### 2. 推理引擎不兼容
```bash
# 强制指定引擎
export INFERENCE_ENGINE=llama_cpp

# 检查平台信息
python src/utils/platform_detector.py
```

#### 3. 性能问题
```bash
# 调整并发参数
export MAX_CONCURRENT_REQUESTS=50
export WORKERS=8

# 启用缓存
export ENABLE_CACHING=True
```

## 📝 开发规范

### 代码风格
- 使用 Python 3.11+
- 遵循 PEP 8 编码规范
- 使用类型提示 (Type Hints)
- 添加适当的文档字符串

### Git 提交规范
```
feat: 添加新功能
fix: 修复 bug  
docs: 更新文档
style: 代码格式化
refactor: 重构代码
test: 添加测试
chore: 构建过程或工具变更
```

### 分支管理
- `main`: 主分支，稳定版本
- `develop`: 开发分支
- `feature/*`: 功能分支
- `hotfix/*`: 热修复分支

## 🔮 未来规划

### 短期目标 (1-2 个月)
- [ ] 完善 MLX 引擎集成
- [ ] 添加模型量化支持
- [ ] 实现请求队列管理
- [ ] 前端管理界面开发

### 中期目标 (3-6 个月)
- [ ] 支持更多推理引擎 (TensorRT-LLM, OpenVINO)
- [ ] 实现模型并行推理
- [ ] 添加分布式部署支持
- [ ] 性能优化和内存管理

### 长期目标 (6+ 个月)
- [ ] 支持多模态模型 (文本+图像)
- [ ] 实现模型微调接口
- [ ] 云原生部署 (Kubernetes)
- [ ] 商业化功能 (计费、监控等)

---

## 📞 联系方式

如有问题或建议，请提交 Issue 或 Pull Request。

**开发者**: [Your Name]  
**项目地址**: [GitHub Repository]  
**文档更新**: 2024-01-20