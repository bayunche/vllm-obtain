# ğŸš€ éƒ¨ç½²æŒ‡å—

## ğŸ¯ éƒ¨ç½²æ–¹å¼é€‰æ‹©

| éƒ¨ç½²æ–¹å¼ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ | æ€§èƒ½ |
|---------|----------|------|------|
| **æœ¬åœ°å¼€å‘** | å¼€å‘æµ‹è¯• | â­ | ä¸­ç­‰ |
| **ç”Ÿäº§å•æœº** | å°è§„æ¨¡ç”Ÿäº§ | â­â­ | é«˜ |
| **Dockerå®¹å™¨** | æ ‡å‡†åŒ–éƒ¨ç½² | â­â­â­ | é«˜ |
| **è´Ÿè½½å‡è¡¡é›†ç¾¤** | å¤§è§„æ¨¡ç”Ÿäº§ | â­â­â­â­ | æé«˜ |

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

#### macOS (æ¨è)
```bash
# æœ€ä½é…ç½®
- macOS 12.0+
- Apple Silicon (M1/M2/M3/M4)
- 16GB RAM
- 50GB å¯ç”¨å­˜å‚¨

# æ¨èé…ç½®  
- macOS 14.0+
- M3 Ultra / M4 Pro
- 64GB RAM
- 500GB å¯ç”¨å­˜å‚¨
```

#### Linux
```bash
# æœ€ä½é…ç½®
- Ubuntu 20.04+ / CentOS 8+
- NVIDIA GTX 1080+ (8GB VRAM)
- 16GB RAM
- 50GB å¯ç”¨å­˜å‚¨

# æ¨èé…ç½®
- Ubuntu 22.04+
- NVIDIA RTX 4090 (24GB VRAM)  
- 32GB RAM
- 1TB NVMe SSD
```

#### Windows  
```bash
# æœ€ä½é…ç½®
- Windows 10/11
- NVIDIA GTX 1070+ (8GB VRAM)
- 16GB RAM
- 50GB å¯ç”¨å­˜å‚¨

# æ¨èé…ç½®
- Windows 11
- NVIDIA RTX 4080+ (16GB VRAM)
- 32GB RAM
- 1TB NVMe SSD
```

---

## ğŸ  æœ¬åœ°å¼€å‘éƒ¨ç½²

### å¿«é€Ÿå¯åŠ¨
```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd vllm-obtain

# 2. è‡ªåŠ¨å®‰è£…ä¾èµ–
python run.py

# 3. æ³¨å†Œæ¨¡å‹
python register_models.py

# 4. å¯åŠ¨æœåŠ¡
python run.py
```

### é…ç½®è°ƒä¼˜
```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim .env.mac     # macOS
vim .env.linux   # Linux
vim .env.windows # Windows

# å…³é”®å‚æ•°
MAX_CONCURRENT_MODELS=2        # å¹¶å‘æ¨¡å‹æ•°
MAX_CONCURRENT_REQUESTS=10     # å¹¶å‘è¯·æ±‚æ•°  
MEMORY_LIMIT=80               # å†…å­˜ä½¿ç”¨é™åˆ¶(%)
REQUEST_TIMEOUT=300           # è¯·æ±‚è¶…æ—¶(ç§’)
```

---

## ğŸ­ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ä½¿ç”¨ Gunicorn (æ¨è)
```bash
# å®‰è£… Gunicorn
pip install gunicorn gevent

# å¯åŠ¨æœåŠ¡
gunicorn -c gunicorn.conf.py src.api.app:application

# åå°è¿è¡Œ
nohup gunicorn -c gunicorn.conf.py src.api.app:application > gunicorn.log 2>&1 &
```

### Gunicorn é…ç½®æ–‡ä»¶
```python
# gunicorn.conf.py
import multiprocessing
import os

# åŸºç¡€é…ç½®
bind = "0.0.0.0:8001"
workers = 1  # MLXåªæ”¯æŒå•è¿›ç¨‹ï¼ŒVLLMå¯å¢åŠ 
worker_class = "gevent"
worker_connections = 500

# æ€§èƒ½è°ƒä¼˜
max_requests = 1000
max_requests_jitter = 100
preload_app = True
timeout = 300
keepalive = 2

# æ—¥å¿—é…ç½®
accesslog = "./logs/gunicorn_access.log"
errorlog = "./logs/gunicorn_error.log" 
loglevel = "info"
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# è¿›ç¨‹ç®¡ç†
pid = "./gunicorn.pid"
daemon = False  # è®¾ä¸ºTrueåå°è¿è¡Œ

# èµ„æºé™åˆ¶
worker_tmp_dir = "/dev/shm"  # ä½¿ç”¨å†…å­˜ä¸´æ—¶ç›®å½•
tmp_upload_dir = None
```

### ä½¿ç”¨ Waitress (Windowsæ¨è)
```bash
# å®‰è£… Waitress
pip install waitress

# å¯åŠ¨æœåŠ¡
waitress-serve --host=0.0.0.0 --port=8001 --threads=10 src.api.app:application
```

---

## ğŸ³ Docker éƒ¨ç½²

### Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \\
    build-essential \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æºç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p models logs cache

# æš´éœ²ç«¯å£
EXPOSE 8001

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\
    CMD curl -f http://localhost:8001/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["gunicorn", "-c", "gunicorn.conf.py", "src.api.app:application"]
```

### Docker Compose
```yaml
version: '3.8'

services:
  vllm-inference:
    build: .
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./cache:/app/cache
    environment:
      - INFERENCE_ENGINE=auto
      - MAX_CONCURRENT_MODELS=3
      - DEBUG=false
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # å¯é€‰ï¼šNginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - vllm-inference
    restart: unless-stopped
```

### æ„å»ºå’Œè¿è¡Œ
```bash
# æ„å»ºé•œåƒ
docker build -t vllm-inference:latest .

# è¿è¡Œå®¹å™¨
docker run -d \\
  --name vllm-service \\
  -p 8001:8001 \\
  -v $(pwd)/models:/app/models \\
  -v $(pwd)/logs:/app/logs \\
  --restart unless-stopped \\
  vllm-inference:latest

# ä½¿ç”¨ Docker Compose
docker-compose up -d
```

---

## âš–ï¸ è´Ÿè½½å‡è¡¡éƒ¨ç½²

### å¤šå®ä¾‹è´Ÿè½½å‡è¡¡
```bash
# å¯åŠ¨è´Ÿè½½å‡è¡¡å™¨
python -m src.api.load_balanced_app --workers 3 --port 8001

# æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶
python -m src.api.load_balanced_app --config load_balance.yaml
```

### è´Ÿè½½å‡è¡¡é…ç½®
```yaml
# load_balance.yaml
load_balancer:
  port: 8001
  workers: 3
  strategy: "round_robin"  # round_robin, least_loaded, hash
  
instances:
  - host: "127.0.0.1"
    port: 8002
    weight: 1
    max_connections: 100
  - host: "127.0.0.1"  
    port: 8003
    weight: 1
    max_connections: 100
  - host: "127.0.0.1"
    port: 8004
    weight: 2
    max_connections: 150

health_check:
  interval: 30
  timeout: 5
  path: "/health"
```

### Nginx è´Ÿè½½å‡è¡¡
```nginx
upstream vllm_backend {
    least_conn;
    server 127.0.0.1:8002 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8003 weight=1 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8004 weight=2 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # æµå¼å“åº”æ”¯æŒ
        proxy_buffering off;
        proxy_cache off;
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
    
    # å¥åº·æ£€æŸ¥
    location /health {
        access_log off;
        proxy_pass http://vllm_backend/health;
    }
}
```

---

## ğŸ”’ å®‰å…¨é…ç½®

### SSL/TLS é…ç½®
```nginx
server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    ssl_certificate /path/to/certificate.crt;
    ssl_certificate_key /path/to/private.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    
    # å…¶ä»–é…ç½®...
}
```

### é˜²ç«å¢™é…ç½®
```bash
# Ubuntu/Debian
ufw allow 8001/tcp
ufw allow 22/tcp
ufw enable

# CentOS/RHEL
firewall-cmd --permanent --add-port=8001/tcp
firewall-cmd --permanent --add-port=22/tcp
firewall-cmd --reload
```

### API è®¿é—®æ§åˆ¶
```python
# åœ¨ .env æ–‡ä»¶ä¸­é…ç½®
API_KEY_REQUIRED=true
ALLOWED_HOSTS=127.0.0.1,your-domain.com
CORS_ORIGINS=https://your-frontend.com,https://n8n.your-domain.com
RATE_LIMIT_PER_MINUTE=60
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—é…ç½®
```python
# .env é…ç½®
LOG_LEVEL=INFO
LOG_DIR=./logs
ENABLE_LOG_FILE=true
LOG_RETENTION_DAYS=30
LOG_ROTATE_SIZE=100MB
ENABLE_METRICS=true
METRICS_INTERVAL=60
```

### ç›‘æ§æŒ‡æ ‡
```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
curl http://localhost:8001/v1/system/status

# æŸ¥çœ‹å¥åº·çŠ¶æ€  
curl http://localhost:8001/health

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/inference.log
tail -f logs/metrics.log
```

### Prometheus é›†æˆ (å¯é€‰)
```python
# å®‰è£…ä¾èµ–
pip install prometheus-client

# åœ¨ .env ä¸­å¯ç”¨
ENABLE_PROMETHEUS=true
PROMETHEUS_PORT=9090
```

---

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la models/

# æ£€æŸ¥é…ç½®
cat .env.mac

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
tail -f logs/inference.log
```

#### å†…å­˜ä¸è¶³
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h

# è°ƒæ•´é…ç½®
vim .env.mac
# å‡å°‘ MAX_CONCURRENT_MODELS
# é™ä½ MEMORY_LIMIT
```

#### ç«¯å£å ç”¨
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8001
netstat -tulpn | grep 8001

# æ€æ­»å ç”¨è¿›ç¨‹
kill -9 <PID>
```

#### æƒé™é—®é¢˜
```bash
# ä¿®å¤æƒé™
chmod +x run.py
chmod -R 755 models/
chown -R $USER:$USER logs/ cache/
```

### æ€§èƒ½è°ƒä¼˜

#### å†…å­˜ä¼˜åŒ–
```bash
# .env é…ç½®è°ƒä¼˜
MEMORY_LIMIT=70                # é™ä½å†…å­˜é™åˆ¶
MAX_CONCURRENT_MODELS=2        # å‡å°‘å¹¶å‘æ¨¡å‹
MLX_MEMORY_FRACTION=0.6        # é™ä½MLXå†…å­˜ä½¿ç”¨
```

#### å¹¶å‘ä¼˜åŒ–
```bash
# æ ¹æ®ç¡¬ä»¶è°ƒæ•´
MAX_CONCURRENT_REQUESTS=20     # Mac Studio M3 Ultra
MAX_CONCURRENT_REQUESTS=30     # Linux RTX 4090  
MAX_CONCURRENT_REQUESTS=15     # Windows GTX 1080
```

#### ç£ç›˜ä¼˜åŒ–
```bash
# ä½¿ç”¨ SSD å­˜å‚¨æ¨¡å‹
MODEL_BASE_PATH=/fast/ssd/models

# å¯ç”¨ç£ç›˜ç¼“å­˜
ENABLE_CACHING=true
CACHE_SIZE=2000
```

---

## ğŸ“ˆ æ‰©å®¹æŒ‡å—

### å‚ç›´æ‰©å®¹ (å‡çº§ç¡¬ä»¶)
- **CPU**: å¢åŠ æ ¸å¿ƒæ•°æå‡å¹¶å‘å¤„ç†
- **å†…å­˜**: å¢åŠ RAMæ”¯æŒæ›´å¤šæ¨¡å‹å¹¶å‘
- **å­˜å‚¨**: ä½¿ç”¨NVMe SSDæå‡æ¨¡å‹åŠ è½½é€Ÿåº¦
- **GPU**: å‡çº§æ˜¾å¡æå‡æ¨ç†æ€§èƒ½

### æ°´å¹³æ‰©å®¹ (å¢åŠ å®ä¾‹)
1. **å¤šå®ä¾‹éƒ¨ç½²**: å¯åŠ¨å¤šä¸ªæœåŠ¡å®ä¾‹
2. **è´Ÿè½½å‡è¡¡**: ä½¿ç”¨Nginxæˆ–ä¸“ä¸šè´Ÿè½½å‡è¡¡å™¨
3. **å¥åº·æ£€æŸ¥**: è‡ªåŠ¨æ£€æµ‹å’Œå‰”é™¤æ•…éšœå®ä¾‹
4. **ä¼šè¯ç²˜æ€§**: æ ¹æ®éœ€è¦é…ç½®ä¼šè¯ä¿æŒ

### è‡ªåŠ¨æ‰©å®¹
```bash
# ä½¿ç”¨ systemd è‡ªåŠ¨é‡å¯
sudo systemctl enable vllm-inference
sudo systemctl start vllm-inference

# ä½¿ç”¨ Docker Swarm æˆ– Kubernetes
# å®ç°å®¹å™¨ç¼–æ’å’Œè‡ªåŠ¨æ‰©å®¹
```