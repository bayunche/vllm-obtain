# ğŸš€ VLLM è·¨å¹³å°æ¨ç†æœåŠ¡

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Windows%20|%20macOS%20|%20Linux-lightgrey.svg)](README.md)
[![OpenAI](https://img.shields.io/badge/OpenAI-Compatible-green.svg)](README.md)

> ğŸ¯ **ä¸€é”®éƒ¨ç½²ï¼Œæ™ºèƒ½é€‰æ‹©ï¼Œå®Œå…¨å…¼å®¹ OpenAI API çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡**

ä¸“ä¸ºä¼ä¸šå’Œå¼€å‘è€…è®¾è®¡çš„é«˜æ€§èƒ½è·¨å¹³å°æ¨ç†æœåŠ¡ï¼Œæ”¯æŒåœ¨ **n8n**ã€**Dify**ã€**LangChain** ç­‰å·¥å…·ä¸­æ— ç¼æ›¿æ¢ OpenAI APIï¼Œå®ç°å®Œå…¨æœ¬åœ°åŒ–çš„ AI èƒ½åŠ›ã€‚

## âœ¨ ä¸ºä»€ä¹ˆé€‰æ‹©æˆ‘ä»¬

### ğŸ¯ **æ™ºèƒ½å¹³å°é€‚é…**
- **macOS Apple Silicon** â†’ è‡ªåŠ¨å¯ç”¨ MLX å¼•æ“ (Metal ä¼˜åŒ–)
- **Linux/Windows CUDA** â†’ è‡ªåŠ¨å¯ç”¨ VLLM å¼•æ“ (é«˜æ€§èƒ½æ¨ç†)
- **é€šç”¨å¹³å°/CPU** â†’ è‡ªåŠ¨å›é€€ llama.cpp å¼•æ“ (æœ€å¹¿å…¼å®¹)
- **æ— éœ€æ‰‹åŠ¨é…ç½®** â†’ ä¸€é”®å¯åŠ¨ï¼Œæ™ºèƒ½è¯†åˆ«æœ€ä½³æ–¹æ¡ˆ

### ğŸ”Œ **å®Œç¾ OpenAI å…¼å®¹**
- âœ… **100% å…¼å®¹** OpenAI API æ ¼å¼å’Œå“åº”
- âœ… **ç›´æ¥åœ¨ n8n ä¸­ä½¿ç”¨** - åªéœ€ä¿®æ”¹ Base URL
- âœ… **æ”¯æŒæµå¼å“åº”** - å®æ—¶æ–‡æœ¬ç”Ÿæˆä½“éªŒ
- âœ… **é›¶ä»£ç è¿ç§»** - ç°æœ‰ OpenAI ä»£ç æ— éœ€ä¿®æ”¹

### âš¡ **ä¼ä¸šçº§æ€§èƒ½**
- ğŸš€ **åŠ¨æ€æ¨¡å‹ç®¡ç†** - è¿è¡Œæ—¶åŠ è½½/å¸è½½ï¼ŒèŠ‚çœèµ„æº
- ğŸ“Š **å®æ—¶æ€§èƒ½ç›‘æ§** - Tokené€Ÿåº¦ã€å»¶è¿Ÿã€èµ„æºä½¿ç”¨
- ğŸ”„ **å¤šæ¨¡å‹å¹¶è¡Œ** - é«˜é…ç½®è®¾å¤‡å¯åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹
- ğŸ›¡ï¸ **æ•…éšœè‡ªæ„ˆ** - è‡ªåŠ¨é‡è¯•ã€ä¼˜é›…é™çº§ã€å¥åº·æ£€æŸ¥

### ğŸ® **å¼€å‘è€…å‹å¥½**
- ğŸ¯ **ä¸€é”®å¯åŠ¨** - `python run.py --mode dev`
- ğŸ” **æ™ºèƒ½æ£€æµ‹** - è‡ªåŠ¨éªŒè¯ç¯å¢ƒå’Œä¾èµ–
- ğŸ“ **è¯¦ç»†æ—¥å¿—** - ç»“æ„åŒ–æ—¥å¿—ï¼Œä¾¿äºè°ƒè¯•
- ğŸ§ª **å®Œæ•´æµ‹è¯•** - OpenAI å…¼å®¹æ€§éªŒè¯

---

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd vllmæ¨ç†æ¡†æ¶

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# å®‰è£…ä¾èµ– (ä¼šè‡ªåŠ¨æ ¹æ®å¹³å°é€‰æ‹©åˆé€‚çš„æ¨ç†å¼•æ“)
pip install -r requirements.txt
```

### 2. ä¸€é”®å¯åŠ¨

```bash
# å¼€å‘æ¨¡å¼å¯åŠ¨ (è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå¹¶å¯åŠ¨æœ€ä½³é…ç½®)
python run.py --mode dev

# ğŸ‰ æœåŠ¡å¯åŠ¨æˆåŠŸï¼
# è®¿é—®åœ°å€: http://localhost:8000
# OpenAI API: http://localhost:8000/v1
```

### 3. éªŒè¯æœåŠ¡

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# è¿è¡Œå®Œæ•´çš„ OpenAI å…¼å®¹æ€§æµ‹è¯•
python test_openai_compatibility.py
```

**âœ… å®Œæˆï¼** ç°åœ¨ä½ æœ‰ä¸€ä¸ªå®Œå…¨å…¼å®¹ OpenAI API çš„æœ¬åœ°æ¨ç†æœåŠ¡ã€‚

---

## ğŸ® ä½¿ç”¨ç¤ºä¾‹

### åœ¨ n8n ä¸­ä½¿ç”¨ (æ¨è)

1. **æ·»åŠ  OpenAI èŠ‚ç‚¹**
2. **ä¿®æ”¹è¿æ¥é…ç½®**:
   ```
   Base URL: http://localhost:8000/v1
   API Key: å¯ä»¥ç•™ç©ºæˆ–éšæ„å¡«å†™
   ```
3. **é€‰æ‹©æ¨¡å‹**: ä½¿ç”¨ `Qwen2.5-7B-Instruct` æˆ–å…¶ä»–å·²åŠ è½½æ¨¡å‹
4. **å¼€å§‹ä½¿ç”¨**: å®Œå…¨å…¼å®¹æ‰€æœ‰ ChatGPT åŠŸèƒ½

### ä½¿ç”¨ OpenAI Python å®¢æˆ·ç«¯

```python
import openai

# æ›¿æ¢ API åŸºç¡€åœ°å€ï¼Œå…¶ä»–ä»£ç æ— éœ€ä¿®æ”¹
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "any-key"  # å¯é€‰ï¼Œå¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ä¸²

# å¯¹è¯è¡¥å…¨ - å®Œå…¨å…¼å®¹ OpenAI æ ¼å¼
response = openai.ChatCompletion.create(
    model="Qwen2.5-7B-Instruct",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"},
        {"role": "user", "content": "è¯·å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"}
    ],
    max_tokens=500,
    temperature=0.7,
    stream=False  # æ”¯æŒæµå¼: stream=True
)

print(response.choices[0].message.content)
```

### ä½¿ç”¨ cURL è°ƒç”¨

```bash
# èŠå¤©å¯¹è¯æ¥å£
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œç”¨ç®€å•çš„è¯"}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'

# æµå¼å¯¹è¯ (å®æ—¶ç”Ÿæˆ)
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct", 
    "messages": [{"role": "user", "content": "å†™ä¸€é¦–å…³äºAIçš„è¯—"}],
    "stream": true
  }'

# æ–‡æœ¬è¡¥å…¨æ¥å£
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "prompt": "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿åŒ…æ‹¬",
    "max_tokens": 150
  }'
```

---

## ğŸ“‹ API æ–‡æ¡£

### OpenAI å…¼å®¹æ¥å£

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° | å…¼å®¹æ€§ |
|------|------|------|--------|
| `/v1/models` | GET | è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ | âœ… 100% |
| `/v1/models/{id}` | GET | è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯ | âœ… 100% |
| `/v1/chat/completions` | POST | èŠå¤©å¯¹è¯è¡¥å…¨ | âœ… 100% |
| `/v1/completions` | POST | æ–‡æœ¬è¡¥å…¨ | âœ… 100% |
| `/v1/embeddings` | POST | æ–‡æœ¬åµŒå…¥ | ğŸš§ è®¡åˆ’ä¸­ |

### ç®¡ç†æ¥å£ (æ‰©å±•åŠŸèƒ½)

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/health` | GET | æœåŠ¡å¥åº·æ£€æŸ¥ |
| `/v1/system/status` | GET | ç³»ç»ŸçŠ¶æ€è¯¦æƒ… |
| `/v1/system/health` | GET | æ·±åº¦å¥åº·æ£€æŸ¥ |
| `/v1/system/metrics` | GET | æ€§èƒ½æŒ‡æ ‡ç›‘æ§ |
| `/v1/models/load` | POST | åŠ¨æ€åŠ è½½æ¨¡å‹ |
| `/v1/models/{name}/unload` | DELETE | å¸è½½æŒ‡å®šæ¨¡å‹ |
| `/v1/models/{name}/status` | GET | æ¨¡å‹çŠ¶æ€æŸ¥è¯¢ |

### è¯·æ±‚ç¤ºä¾‹

#### èŠå¤©è¡¥å…¨

```json
{
  "model": "Qwen2.5-7B-Instruct",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹"},
    {"role": "user", "content": "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº"}
  ],
  "max_tokens": 1000,
  "temperature": 0.7,
  "top_p": 0.9,
  "stream": false,
  "stop": ["```"]
}
```

#### å“åº”æ ¼å¼

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "Qwen2.5-7B-Instruct",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "è¿™é‡Œæ˜¯ç”Ÿæˆçš„å›å¤å†…å®¹..."
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 28,
    "completion_tokens": 120,
    "total_tokens": 148
  }
}
```

---

## âš™ï¸ é…ç½®å‚è€ƒ

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¤åˆ¶é…ç½®æ¨¡æ¿
cp .env.example .env
```

#### æ ¸å¿ƒé…ç½®

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `INFERENCE_ENGINE` | `auto` | æ¨ç†å¼•æ“é€‰æ‹© (auto/vllm/mlx/llama_cpp) |
| `DEVICE_TYPE` | `auto` | è®¾å¤‡ç±»å‹ (auto/cuda/mps/cpu) |
| `MAX_CONCURRENT_MODELS` | `1` | æœ€å¤§å¹¶å‘æ¨¡å‹æ•° |
| `DEFAULT_MODEL` | `Qwen2.5-7B-Instruct` | é»˜è®¤æ¨¡å‹åç§° |
| `MODEL_BASE_PATH` | `./models` | æ¨¡å‹å­˜å‚¨è·¯å¾„ |

#### æœåŠ¡é…ç½®

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `HOST` | `0.0.0.0` | æœåŠ¡ç›‘å¬åœ°å€ |
| `PORT` | `8000` | æœåŠ¡ç«¯å£ |
| `WORKERS` | `4` | Gunicorn Worker æ•°é‡ |
| `MAX_CONCURRENT_REQUESTS` | `100` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° |

### å¹³å°ä¼˜åŒ–é…ç½®

#### macOS (M3 Ultra) é«˜æ€§èƒ½é…ç½®
```bash
INFERENCE_ENGINE=auto          # è‡ªåŠ¨é€‰æ‹© MLX
DEVICE_TYPE=mps               # Metal Performance Shaders
MAX_GPU_MEMORY=0.8            # ä½¿ç”¨ 80% GPU å†…å­˜
MAX_CONCURRENT_MODELS=3       # åˆ©ç”¨å¤§å†…å­˜ä¼˜åŠ¿
MAX_CONCURRENT_REQUESTS=150   # é«˜å¹¶å‘æ”¯æŒ
```

#### Linux/Windows (CUDA) é«˜æ€§èƒ½é…ç½®
```bash
INFERENCE_ENGINE=auto          # è‡ªåŠ¨é€‰æ‹© VLLM
DEVICE_TYPE=cuda              # CUDA åŠ é€Ÿ
MAX_GPU_MEMORY=0.8            # GPU å†…å­˜é™åˆ¶
MAX_CONCURRENT_MODELS=2       # æ ¹æ®æ˜¾å­˜è°ƒæ•´
ENABLE_CACHING=True           # å¯ç”¨æ™ºèƒ½ç¼“å­˜
```

#### CPU é€šç”¨é…ç½®
```bash
INFERENCE_ENGINE=llama_cpp     # CPU ä¼˜åŒ–å¼•æ“
DEVICE_TYPE=cpu               # CPU æ¨¡å¼
MAX_CPU_THREADS=8             # CPU çº¿ç¨‹æ•°
MAX_CONCURRENT_MODELS=1       # å•æ¨¡å‹æ¨¡å¼
```

---

## ğŸ“¦ æ¨¡å‹ç®¡ç†

### æ”¯æŒçš„æ¨¡å‹æ ¼å¼

| æ¨ç†å¼•æ“ | æ”¯æŒæ ¼å¼ | æœ€ä½³ä½¿ç”¨åœºæ™¯ |
|----------|----------|--------------|
| **VLLM** | HuggingFace, Safetensors | Linux/Windows GPU é«˜æ€§èƒ½æ¨ç† |
| **MLX** | MLX æ ¼å¼, HuggingFace | macOS Apple Silicon ä¼˜åŒ– |
| **LlamaCpp** | GGUF æ ¼å¼ | è·¨å¹³å°å…¼å®¹ï¼Œèµ„æºå—é™ç¯å¢ƒ |

### å¿«é€Ÿè·å–æ¨¡å‹

#### 1. GGUF æ¨¡å‹ (æ¨èæ–°æ‰‹)
```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models

# ä¸‹è½½ Qwen2.5-7B GGUF æ¨¡å‹ (çº¦ 4.3GB)
cd models
wget https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_0.gguf

# æˆ–ä½¿ç”¨ Hugging Face CLI
pip install huggingface_hub
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-GGUF qwen2.5-7b-instruct-q4_0.gguf --local-dir models
```

#### 2. HuggingFace æ¨¡å‹
```bash
# ä¸‹è½½å®Œæ•´æ¨¡å‹ (çº¦ 15GB)
git lfs install
git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct models/Qwen2.5-7B-Instruct
```

### åŠ¨æ€æ¨¡å‹ç®¡ç†

```bash
# åŠ è½½æ–°æ¨¡å‹
curl -X POST http://localhost:8000/v1/models/load \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "my-custom-model",
    "model_path": "./models/my-model.gguf",
    "engine_type": "llama_cpp"
  }'

# æŸ¥çœ‹å·²åŠ è½½æ¨¡å‹
curl http://localhost:8000/v1/models

# æŸ¥çœ‹æ¨¡å‹è¯¦ç»†çŠ¶æ€
curl http://localhost:8000/v1/models/my-custom-model/status

# å¸è½½æ¨¡å‹é‡Šæ”¾èµ„æº
curl -X DELETE http://localhost:8000/v1/models/my-custom-model/unload
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### å®æ—¶ç›‘æ§

```bash
# ç³»ç»Ÿæ•´ä½“çŠ¶æ€
curl http://localhost:8000/v1/system/status

# æ€§èƒ½æŒ‡æ ‡
curl http://localhost:8000/v1/system/metrics

# æœ€è¿‘æ—¥å¿— (æœ€å100è¡Œ)
curl http://localhost:8000/v1/logs/recent?limit=100
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹æ¨ç†æ€§èƒ½
tail -f logs/inference.log | grep "tokens_per_second"

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" logs/inference.log | tail -20

# æŸ¥çœ‹æ¨¡å‹æ“ä½œæ—¥å¿—
grep "model_operation" logs/metrics.log
```

### æ€§èƒ½æŒ‡æ ‡è§£è¯»

```json
{
  "system": {
    "cpu_usage": 45.2,          // CPU ä½¿ç”¨ç‡ %
    "memory_usage": 67.8,       // å†…å­˜ä½¿ç”¨ç‡ %
    "gpu_memory_usage": 82.3    // GPU æ˜¾å­˜ä½¿ç”¨ç‡ %
  },
  "inference": {
    "active_models": 2,         // å½“å‰æ´»è·ƒæ¨¡å‹æ•°
    "total_requests": 1542,     // æ€»è¯·æ±‚æ•°
    "avg_response_time": 1.23,  // å¹³å‡å“åº”æ—¶é—´ (ç§’)
    "tokens_per_second": 125.4  // å¹³å‡ç”Ÿæˆé€Ÿåº¦ (token/ç§’)
  }
}
```

---

## ğŸš€ éƒ¨ç½²æŒ‡å—

### å¼€å‘ç¯å¢ƒ

```bash
# å¯åŠ¨å¼€å‘æœåŠ¡å™¨ (è‡ªåŠ¨é‡è½½)
python run.py --mode dev

# å¯åŠ¨æ—¶è·³è¿‡ä¾èµ–æ£€æŸ¥ (åŠ å¿«å¯åŠ¨)
python run.py --mode dev --skip-check
```

### ç”Ÿäº§ç¯å¢ƒ

```bash
# ç”Ÿäº§æ¨¡å¼å¯åŠ¨ (ä½¿ç”¨ Gunicorn)
python run.py --mode prod

# æ‰‹åŠ¨ä½¿ç”¨ Gunicorn å¯åŠ¨
pip install gunicorn gevent
gunicorn --bind 0.0.0.0:8000 --workers 4 --worker-class gevent \
  --timeout 300 --keepalive 5 --max-requests 1000 \
  src.api.app:get_app
```

### Docker éƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "run.py", "--mode", "prod"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t vllm-inference-service .
docker run -d -p 8000:8000 \
  -v ./models:/app/models \
  -v ./logs:/app/logs \
  --name vllm-service \
  vllm-inference-service
```

### ä½¿ç”¨ Docker Compose

```yaml
# docker-compose.yml
version: '3.8'
services:
  vllm-service:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
      - ./cache:/app/cache
    environment:
      - INFERENCE_ENGINE=auto
      - MAX_CONCURRENT_MODELS=2
      - LOG_LEVEL=INFO
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - vllm-service
```

### ç³»ç»ŸæœåŠ¡ (Linux)

```ini
# /etc/systemd/system/vllm-inference.service
[Unit]
Description=VLLM Inference Service
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/opt/vllm-inference
ExecStart=/opt/vllm-inference/venv/bin/python run.py --mode prod
Restart=always
RestartSec=10
Environment=PATH=/opt/vllm-inference/venv/bin

[Install]
WantedBy=multi-user.target
```

```bash
# å®‰è£…å’Œå¯åŠ¨ç³»ç»ŸæœåŠ¡
sudo systemctl enable vllm-inference
sudo systemctl start vllm-inference
sudo systemctl status vllm-inference
```

---

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### âŒ æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**: `ModelLoadError: æ¨¡å‹åŠ è½½å¤±è´¥`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la models/

# 2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æƒé™
chmod 644 models/*.gguf

# 3. æ£€æŸ¥å¯ç”¨å†…å­˜/æ˜¾å­˜
# GPU æ˜¾å­˜æ£€æŸ¥ (CUDA)
nvidia-smi
# ç³»ç»Ÿå†…å­˜æ£€æŸ¥
free -h

# 4. é™ä½å†…å­˜ä½¿ç”¨
export MAX_GPU_MEMORY=0.6
export MAX_CONCURRENT_MODELS=1
```

#### âŒ æ¨ç†é€Ÿåº¦æ…¢

**é—®é¢˜**: Token ç”Ÿæˆé€Ÿåº¦ < 10 tokens/sec

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ¨ç†å¼•æ“
curl http://localhost:8000/v1/system/status

# 2. ä¼˜åŒ–å¹¶å‘é…ç½®
export MAX_CONCURRENT_REQUESTS=50
export WORKERS=8

# 3. å¯ç”¨ç¼“å­˜åŠ é€Ÿ
export ENABLE_CACHING=True
export CACHE_SIZE=1000

# 4. æ£€æŸ¥è®¾å¤‡é…ç½®
# ç¡®ä¿ä½¿ç”¨ GPU åŠ é€Ÿ
export DEVICE_TYPE=cuda  # æˆ– mps (macOS)
```

#### âŒ CUDA/GPU ç›¸å…³é”™è¯¯

**é—®é¢˜**: `CUDA out of memory` æˆ– GPU ä¸å¯ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. é™ä½ GPU å†…å­˜ä½¿ç”¨
export MAX_GPU_MEMORY=0.5

# 2. ä½¿ç”¨ CPU æ¨¡å¼
export INFERENCE_ENGINE=llama_cpp
export DEVICE_TYPE=cpu

# 3. æ£€æŸ¥ CUDA å®‰è£…
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# 4. é‡å¯æœåŠ¡æ¸…ç†å†…å­˜
sudo systemctl restart vllm-inference
```

#### âŒ n8n è¿æ¥é—®é¢˜

**é—®é¢˜**: n8n ä¸­æ— æ³•è¿æ¥åˆ°æœåŠ¡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ç¡®ä¿æœåŠ¡è¿è¡Œåœ¨æ­£ç¡®ç«¯å£
curl http://localhost:8000/health

# 2. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
sudo ufw allow 8000/tcp

# 3. n8n é…ç½®æ£€æŸ¥
# Base URL: http://YOUR_SERVER_IP:8000/v1
# API Key: å¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ä¸²

# 4. ç½‘ç»œè¿é€šæ€§æµ‹è¯•
curl -X POST http://YOUR_SERVER_IP:8000/v1/models
```

### è°ƒè¯•å·¥å…·

```bash
# è¿è¡Œå®Œæ•´çš„ç³»ç»Ÿè¯Šæ–­
python run.py --mode dev --debug

# OpenAI API å…¼å®¹æ€§æµ‹è¯•
python test_openai_compatibility.py

# å¹³å°å…¼å®¹æ€§æ£€æŸ¥
python src/utils/platform_detector.py

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
tail -f logs/inference.log

# æ€§èƒ½åŸºå‡†æµ‹è¯•
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{...}' \
  -w "Response Time: %{time_total}s\n"
```

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### æœ€ä½è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| **Python** | 3.11+ | 3.11+ |
| **å†…å­˜** | 8GB | 32GB+ |
| **å­˜å‚¨** | 20GB | 500GB+ SSD |
| **ç½‘ç»œ** | 1Mbps | 100Mbps+ |

### ç¡¬ä»¶æ¨è

#### ğŸ macOS (æ¨èé…ç½®)
- **CPU**: M3 Ultra æˆ–æ›´é«˜
- **å†…å­˜**: 64GB+ (æ”¯æŒå¤šä¸ªå¤§æ¨¡å‹å¹¶è¡Œ)
- **å­˜å‚¨**: 1TB+ SSD
- **æ¨ç†å¼•æ“**: MLX (Metal ä¼˜åŒ–)

#### ğŸ§ Linux é«˜æ€§èƒ½é…ç½®
- **CPU**: Intel Xeon æˆ– AMD EPYC
- **GPU**: RTX 4090 (24GB) / A100 (80GB)
- **å†…å­˜**: 64GB+ DDR4/DDR5
- **å­˜å‚¨**: 1TB+ NVMe SSD
- **æ¨ç†å¼•æ“**: VLLM (CUDA ä¼˜åŒ–)

#### ğŸªŸ Windows é…ç½®
- **CPU**: Intel i7-12700K æˆ–æ›´é«˜
- **GPU**: RTX 4070 Ti (12GB) æˆ–æ›´é«˜
- **å†…å­˜**: 32GB+ DDR4
- **å­˜å‚¨**: 500GB+ NVMe SSD
- **æ¨ç†å¼•æ“**: VLLM æˆ– LlamaCpp

#### ğŸ’» CPU é€šç”¨é…ç½®
- **CPU**: 8æ ¸å¿ƒ 16çº¿ç¨‹ä»¥ä¸Š
- **å†…å­˜**: 16GB+ (æ¨è 32GB)
- **å­˜å‚¨**: 250GB+ SSD
- **æ¨ç†å¼•æ“**: LlamaCpp

---

## ğŸ¤ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
vllmæ¨ç†æ¡†æ¶/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                  # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ inference_engine.py   # æ¨ç†å¼•æ“æŠ½è±¡
â”‚   â”‚   â”œâ”€â”€ model_manager.py      # æ¨¡å‹ç®¡ç†å™¨
â”‚   â”‚   â””â”€â”€ exceptions.py         # å¼‚å¸¸å®šä¹‰
â”‚   â”œâ”€â”€ engines/               # æ¨ç†å¼•æ“å®ç°
â”‚   â”‚   â”œâ”€â”€ vllm_engine.py        # VLLM å¼•æ“
â”‚   â”‚   â”œâ”€â”€ mlx_engine.py         # MLX å¼•æ“
â”‚   â”‚   â””â”€â”€ llamacpp_engine.py    # LlamaCpp å¼•æ“
â”‚   â”œâ”€â”€ api/                   # Web API å±‚
â”‚   â”‚   â”œâ”€â”€ app.py                # Flask åº”ç”¨
â”‚   â”‚   â””â”€â”€ routes/               # API è·¯ç”±
â”‚   â””â”€â”€ utils/                 # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ config.py             # é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ logger.py             # æ—¥å¿—ç³»ç»Ÿ
â”‚       â””â”€â”€ platform_detector.py  # å¹³å°æ£€æµ‹
â”œâ”€â”€ models/                    # æ¨¡å‹å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                      # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ tests/                     # æµ‹è¯•ä»£ç 
â”œâ”€â”€ run.py                     # å¯åŠ¨è„šæœ¬
â””â”€â”€ test_openai_compatibility.py  # å…¼å®¹æ€§æµ‹è¯•
```

### æœ¬åœ°å¼€å‘

```bash
# å…‹éš†å¹¶è®¾ç½®å¼€å‘ç¯å¢ƒ
git clone <repository-url>
cd vllmæ¨ç†æ¡†æ¶
python -m venv venv
source venv/bin/activate  # Linux/macOS
pip install -r requirements.txt

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
python run.py --mode dev

# è¿è¡Œæµ‹è¯•
python -m pytest tests/
python test_openai_compatibility.py

# ä»£ç æ ¼å¼åŒ–
pip install black isort
black src/
isort src/
```

### æ·»åŠ æ–°çš„æ¨ç†å¼•æ“

1. åœ¨ `src/engines/` ä¸­åˆ›å»ºæ–°çš„å¼•æ“å®ç°
2. ç»§æ‰¿ `InferenceEngine` åŸºç±»
3. å®ç°å¿…éœ€çš„æŠ½è±¡æ–¹æ³•
4. åœ¨ `src/engines/__init__.py` ä¸­æ³¨å†Œ
5. æ›´æ–°å¹³å°æ£€æµ‹é€»è¾‘

### æäº¤ä»£ç 

```bash
# ä»£ç è´¨é‡æ£€æŸ¥
black src/ tests/
isort src/ tests/
flake8 src/ tests/

# è¿è¡Œæµ‹è¯•
python -m pytest tests/ -v
python test_openai_compatibility.py

# æäº¤ä»£ç 
git add .
git commit -m "feat: æ·»åŠ æ–°åŠŸèƒ½"
git push origin feature/your-feature
```

---

## ğŸ¤ è´¡çŒ®å’Œæ”¯æŒ

### å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼

1. **ğŸ› æŠ¥å‘Š Bug**: [åˆ›å»º Issue](../../issues)
2. **ğŸ’¡ åŠŸèƒ½å»ºè®®**: [åŠŸèƒ½è¯·æ±‚](../../issues)
3. **ğŸ“ æ”¹è¿›æ–‡æ¡£**: æäº¤æ–‡æ¡£ PR
4. **ğŸ’» ä»£ç è´¡çŒ®**: Fork é¡¹ç›®å¹¶æäº¤ PR

### è´¡çŒ®æµç¨‹

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹: `git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/amazing-feature`
5. åˆ›å»º Pull Request

### å¼€å‘è§„èŒƒ

- âœ… éµå¾ª PEP 8 ä»£ç è§„èŒƒ
- âœ… æ·»åŠ ç±»å‹æç¤º (Type Hints)
- âœ… ç¼–å†™å•å…ƒæµ‹è¯•
- âœ… æ›´æ–°ç›¸å…³æ–‡æ¡£
- âœ… ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬å·

### è·å–å¸®åŠ©

- ğŸ“š **è¯¦ç»†æ–‡æ¡£**: [å¼€å‘æ–‡æ¡£.md](å¼€å‘æ–‡æ¡£.md) | [ä½¿ç”¨æŒ‡å—.md](ä½¿ç”¨æŒ‡å—.md)
- ğŸ› **é—®é¢˜æŠ¥å‘Š**: [GitHub Issues](../../issues)
- ğŸ’¬ **ç¤¾åŒºè®¨è®º**: [GitHub Discussions](../../discussions)
- ğŸ“§ **æŠ€æœ¯æ”¯æŒ**: é€šè¿‡ Issue è”ç³»æˆ‘ä»¬

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚æ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘æœ¬è½¯ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„è´¡çŒ®ï¼š

- **[VLLM](https://github.com/vllm-project/vllm)** - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- **[MLX](https://github.com/ml-explore/mlx)** - Apple Silicon ä¼˜åŒ–æ¡†æ¶
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - è·¨å¹³å° CPU æ¨ç†
- **[Flask](https://flask.palletsprojects.com/)** - Web åº”ç”¨æ¡†æ¶
- **[Qwen](https://huggingface.co/Qwen)** - ä¼˜ç§€çš„å¼€æºæ¨¡å‹

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€

- âœ… **ç¨³å®šç‰ˆæœ¬**: v1.0.0
- ğŸš€ **æ´»è·ƒå¼€å‘**: æŒç»­æ›´æ–°å’Œæ”¹è¿›
- ğŸ”„ **å…¼å®¹æ€§**: å®Œå…¨ OpenAI API å…¼å®¹
- ğŸŒ **è·¨å¹³å°**: Windows, macOS, Linux å…¨æ”¯æŒ

---

<div align="center">

**ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

[â­ Star this repo](../../stargazers) | [ğŸ› Report bug](../../issues) | [ğŸ’¡ Request feature](../../issues) | [ğŸ“– Documentation](å¼€å‘æ–‡æ¡£.md)

---

*è®©æ¯ä¸ªå¼€å‘è€…éƒ½èƒ½è½»æ¾æ‹¥æœ‰æœ¬åœ°åŒ–çš„ AI èƒ½åŠ›* ğŸš€

</div>