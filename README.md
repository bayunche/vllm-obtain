# ğŸš€ VLLM è·¨å¹³å°æ¨ç†æœåŠ¡

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Windows%20|%20macOS%20|%20Linux-lightgrey.svg)](README.md)
[![OpenAI](https://img.shields.io/badge/OpenAI-Compatible-green.svg)](README.md)

> **ğŸ¯ ä¼ä¸šçº§ AI æ¨ç†æœåŠ¡ï¼Œ100% OpenAI API å…¼å®¹ï¼Œæ”¯æŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢**

ä¸“ä¸ºç°ä»£ AI åº”ç”¨è®¾è®¡çš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡ï¼Œå®Œç¾æ”¯æŒ **n8n**ã€**Dify**ã€**LangChain**ã€**AutoGPT** ç­‰å·¥å…·ï¼Œé›¶ä¿®æ”¹æ›¿æ¢ OpenAI APIã€‚

## â­ äº®ç‚¹ç‰¹æ€§

### ğŸ”¥ **åŠ¨æ€æ¨¡å‹ç®¡ç†** 
- âœ¨ **çƒ­åˆ‡æ¢æ¨¡å‹** - è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢ï¼Œæ— éœ€é‡å¯
- ğŸ§  **æ™ºèƒ½å†…å­˜ç®¡ç†** - è‡ªåŠ¨åŠ è½½/å¸è½½ï¼Œé¿å…å†…å­˜æº¢å‡º
- ğŸ”„ **å¹¶å‘æ§åˆ¶** - æ”¯æŒå¤šæ¨¡å‹åŒæ—¶è¿è¡Œ
- ğŸ“Š **æ¨¡å‹ç›‘æ§** - å®æ—¶çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡ç›‘æ§

### âš¡ **æ™ºèƒ½å¼•æ“é€‰æ‹©**
- **ğŸ macOS Apple Silicon** â†’ MLX (Metal æé€Ÿä¼˜åŒ–)  
- **ğŸ§ Linux CUDA** â†’ VLLM (GPU é«˜æ€§èƒ½æ¨ç†)
- **ğŸªŸ Windows** â†’ LlamaCpp (æœ€ä½³å…¼å®¹æ€§)
- **ğŸ”„ è‡ªåŠ¨å›é€€** â†’ å¼•æ“æ•…éšœæ—¶æ™ºèƒ½åˆ‡æ¢

### ğŸ”Œ **å®Œç¾ OpenAI å…¼å®¹**
- âœ… 100% API æ ¼å¼å…¼å®¹
- âœ… æµå¼å“åº”æ”¯æŒ  
- âœ… é›¶ä»£ç è¿ç§»
- âœ… ä¼ä¸šå·¥å…·ç›´æ¥æ›¿æ¢

### ğŸ—ï¸ **ä¼ä¸šçº§æ¶æ„**
- ğŸš€ Flask + å¼‚æ­¥äº‹ä»¶å¾ªç¯
- ğŸ”’ å¹¶å‘å®‰å…¨é”æœºåˆ¶  
- ğŸ›¡ï¸ å®Œæ•´é”™è¯¯å¤„ç†
- ğŸ“Š ç»“æ„åŒ–æ—¥å¿—å’Œç›‘æ§
- ğŸ§ª å…¨é¢æµ‹è¯•è¦†ç›–

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å®‰è£…ä¾èµ–
```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo>
cd vllm-obtain

# å®‰è£…ä¾èµ–ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹³å°ï¼‰
python run.py
```

### 2ï¸âƒ£ æ³¨å†Œæ¨¡å‹
```bash
# è‡ªåŠ¨å‘ç°å¹¶æ³¨å†Œæ‰€æœ‰å¯ç”¨æ¨¡å‹
python tools/register_models.py

# æµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½
python tools/register_models.py --test
```

### 3ï¸âƒ£ å¯åŠ¨æœåŠ¡
```bash
# ä¸€é”®å¯åŠ¨ï¼ˆè‡ªåŠ¨é…ç½®ï¼‰
python run.py

# æˆ–ä½¿ç”¨ç”Ÿäº§æ¨¡å¼
gunicorn -c gunicorn.conf.py src.api.app:application
```

### 4ï¸âƒ£ éªŒè¯æœåŠ¡
```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8001/health

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8001/v1/models

# æµ‹è¯•èŠå¤©
curl -X POST http://localhost:8001/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{
    "model": "qwen-0.5b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

---

## ğŸ’¡ ä½¿ç”¨æ–¹å¼

### ğŸ”„ åŠ¨æ€æ¨¡å‹åˆ‡æ¢
```python
# åŒä¸€ä¸ªAPIç«¯ç‚¹ï¼Œä¸åŒæ¨¡å‹
import openai

client = openai.OpenAI(
    api_key="not-needed",
    base_url="http://localhost:8001/v1"
)

# ä½¿ç”¨æ¨¡å‹1
response = client.chat.completions.create(
    model="qwen-0.5b",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

# æ— ç¼åˆ‡æ¢åˆ°æ¨¡å‹2ï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
response = client.chat.completions.create(
    model="GLM-4.5V", 
    messages=[{"role": "user", "content": "Hello"}]
)
```

### ğŸ› ï¸ n8n é›†æˆ
1. æ‰“å¼€ n8n OpenAI èŠ‚ç‚¹é…ç½®
2. ä¿®æ”¹ Base URL: `http://localhost:8001/v1`
3. API Key: å¡«å…¥ä»»æ„å€¼ï¼ˆæœ¬åœ°ä¸éœ€è¦ï¼‰
4. åœ¨ `model` å‚æ•°ä¸­æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹

### ğŸ”§ é…ç½®æ–‡ä»¶
```bash
# ç¼–è¾‘å¹³å°é…ç½®
vim .env.mac     # macOS
vim .env.linux   # Linux  
vim .env.windows # Windows

# å…³é”®é…ç½®
MAX_CONCURRENT_MODELS=3    # åŒæ—¶è¿è¡Œçš„æ¨¡å‹æ•°é‡
MODEL_BASE_PATH=./models   # æ¨¡å‹åŸºç¡€è·¯å¾„
INFERENCE_ENGINE=auto      # è‡ªåŠ¨é€‰æ‹©å¼•æ“
```

---

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

### ğŸ¯ æµ‹è¯•éªŒè¯çš„æ¨¡å‹
| æ¨¡å‹ | å¤§å° | æ”¯æŒå¼•æ“ | çŠ¶æ€ |
|------|------|----------|------|
| **Qwen2.5-0.5B** | 1.2GB | MLX/VLLM/LlamaCpp | âœ… å®Œå…¨æ”¯æŒ |
| **GLM-4.5V** | 200GB | MLX/VLLM | âš ï¸ éœ€å®Œæ•´ä¸‹è½½ |
| **Llama 3.1** | 8B+ | æ‰€æœ‰å¼•æ“ | âœ… å®Œå…¨æ”¯æŒ |

### ğŸ”½ æ¨¡å‹ä¸‹è½½
```bash
# ä¸‹è½½é¢„é…ç½®æ¨¡å‹
python scripts/setup/download_model.py --model qwen-0.5b

# ä» ModelScope ä¸‹è½½
python scripts/setup/download_glm4v.py

# ä» HuggingFace ä¸‹è½½
python scripts/setup/download_model.py --source huggingface --model microsoft/DialoGPT-small
```

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
vllm-obtain/
â”œâ”€â”€ ğŸ“ src/                     # ğŸ“¦ æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ api/                    # ğŸŒ Flask APIå±‚
â”‚   â”‚   â”œâ”€â”€ app.py             # ğŸš€ ä¸»åº”ç”¨å…¥å£
â”‚   â”‚   â””â”€â”€ routes/            # ğŸ›¤ï¸ APIè·¯ç”±
â”‚   â”‚       â”œâ”€â”€ openai_compat.py   # ğŸ”Œ OpenAIå…¼å®¹æ¥å£
â”‚   â”‚       â””â”€â”€ management.py      # âš™ï¸ ç®¡ç†æ¥å£
â”‚   â”œâ”€â”€ core/                  # ğŸ§  æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ model_manager.py   # ğŸ”¥ åŠ¨æ€æ¨¡å‹ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ inference_engine.py # âš¡ æ¨ç†å¼•æ“æŠ½è±¡
â”‚   â”‚   â””â”€â”€ exceptions.py      # ğŸš¨ å¼‚å¸¸å¤„ç†
â”‚   â”œâ”€â”€ engines/               # ğŸš‚ æ¨ç†å¼•æ“å®ç°
â”‚   â”‚   â”œâ”€â”€ mlx_engine.py      # ğŸ Apple MLXå¼•æ“
â”‚   â”‚   â”œâ”€â”€ vllm_engine.py     # ğŸ§ VLLMå¼•æ“ (Linux)
â”‚   â”‚   â””â”€â”€ llamacpp_engine.py # ğŸ’» LlamaCppå¼•æ“ (CPU)
â”‚   â””â”€â”€ utils/                 # ğŸ› ï¸ å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ config.py          # âš™ï¸ é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ logger.py          # ğŸ“ æ—¥å¿—ç³»ç»Ÿ
â”‚       â””â”€â”€ platform_detector.py # ğŸ” å¹³å°æ£€æµ‹
â”‚
â”œâ”€â”€ ğŸ“ tools/                   # ğŸ”§ ç®¡ç†å·¥å…·
â”‚   â””â”€â”€ register_models.py     # ğŸ“‹ æ¨¡å‹æ³¨å†Œå·¥å…·
â”‚
â”œâ”€â”€ ğŸ“ scripts/                 # ğŸ“œ è„šæœ¬å·¥å…·
â”‚   â”œâ”€â”€ setup/                 # ğŸ—ï¸ å®‰è£…é…ç½®è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ install.sh         # ğŸ› ï¸ è‡ªåŠ¨å®‰è£…è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ download_model.py  # â¬‡ï¸ æ¨¡å‹ä¸‹è½½å·¥å…·
â”‚   â”‚   â””â”€â”€ check_dependencies.py # âœ… ä¾èµ–æ£€æŸ¥
â”‚   â”œâ”€â”€ tests/                 # ğŸ§ª æµ‹è¯•è„šæœ¬
â”‚   â””â”€â”€ benchmarks/            # ğŸ“Š æ€§èƒ½æµ‹è¯•
â”‚
â”œâ”€â”€ ğŸ“ tests/                   # ğŸ§ª å•å…ƒæµ‹è¯•
â”œâ”€â”€ ğŸ“ models/                  # ğŸ¤– æ¨¡å‹å­˜å‚¨ (gitignore)
â”œâ”€â”€ ğŸ“ logs/                    # ğŸ“„ æ—¥å¿—æ–‡ä»¶ (gitignore)  
â”œâ”€â”€ ğŸ“ cache/                   # ğŸ’¾ ç¼“å­˜ç›®å½• (gitignore)
â””â”€â”€ ğŸ“ archive/                 # ğŸ“š å†å²æ–‡æ¡£å½’æ¡£
```

---

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### å¿«é€Ÿæµ‹è¯•
```bash
# è¿è¡Œå…¼å®¹æ€§æµ‹è¯•
python scripts/tests/test_compatibility_report.py

# å¹¶å‘æ€§èƒ½æµ‹è¯•  
./run_tests.sh

# å•å…ƒæµ‹è¯•
pytest tests/
```

### æµ‹è¯•æŠ¥å‘Š
- **âœ… APIå…¼å®¹æ€§**: 100% OpenAIæ ¼å¼å…¼å®¹
- **âœ… å¹¶å‘å¤„ç†**: æ”¯æŒ20+å¹¶å‘è¯·æ±‚
- **âœ… æ¨¡å‹åˆ‡æ¢**: åŠ¨æ€åŠ è½½å¹³å‡ < 5ç§’
- **âœ… é”™è¯¯æ¢å¤**: æ•…éšœè‡ªåŠ¨å¤„ç†å’Œæ¢å¤

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

| æŒ‡æ ‡ | macOS M3 Ultra | Linux RTX 4090 | Windows GTX 1080 |
|------|----------------|-----------------|------------------|
| **å¹¶å‘è¯·æ±‚** | 20+ | 30+ | 15+ |
| **å“åº”æ—¶é—´** | < 100ms | < 80ms | < 200ms |
| **ååé‡** | 100+ tokens/s | 150+ tokens/s | 60+ tokens/s |
| **æ¨¡å‹åˆ‡æ¢** | < 3s | < 5s | < 10s |

---

## ğŸ”§ ç”Ÿäº§éƒ¨ç½²

### Docker éƒ¨ç½²
```bash
# æ„å»ºé•œåƒ
docker build -t vllm-inference .

# è¿è¡Œå®¹å™¨
docker run -p 8001:8001 -v ./models:/app/models vllm-inference
```

### è´Ÿè½½å‡è¡¡
```bash
# å¯åŠ¨å¤šå®ä¾‹è´Ÿè½½å‡è¡¡
python -m src.api.load_balanced_app --workers 3 --port 8001
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒ
```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œæµ‹è¯•
pytest tests/

# ä»£ç æ ¼å¼åŒ–
black src/ tests/
```

### æ·»åŠ æ–°å¼•æ“
1. ç»§æ‰¿ `InferenceEngine` åŸºç±»
2. å®ç°å¿…éœ€çš„æ¥å£æ–¹æ³•
3. åœ¨ `engines/__init__.py` ä¸­æ³¨å†Œ
4. æ·»åŠ å¯¹åº”æµ‹è¯•

---

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™‹â€â™‚ï¸ æ”¯æŒä¸åé¦ˆ

- **ğŸ› BugæŠ¥å‘Š**: [Issues](https://github.com/your-repo/issues)
- **ğŸ’¡ åŠŸèƒ½å»ºè®®**: [Discussions](https://github.com/your-repo/discussions)  
- **ğŸ“§ å•†ä¸šæ”¯æŒ**: your-email@example.com

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒï¼**