# ğŸš€ VLLM è·¨å¹³å°æ¨ç†æœåŠ¡

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Windows%20|%20macOS%20|%20Linux-lightgrey.svg)](README.md)
[![OpenAI](https://img.shields.io/badge/OpenAI-Compatible-green.svg)](README.md)
[![Tested](https://img.shields.io/badge/Tested-Mac%20Studio%20M3%20Ultra-success.svg)](Mac_Studio_å®Œæ•´æµ‹è¯•æŠ¥å‘Š_20250821.md)

> ğŸ¯ **ä¼ä¸šçº§è·¨å¹³å° AI æ¨ç†æœåŠ¡ï¼Œå®Œå…¨å…¼å®¹ OpenAI API**

ä¸“ä¸ºä¼ä¸šå’Œå¼€å‘è€…è®¾è®¡çš„é«˜æ€§èƒ½è·¨å¹³å°æ¨ç†æœåŠ¡ï¼Œæ”¯æŒåœ¨ **n8n**ã€**Dify**ã€**LangChain** ç­‰å·¥å…·ä¸­æ— ç¼æ›¿æ¢ OpenAI APIï¼Œå®ç°å®Œå…¨æœ¬åœ°åŒ–çš„ AI èƒ½åŠ›ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ **æ™ºèƒ½å¹³å°é€‚é…**
- **ğŸ macOS Apple Silicon** â†’ MLX å¼•æ“ (Metal åŠ é€Ÿï¼Œæè‡´ä¼˜åŒ–)
- **ğŸ§ Linux CUDA/ROCm** â†’ VLLM å¼•æ“ (é«˜æ€§èƒ½ GPU æ¨ç†)  
- **ğŸªŸ Windows CUDA** â†’ VLLM/LlamaCpp å¼•æ“ (å…¼å®¹æ€§ä¼˜å…ˆ)
- **ğŸ’» é€šç”¨ CPU** â†’ LlamaCpp å¼•æ“ (æœ€å¹¿å…¼å®¹æ€§)
- **âš¡ é›¶é…ç½®å¯åŠ¨** â†’ æ™ºèƒ½æ£€æµ‹ç¡¬ä»¶ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å¼•æ“

### ğŸ”Œ **å®Œç¾ OpenAI å…¼å®¹**
- âœ… **100% API å…¼å®¹** OpenAI ChatGPT API æ ¼å¼
- âœ… **é›¶ä»£ç è¿ç§»** ç°æœ‰ OpenAI å®¢æˆ·ç«¯æ— éœ€ä¿®æ”¹
- âœ… **æµå¼å“åº”æ”¯æŒ** å®æ—¶ Token ç”Ÿæˆä½“éªŒ
- âœ… **n8n ç›´æ¥æ›¿æ¢** åªéœ€ä¿®æ”¹ Base URL

### âš¡ **ç”Ÿäº§çº§æ€§èƒ½**
- ğŸš€ **ç»è¿‡éªŒè¯**: Mac Studio M3 Ultra æµ‹è¯•é€šè¿‡ ([æµ‹è¯•æŠ¥å‘Š](Mac_Studio_å®Œæ•´æµ‹è¯•æŠ¥å‘Š_20250821.md))
- ğŸ“Š **é«˜å¹¶å‘æ”¯æŒ**: å•æœº 20+ å¹¶å‘ï¼Œ100% æˆåŠŸç‡  
- ğŸ”„ **åŠ¨æ€æ¨¡å‹ç®¡ç†** è¿è¡Œæ—¶åŠ è½½/å¸è½½æ¨¡å‹
- ğŸ›¡ï¸ **æ•…éšœè‡ªæ„ˆ** è‡ªåŠ¨é‡è¯•ã€ä¼˜é›…é™çº§ã€å¥åº·æ£€æŸ¥

### ğŸ® **å¼€å‘è€…å‹å¥½**
- ğŸ¯ **ä¸€é”®å¯åŠ¨** `python run.py`
- ğŸ” **æ™ºèƒ½è¯Šæ–­** è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå’Œä¾èµ–é—®é¢˜
- ğŸ“ **ç»“æ„åŒ–æ—¥å¿—** è¯¦ç»†æ€§èƒ½ç›‘æ§å’Œé”™è¯¯è¿½è¸ª
- ğŸ§ª **å®Œæ•´æµ‹è¯•** OpenAI å…¼å®¹æ€§å…¨é¢éªŒè¯

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

| å¹³å° | æœ€ä½é…ç½® | æ¨èé…ç½® | æ¨ç†å¼•æ“ |
|------|----------|----------|----------|
| **macOS** | M1, 16GB RAM | M3 Ultra, 64GB RAM | MLX |
| **Linux** | GTX 1080, 16GB RAM | RTX 4090, 32GB RAM | VLLM |
| **Windows** | GTX 1070, 16GB RAM | RTX 4080, 32GB RAM | VLLM/LlamaCpp |
| **é€šç”¨CPU** | 8æ ¸, 16GB RAM | 32æ ¸, 64GB RAM | LlamaCpp |

### ğŸ› ï¸ å®‰è£…éƒ¨ç½²

#### æ–¹å¼ä¸€ï¼šè‡ªåŠ¨å®‰è£…è„šæœ¬ (æ¨è)

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/vllm-obtain
cd vllm-obtain

# 2. è¿è¡Œä¸€é”®å®‰è£…è„šæœ¬
./install.sh  # Linux/macOS
# æˆ– install.bat  # Windows

# 3. å¯åŠ¨æœåŠ¡ 
python run.py
```

#### æ–¹å¼äºŒï¼šæ‰‹åŠ¨å®‰è£…

<details>
<summary>ğŸ“± macOS (Apple Silicon) å®‰è£…æ­¥éª¤</summary>

```bash
# 1. ç¯å¢ƒå‡†å¤‡
python3 -m venv venv
source venv/bin/activate

# 2. å®‰è£… macOS ä¸“ç”¨ä¾èµ–
pip install -r requirements-mac.txt

# 3. é…ç½®ç¯å¢ƒ (ä½¿ç”¨ Mac ä¼˜åŒ–é…ç½®)
cp .env.mac .env

# 4. ä¸‹è½½æ¨¡å‹ (ä½¿ç”¨ ModelScope å›½å†…é•œåƒ)
python -c "
from modelscope import snapshot_download
snapshot_download('qwen/Qwen2.5-0.5B-Instruct', 
                 cache_dir='./models/qwen-0.5b')
"

# 5. å¯åŠ¨æœåŠ¡
python run.py --mode prod

# âœ… æœåŠ¡åœ°å€: http://127.0.0.1:8001
# ğŸ¯ é…ç½®ç‰¹ç‚¹: MLXå¼•æ“ + å•worker + Geventå¼‚æ­¥
```

**Mac ä¸“ç”¨ä¼˜åŠ¿**:
- ğŸ”¥ **MLX å¼•æ“**: ä¸“ä¸º Apple Silicon ä¼˜åŒ–
- âš¡ **Metal åŠ é€Ÿ**: GPU ç®—åŠ›å®Œå…¨å‘æŒ¥
- ğŸ’¾ **ç»Ÿä¸€å†…å­˜**: 512GB å†…å­˜é«˜æ•ˆåˆ©ç”¨
- ğŸ”‡ **é™éŸ³è¿è¡Œ**: åŠŸè€—ä½ï¼Œå‡ ä¹æ— å™ªéŸ³

</details>

<details>
<summary>ğŸ§ Linux (NVIDIA GPU) å®‰è£…æ­¥éª¤</summary>

```bash
# 1. ç³»ç»Ÿä¾èµ– (Ubuntu/Debian)
sudo apt update
sudo apt install -y python3-dev python3-venv build-essential

# 2. Python ç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# 3. å®‰è£… Linux GPU ä¸“ç”¨ä¾èµ–
pip install -r requirements-linux.txt

# 4. éªŒè¯ CUDA ç¯å¢ƒ
nvidia-smi  # ç¡®è®¤ GPU å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"

# 5. é…ç½®ç¯å¢ƒ (ä½¿ç”¨ Linux å¤š worker é…ç½®)
cp .env.linux .env

# 6. ä¸‹è½½æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir ./models/qwen-0.5b

# 7. å¯åŠ¨é«˜æ€§èƒ½æœåŠ¡
python run.py --mode prod

# âœ… æœåŠ¡åœ°å€: http://0.0.0.0:8001 (ç›‘å¬æ‰€æœ‰æ¥å£)
# ğŸ¯ é…ç½®ç‰¹ç‚¹: VLLMå¼•æ“ + 4workers + é«˜å¹¶å‘
```

**Linux ä¸“ç”¨ä¼˜åŠ¿**:
- ğŸš€ **VLLM å¼•æ“**: æœ€é«˜æ€§èƒ½æ¨ç†å¼•æ“
- ğŸ”¥ **CUDA åŠ é€Ÿ**: NVIDIA GPU å®Œå…¨å‘æŒ¥
- ğŸ“Š **å¤šè¿›ç¨‹**: 4+ workers å¹¶è¡Œå¤„ç†
- ğŸŒ **é«˜å¹¶å‘**: æ”¯æŒ 100+ å¹¶å‘è¯·æ±‚

</details>

<details>
<summary>ğŸªŸ Windows (NVIDIA GPU) å®‰è£…æ­¥éª¤</summary>

```powershell
# 1. ç¯å¢ƒå‡†å¤‡ (éœ€è¦ç®¡ç†å‘˜æƒé™å®‰è£… Visual Studio Build Tools)
# ä¸‹è½½å¹¶å®‰è£…: Visual Studio Community æˆ– Build Tools

# 2. Python ç¯å¢ƒ
python -m venv venv
.\venv\Scripts\activate

# 3. å®‰è£… Windows ä¸“ç”¨ä¾èµ–
pip install -r requirements-windows.txt

# 4. éªŒè¯ GPU ç¯å¢ƒ (å¯é€‰)
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# 5. é…ç½®ç¯å¢ƒ (Windows å…¼å®¹æ€§é…ç½®)
copy .env.windows .env

# 6. ä¸‹è½½æ¨¡å‹ (æ¨èä½¿ç”¨ GGUF æ ¼å¼)
python download_model.py --model qwen-0.5b --format gguf

# 7. å¯åŠ¨æœåŠ¡ (ä½¿ç”¨ Waitress æœåŠ¡å™¨)
python run.py --mode prod

# âœ… æœåŠ¡åœ°å€: http://127.0.0.1:8001
# ğŸ¯ é…ç½®ç‰¹ç‚¹: LlamaCppå¼•æ“ + Waitress + åŒæ­¥å¤„ç†
```

**Windows ä¸“ç”¨ä¼˜åŠ¿**:
- ğŸ”§ **å…¼å®¹æ€§**: æ”¯æŒå„ç§ Windows ç‰ˆæœ¬
- ğŸ›¡ï¸ **ç¨³å®šæ€§**: Waitress æœåŠ¡å™¨ç¨³å®šå¯é 
- ğŸ® **æ¸¸æˆå‹å¥½**: ä¸æŠ¢å æ¸¸æˆ GPU èµ„æº
- ğŸ’» **é›†æˆåº¦**: å®Œç¾èå…¥ Windows ç”Ÿæ€

</details>

<details>
<summary>ğŸ’» CPU é€šç”¨æ¨¡å¼å®‰è£…æ­¥éª¤</summary>

```bash
# é€‚ç”¨äºä»»ä½•å¹³å°çš„ CPU æ¨ç†æ¨¡å¼

# 1. Python ç¯å¢ƒ (é€‚ç”¨äºæ‰€æœ‰å¹³å°)
python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:  
venv\Scripts\activate

# 2. å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# 3. CPU æ¨¡å¼é…ç½®
export INFERENCE_ENGINE=llamacpp
export DEVICE_TYPE=cpu
export MAX_CPU_THREADS=8

# 4. ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹ (CPU ä¼˜åŒ–)
mkdir -p models
wget https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_0.gguf \
     -O models/qwen2.5-0.5b-instruct-q4_0.gguf

# 5. å¯åŠ¨ CPU æ¨¡å¼æœåŠ¡
python run.py

# âœ… é€šç”¨å…¼å®¹ï¼Œæ‰€æœ‰å¹³å°éƒ½èƒ½è¿è¡Œ
```

**CPU æ¨¡å¼ä¼˜åŠ¿**:
- ğŸŒ **é€šç”¨å…¼å®¹**: ä»»ä½•å¹³å°éƒ½èƒ½è¿è¡Œ
- ğŸ’° **æˆæœ¬ä½**: æ— éœ€æ˜‚è´µçš„GPUç¡¬ä»¶
- ğŸ”‹ **åŠŸè€—ä½**: é€‚åˆé•¿æ—¶é—´è¿è¡Œ
- ğŸ› ï¸ **æ˜“ç»´æŠ¤**: ä¾èµ–ç®€å•ï¼Œé—®é¢˜å°‘

</details>

---

## ğŸ“Š å¹³å°å¯¹æ¯”å’Œé€‰æ‹©æŒ‡å—

### æ€§èƒ½å¯¹æ¯”è¡¨

| å¹³å°é…ç½® | ååé‡ | å“åº”æ—¶é—´ | å¹¶å‘æ•° | Tokené€Ÿåº¦ | åŠŸè€— | æ¨èåœºæ™¯ |
|----------|--------|----------|--------|-----------|------|----------|
| **Mac Studio M3 Ultra** | 2.6 req/s | 0.77-7s | 20+ | 6.7 t/s | æä½ | â­â­â­â­â­ å¼€å‘/åŸå‹ |
| **Linux RTX 4090** | 15+ req/s | 0.1-0.5s | 100+ | 200+ t/s | é«˜ | â­â­â­â­â­ ç”Ÿäº§/é«˜å¹¶å‘ |
| **Windows RTX 4080** | 8+ req/s | 0.2-1s | 50+ | 150+ t/s | ä¸­ | â­â­â­â­ ä¼ä¸š/åŠå…¬ |
| **é€šç”¨ CPU (32æ ¸)** | 1 req/s | 2-10s | 5+ | 10+ t/s | ä½ | â­â­â­ å…¼å®¹/å¤‡ç”¨ |

### ğŸ¯ æ¨èé…ç½®é€‰æ‹©

```bash
# ğŸ macOS ç”¨æˆ· (å¼€å‘å’Œæ¼”ç¤ºåœºæ™¯)
å¦‚æœä½ æœ‰: Mac Studio/MacBook Pro M3+
æ¨èé…ç½®: MLX + å•worker + 10å¹¶å‘
ä¼˜åŠ¿: åŠŸè€—ä½ã€å™ªéŸ³å°ã€å¼€ç®±å³ç”¨
```

```bash
# ğŸ§ Linux ç”¨æˆ· (ç”Ÿäº§å’Œé«˜æ€§èƒ½åœºæ™¯) 
å¦‚æœä½ æœ‰: Linux + NVIDIA RTX 4090/A100
æ¨èé…ç½®: VLLM + 4workers + 100å¹¶å‘
ä¼˜åŠ¿: æ€§èƒ½æœ€å¼ºã€å¹¶å‘æœ€é«˜ã€æ‰©å±•æ€§å¥½
```

```bash
# ğŸªŸ Windows ç”¨æˆ· (ä¼ä¸šå’Œä¸ªäººåœºæ™¯)
å¦‚æœä½ æœ‰: Windows + NVIDIA RTX 4070+
æ¨èé…ç½®: VLLM/LlamaCpp + 2workers + 50å¹¶å‘
ä¼˜åŠ¿: å…¼å®¹æ€§å¥½ã€ç¨³å®šå¯é ã€æ˜“ç»´æŠ¤
```

```bash
# ğŸ’» é€šç”¨ CPU ç”¨æˆ· (å…¼å®¹å’Œå¤‡ç”¨åœºæ™¯)
å¦‚æœä½ åªæœ‰: æ™®é€š CPU æœåŠ¡å™¨
æ¨èé…ç½®: LlamaCpp + 1worker + 5å¹¶å‘
ä¼˜åŠ¿: é›¶é—¨æ§›ã€æˆæœ¬ä½ã€æ™®é€‚æ€§å¼º
```

---

## ğŸ”§ é…ç½®å’Œç¯å¢ƒç®¡ç†

### æ™ºèƒ½é…ç½®ç³»ç»Ÿ

é¡¹ç›®æä¾›å¹³å°ç‰¹å®šçš„é…ç½®æ–‡ä»¶ï¼Œè‡ªåŠ¨ä¼˜åŒ–æ€§èƒ½ï¼š

```bash
# é…ç½®æ–‡ä»¶è¯´æ˜
.env.mac        # macOS ä¸“ç”¨ä¼˜åŒ–é…ç½® (MLX + Metal)
.env.linux      # Linux ä¸“ç”¨ä¼˜åŒ–é…ç½® (VLLM + CUDA) 
.env.windows    # Windows ä¸“ç”¨ä¼˜åŒ–é…ç½® (å…¼å®¹æ€§ä¼˜å…ˆ)
.env.example    # é€šç”¨é…ç½®æ¨¡æ¿
```

### ä¾èµ–ç®¡ç†ç³»ç»Ÿ

```bash
# å¹³å°ç‰¹å®šä¾èµ–æ–‡ä»¶
requirements-mac.txt     # macOS: MLX + ç›¸å…³ä¾èµ–
requirements-linux.txt   # Linux: VLLM + CUDA ä¾èµ–  
requirements-windows.txt # Windows: å…¼å®¹æ€§ä¾èµ–
requirements.txt        # é€šç”¨åŸºç¡€ä¾èµ–
```

### ç¯å¢ƒå˜é‡é…ç½®

<details>
<summary>ğŸ”§ æ ¸å¿ƒé…ç½®å‚æ•°è¯´æ˜</summary>

```bash
# === å¼•æ“é€‰æ‹©é…ç½® ===
INFERENCE_ENGINE=auto    # auto/mlx/vllm/llamacpp
DEVICE_TYPE=auto        # auto/cuda/mps/cpu
WORKERS=1              # workerè¿›ç¨‹æ•° (Macå¿…é¡»ä¸º1)

# === æ€§èƒ½è°ƒä¼˜é…ç½® ===
MAX_CONCURRENT_REQUESTS=10   # æœ€å¤§å¹¶å‘è¯·æ±‚
REQUEST_TIMEOUT=120         # è¯·æ±‚è¶…æ—¶æ—¶é—´
WORKER_CLASS=gevent         # workerç±»å‹
MEMORY_LIMIT=80            # å†…å­˜ä½¿ç”¨ä¸Šé™(%)

# === æ¨¡å‹ç®¡ç†é…ç½® ===
DEFAULT_MODEL=qwen-0.5b    # é»˜è®¤æ¨¡å‹
MODEL_DIR=./models         # æ¨¡å‹å­˜å‚¨ç›®å½•
CACHE_SIZE=1000           # ç¼“å­˜å¤§å°
ENABLE_CACHING=true       # å¯ç”¨ç¼“å­˜

# === æ—¥å¿—ç›‘æ§é…ç½® ===
LOG_LEVEL=INFO            # æ—¥å¿—çº§åˆ«
LOG_RETENTION_DAYS=30     # æ—¥å¿—ä¿ç•™å¤©æ•°
ENABLE_METRICS=true       # å¯ç”¨æ€§èƒ½ç›‘æ§
```

</details>

---

## ğŸ§ª éªŒè¯å’Œæµ‹è¯•

### ä¸€é”®åŠŸèƒ½éªŒè¯

```bash
# 1. ç³»ç»Ÿå¥åº·æ£€æŸ¥
curl http://localhost:8001/health

# 2. OpenAI å…¼å®¹æ€§æµ‹è¯• 
python test_openai_compatibility.py

# 3. å¹¶å‘æ€§èƒ½æµ‹è¯•
python concurrent_test.py

# 4. å¹³å°å…¼å®¹æ€§æ£€æŸ¥
python src/utils/platform_detector.py
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# å“åº”æ—¶é—´æµ‹è¯•
curl -w "Time: %{time_total}s\n" -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen-0.5b","messages":[{"role":"user","content":"Hello"}]}'

# å¹¶å‘å‹åŠ›æµ‹è¯• 
python -c "
import concurrent.futures, requests, time
def test_req(): 
    return requests.post('http://localhost:8001/v1/chat/completions', 
                        json={'model':'qwen-0.5b','messages':[{'role':'user','content':'test'}]})
start = time.time()
with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
    results = list(executor.map(lambda _: test_req(), range(10)))
print(f'10 concurrent requests: {time.time()-start:.2f}s')
print(f'Success rate: {sum(1 for r in results if r.status_code==200)/10*100}%')
"
```

---

## ğŸ”Œ OpenAI API å…¼å®¹ä½¿ç”¨

### åœ¨å„ç§å·¥å…·ä¸­ä½¿ç”¨

#### n8n å·¥ä½œæµä½¿ç”¨
```
1. æ·»åŠ  "OpenAI" èŠ‚ç‚¹
2. é…ç½®è¿æ¥:
   - Base URL: http://your-server:8001/v1
   - API Key: any-string (å¯é€‰)
3. é€‰æ‹©æ¨¡å‹: qwen-0.5b
4. å¼€å§‹ä½¿ç”¨! ğŸ‰
```

#### Python OpenAI å®¢æˆ·ç«¯
```python
import openai

# åªéœ€ä¿®æ”¹è¿™ä¸¤è¡Œ
openai.api_base = "http://localhost:8001/v1" 
openai.api_key = "your-key"  # å¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ä¸²

# å…¶ä»–ä»£ç å®Œå…¨ä¸å˜!
response = openai.ChatCompletion.create(
    model="qwen-0.5b",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

#### LangChain é›†æˆ
```python
from langchain.llms import OpenAI

llm = OpenAI(
    openai_api_base="http://localhost:8001/v1",
    openai_api_key="any-key",
    model_name="qwen-0.5b"
)

result = llm("Explain quantum computing")
```

#### cURL å‘½ä»¤è¡Œ
```bash
# èŠå¤©è¡¥å…¨
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-0.5b",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": true
  }'

# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:8001/v1/models
```

---

## ğŸ“Š ç›‘æ§å’Œè¿ç»´

### å®æ—¶ç›‘æ§

```bash
# æœåŠ¡çŠ¶æ€æ£€æŸ¥
curl http://localhost:8001/v1/system/status | jq

# æ€§èƒ½æŒ‡æ ‡ç›‘æ§
curl http://localhost:8001/v1/system/metrics | jq

# å¥åº·æ£€æŸ¥
curl http://localhost:8001/health
```

### æ—¥å¿—åˆ†æ

```bash
# å®æ—¶æ—¥å¿—æŸ¥çœ‹
tail -f logs/app.log

# æ€§èƒ½åˆ†æ
grep "tokens_per_second" logs/app.log | tail -10

# é”™è¯¯è¿½è¸ª  
grep "ERROR" logs/app.log | tail -20
```

### ç”Ÿäº§éƒ¨ç½²

<details>
<summary>ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®</summary>

#### Systemd æœåŠ¡ (Linux)
```ini
# /etc/systemd/system/vllm-inference.service
[Unit]
Description=VLLM Inference Service
After=network.target

[Service]
Type=simple
User=vllm
WorkingDirectory=/opt/vllm-obtain
ExecStart=/opt/vllm-obtain/venv/bin/python run.py --mode prod
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

#### Docker éƒ¨ç½²
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8001
CMD ["python", "run.py", "--mode", "prod"]
```

#### Nginx è´Ÿè½½å‡è¡¡
```nginx
upstream vllm_backend {
    server 127.0.0.1:8001;
    server 127.0.0.1:8002;
    server 127.0.0.1:8003;
}

server {
    listen 80;
    location / {
        proxy_pass http://vllm_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

</details>

---

## ğŸ› æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

<details>
<summary>âŒ MLX å¼•æ“ Metal é”™è¯¯ (Mac)</summary>

**é—®é¢˜**: `[metal::Device] Unable to build metal library from source`

**åŸå› **: å¤š worker æ¨¡å¼ä¸‹ MLX å¼•æ“å†²çª

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ç¡®ä¿ä½¿ç”¨å• worker é…ç½®
export WORKERS=1

# 2. ä½¿ç”¨ Mac ä¸“ç”¨é…ç½®
cp .env.mac .env

# 3. é‡å¯æœåŠ¡
python run.py --mode prod
```

**éªŒè¯**: `curl http://localhost:8001/health` æ˜¾ç¤ºå¥åº·çŠ¶æ€

</details>

<details>
<summary>âŒ CUDA å†…å­˜ä¸è¶³ (Linux/Windows)</summary>

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. é™ä½ GPU å†…å­˜ä½¿ç”¨
export CUDA_MEMORY_FRACTION=0.6

# 2. å‡å°‘å¹¶å‘æ•°
export MAX_CONCURRENT_REQUESTS=10

# 3. ä½¿ç”¨é‡åŒ–æ¨¡å‹
python download_model.py --model qwen-0.5b --quantization int4

# 4. å›é€€åˆ° CPU æ¨¡å¼ 
export INFERENCE_ENGINE=llamacpp
export DEVICE_TYPE=cpu
```

</details>

<details>
<summary>âŒ ä¾èµ–å®‰è£…é—®é¢˜</summary>

**é—®é¢˜**: å„ç§åŒ…å®‰è£…å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# Mac: ä½¿ç”¨ Homebrew
brew install python@3.11
pip install -r requirements-mac.txt

# Linux: å®‰è£…ç³»ç»Ÿä¾èµ–
sudo apt install python3-dev build-essential
pip install -r requirements-linux.txt

# Windows: å®‰è£… Visual Studio Build Tools
# ä¸‹è½½: https://visualstudio.microsoft.com/zh-hans/downloads/
pip install -r requirements-windows.txt
```

</details>

### ğŸ” è°ƒè¯•å·¥å…·

```bash
# è¿è¡Œå®Œæ•´è¯Šæ–­
python run.py --mode dev --debug

# å¹³å°æ£€æµ‹
python src/utils/platform_detector.py

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python concurrent_test.py

# OpenAI å…¼å®¹æ€§æµ‹è¯•
python test_openai_compatibility.py
```

---

## ğŸ“š æ–‡æ¡£èµ„æº

- ğŸ“– **[é¡¹ç›®ç»“æ„è¯´æ˜](PROJECT_STRUCTURE.md)** - ä»£ç æ¶æ„å’Œå¼€å‘æŒ‡å—
- ğŸ“‹ **[ç”Ÿäº§éƒ¨ç½²æŒ‡å—](README_PRODUCTION.md)** - è¯¦ç»†éƒ¨ç½²å’Œé…ç½®è¯´æ˜
- ğŸ“Š **[Mac Studio æµ‹è¯•æŠ¥å‘Š](Mac_Studio_å®Œæ•´æµ‹è¯•æŠ¥å‘Š_20250821.md)** - å®Œæ•´æ€§èƒ½æµ‹è¯•ç»“æœ
- ğŸ”§ **[API æ–‡æ¡£](API_DOCS.md)** - è¯¦ç»† API æ¥å£è¯´æ˜
- ğŸ› **[æ•…éšœæ’æŸ¥æ‰‹å†Œ](TROUBLESHOOTING.md)** - å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ

---

## ğŸ¤ ç¤¾åŒºå’Œæ”¯æŒ

### è·å–å¸®åŠ©

- ğŸ› **é—®é¢˜æŠ¥å‘Š**: [GitHub Issues](../../issues)
- ğŸ’¬ **ç¤¾åŒºè®¨è®º**: [GitHub Discussions](../../discussions)  
- ğŸ“§ **æŠ€æœ¯æ”¯æŒ**: é€šè¿‡ Issue è”ç³»æˆ‘ä»¬
- ğŸ“š **è¯¦ç»†æ–‡æ¡£**: æŸ¥çœ‹é¡¹ç›® docs ç›®å½•

### è´¡çŒ®æŒ‡å—

1. **Fork** é¡¹ç›®
2. **åˆ›å»º** åŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/amazing-feature`
3. **æäº¤** æ›´æ”¹: `git commit -m 'Add amazing feature'`
4. **æ¨é€** åˆ†æ”¯: `git push origin feature/amazing-feature`
5. **åˆ›å»º** Pull Request

### å¼€å‘è§„èŒƒ

- âœ… éµå¾ª PEP 8 ä»£ç è§„èŒƒ
- âœ… æ·»åŠ ç±»å‹æç¤º (Type Hints)
- âœ… ç¼–å†™å•å…ƒæµ‹è¯•
- âœ… æ›´æ–°ç›¸å…³æ–‡æ¡£

---

## ğŸ“„ è®¸å¯è¯å’Œè‡´è°¢

### è®¸å¯è¯
æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE) - æ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ã€‚

### è‡´è°¢
æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®:
- [VLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- [MLX](https://github.com/ml-explore/mlx) - Apple Silicon ä¼˜åŒ–
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - CPU æ¨ç†å¼•æ“
- [Flask](https://flask.palletsprojects.com/) - Web æ¡†æ¶
- [Qwen](https://huggingface.co/Qwen) - å¼€æºæ¨¡å‹

---

## ğŸ“ˆ é¡¹ç›®çŠ¶æ€

- âœ… **å½“å‰ç‰ˆæœ¬**: v1.0.0 
- ğŸš€ **å¼€å‘çŠ¶æ€**: æŒç»­ç»´æŠ¤å’Œæ›´æ–°
- ğŸ”„ **API å…¼å®¹**: 100% OpenAI å…¼å®¹
- ğŸŒ **å¹³å°æ”¯æŒ**: Windows, macOS, Linux å…¨æ”¯æŒ
- ğŸ§ª **æµ‹è¯•è¦†ç›–**: Mac Studio M3 Ultra ç”Ÿäº§çº§æµ‹è¯•é€šè¿‡

---

<div align="center">

**ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

[â­ Star é¡¹ç›®](../../stargazers) | [ğŸ› æŠ¥å‘Šé—®é¢˜](../../issues) | [ğŸ’¡ åŠŸèƒ½å»ºè®®](../../issues) | [ğŸ“– æŸ¥çœ‹æ–‡æ¡£](docs/)

---

*è®©æ¯ä¸ªå¼€å‘è€…éƒ½èƒ½è½»æ¾æ‹¥æœ‰æœ¬åœ°åŒ–çš„ä¼ä¸šçº§ AI èƒ½åŠ›* ğŸš€

Made with â¤ï¸ by VLLM Team

</div>